<div class="email-body-content">
<date>
                    
                        May 1, 2024
                    </date>
<h1 class="subject">[AINews] LLMs-as-Juries</h1>
<blockquote>
<p>This is AI News! an MVP of a service that goes thru all AI discords/Twitters/reddits and summarizes what people are talking about, so that you can keep up without the fatigue. Signing up <a href="https://buttondown.email/ainews/" target="_blank">here</a> opts you in to the real thing when we launch it üîú</p>
</blockquote>
<hr/>
<blockquote>
<p>AI News for 4/29/2024-4/30/2024. We checked 7 subreddits and <a href="https://twitter.com/i/lists/1585430245762441216?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank"><strong>373</strong> Twitters</a> and <strong>28</strong> Discords (<strong>417</strong> channels, and <strong>4855</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>579 minutes</strong>.</p>
</blockquote>
<p>In the agent literature it is common to find that multiple agents  outperform single agents (if you conveniently ignore inference cost). <a href="https://twitter.com/cohere/status/1785284142789242932?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Cohere has now found the same for LLMs-as-Judges</a>:</p>
<p><img alt="image.png" class="newsletter-image" src="https://assets.buttondown.email/images/ecea573b-f0e8-4e44-968d-82e8f2f4540e.png?w=960&amp;fit=max"/> </p>
<hr/>
<p><strong>Table of Contents</strong></p>
<div class="toc">
<ul>
<li><a href="#ai-reddit-recap">AI Reddit Recap</a></li>
<li><a href="#ai-twitter-recap">AI Twitter Recap</a></li>
<li><a href="#ai-discord-recap">AI Discord Recap</a></li>
<li><a href="#part-1-high-level-discord-summaries">PART 1: High level Discord summaries</a><ul>
<li><a href="#cuda-mode-discord">CUDA MODE Discord</a></li>
<li><a href="#unsloth-ai-daniel-han-discord">Unsloth AI (Daniel Han) Discord</a></li>
<li><a href="#lm-studio-discord">LM Studio Discord</a></li>
<li><a href="#stabilityai-stable-diffusion-discord">Stability.ai (Stable Diffusion) Discord</a></li>
<li><a href="#perplexity-ai-discord">Perplexity AI Discord</a></li>
<li><a href="#nous-research-ai-discord">Nous Research AI Discord</a></li>
<li><a href="#modular-mojo-discord">Modular (Mojo üî•) Discord</a></li>
<li><a href="#huggingface-discord">HuggingFace Discord</a></li>
<li><a href="#openrouter-alex-atallah-discord">OpenRouter (Alex Atallah) Discord</a></li>
<li><a href="#llamaindex-discord">LlamaIndex Discord</a></li>
<li><a href="#eleuther-discord">Eleuther Discord</a></li>
<li><a href="#laion-discord">LAION Discord</a></li>
<li><a href="#openai-discord">OpenAI Discord</a></li>
<li><a href="#openaccess-ai-collective-axolotl-discord">OpenAccess AI Collective (axolotl) Discord</a></li>
<li><a href="#latent-space-discord">Latent Space Discord</a></li>
<li><a href="#openinterpreter-discord">OpenInterpreter Discord</a></li>
<li><a href="#tinygrad-george-hotz-discord">tinygrad (George Hotz) Discord</a></li>
<li><a href="#cohere-discord">Cohere Discord</a></li>
<li><a href="#langchain-ai-discord">LangChain AI Discord</a></li>
<li><a href="#alignment-lab-ai-discord">Alignment Lab AI Discord</a></li>
<li><a href="#ai-stack-devs-yoko-li-discord">AI Stack Devs (Yoko Li) Discord</a></li>
<li><a href="#skunkworks-ai-discord">Skunkworks AI Discord</a></li>
<li><a href="#mozilla-ai-discord">Mozilla AI Discord</a></li>
<li><a href="#interconnects-nathan-lambert-discord">Interconnects (Nathan Lambert) Discord</a></li>
<li><a href="#llm-perf-enthusiasts-ai-discord">LLM Perf Enthusiasts AI Discord</a></li>
<li><a href="#datasette-llm-simonw-discord">Datasette - LLM (@SimonW) Discord</a></li>
<li><a href="#discoresearch-discord">DiscoResearch Discord</a></li>
</ul>
</li>
<li><a href="#part-2-detailed-by-channel-summaries-and-links">PART 2: Detailed by-Channel summaries and links</a></li>
</ul>
</div>
<hr/>
<h1 id="ai-reddit-recap">AI Reddit Recap</h1>
<blockquote>
<p>Across r/LocalLlama, r/machinelearning, r/openai, r/stablediffusion, r/ArtificialInteligence, /r/LLMDevs, /r/Singularity. Comment crawling works now but has lots to improve!</p>
</blockquote>
<p>Here is the updated summary with the requested formatting and de-ranking of AGI posts:</p>
<p><strong>OpenAI News</strong></p>
<ul>
<li><strong>Memory feature now available to all ChatGPT Plus users</strong>: OpenAI <a href="https://twitter.com/OpenAI/status/1784992796669096181?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">announced on Twitter</a> that the memory feature is now rolled out to all ChatGPT Plus subscribers.</li>
<li><strong>OpenAI partners with Financial Times for AI in news</strong>: OpenAI has <a href="https://www.reuters.com/technology/financial-times-openai-sign-content-licensing-partnership-2024-04-29/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">signed a deal to license content</a> from the Financial Times to train its AI models. An <a href="https://i.redd.it/s09mjga1jgxc1.jpeg?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">image was shared</a> announcing the partnership to develop AI experiences for news. </li>
<li><strong>Concerns over OpenAI's profitability with paid training data</strong>: In /r/OpenAI, a <a href="https://www.reddit.com/r/OpenAI/comments/1cfxd42/how_is_openai_going_to_be_profitable_if_they_have/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">post questioned</a> OpenAI's profitability as they start paying to license training data, speculating local open source models may undercut their business.</li>
<li><strong>Possible reduction in GPT-4 usage limits</strong>: A <a href="https://www.reddit.com/r/OpenAI/comments/1cfxzvl/has_openai_reduced_the_number_of_questions/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">user in /r/OpenAI noticed</a> GPT-4's usage has been reduced from 40 messages per 3 hours to around 20 questions per hour. </li>
<li><strong>Issues with ChatGPT after memory update</strong>: In /r/OpenAI, a user <a href="https://www.reddit.com/r/OpenAI/comments/1cg8zsd/chatgpt_laziness_data_cleansing_and_analysis_is/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">found ChatGPT struggled</a> with data cleansing and analysis tasks after the memory update, producing errors and incomplete outputs.</li>
</ul>
<p><strong>OpenAI API Projects and Discussions</strong></p>
<ul>
<li><strong>Tutorial on building an AI voice assistant with OpenAI</strong>: A <a href="https://www.reddit.com/r/OpenAI/comments/1cgh184/how_i_build_an_ai_voice_assistant_with_openai/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog post was shared</a> in /r/OpenAI on building an AI voice assistant using OpenAI's API along with web speech APIs. </li>
<li><strong>AI-powered side projects discussion</strong>: In /r/OpenAI, a <a href="https://www.reddit.com/r/OpenAI/comments/1cg5mm7/whats_your_ai_backed_side_project/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">post asked others to share</a> their AI-powered side projects. The poster made a requirements analysis tool with GPT-4 and an interactive German tutor with GPT-3.5.</li>
<li><strong>Interface agents powered by LLMs</strong>: A /r/OpenAI <a href="https://www.reddit.com/r/OpenAI/comments/1cg3f2z/p_interface_agents_building_llmenabled_agents/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">post discussed "interface agents"</a> - AI that can interact with and control user interfaces like browsers and apps. It covered key components, tools, challenges and use cases.</li>
<li><strong>Difficulty resizing elements in GPT-4 generated images</strong>: In /r/OpenAI, a <a href="https://www.reddit.com/r/OpenAI/comments/1cga0zy/best_way_to_tell_gpt4_to_shrink_something_in_a/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">user asked for advice</a> on instructing GPT-4 to shrink an element in a generated image, as the model struggles to consistently resize things.</li>
</ul>
<p><strong>Stable Diffusion Models and Extensions</strong></p>
<ul>
<li><strong>Seeking realistic SDXL models comparable to PonyXL</strong>: In /r/StableDiffusion, a <a href="https://www.reddit.com/r/StableDiffusion/comments/1cfv7ga/any_realistic_sdxl_model_as_good_as_ponyxl/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">user asked about</a> realistic SDXL models on par with PonyXL's quality and prompt alignment for photographic styles.</li>
<li><strong>Hi-diffusion extension for ComfyUI</strong>: A /r/StableDiffusion user <a href="https://www.reddit.com/r/StableDiffusion/comments/1cg2394/hidiffusion_is_very_impressive_now_the_comfyui/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">found Hi-diffusion works well</a> for generating detailed 2K images in ComfyUI with SD1.5 models, outperforming Khoya deep shrink. An extension is available but needs improvements.</li>
<li><strong>Virtuoso Nodes v1.1 adds Photoshop features to ComfyUI</strong>: <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgexi9/virtuoso_nodes_release_v11_with_new_photoshop/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Version 1.1 of Virtuoso Nodes</a> for ComfyUI was released, adding 8 new nodes that replicate key Photoshop functions like blend modes, selective color, color balance, etc.</li>
<li><strong>Styles to simplify Pony XL prompts in Fooocus</strong>: A /r/StableDiffusion user <a href="https://www.reddit.com/r/StableDiffusion/comments/1cglyq4/styles_for_fooocus_to_shorten_your_pony_xl/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">created styles for Fooocus</a> to handle the quality tags in Pony XL prompts, allowing cleaner and shorter prompts focused on content.</li>
<li><strong>Anime-style shading LoRA released</strong>: An <a href="https://huggingface.co/2vXpSwA7/iroiro-lora/blob/main/test3/sdxl-shadow_01.safetensors?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">anime-style shading LoRA</a> was announced, recommended for use with Anystyle and other ControlNets. A Hugging Face link to the LoRA file was provided.</li>
</ul>
<p><strong>Stable Diffusion Help and Discussion</strong></p>
<ul>
<li><strong>Avoiding explicit content in generated images</strong>: In /r/StableDiffusion, a user getting <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgjrds/80_of_my_generated_pics_have_dicks_coming_out_of/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">phallic elements in 80% of their generated images</a> asked for negative prompt advice to generate "regular porn" instead.</li>
<li><strong>Creating short video clips with AI images and animated text</strong>: A /r/StableDiffusion <a href="https://www.reddit.com/r/StableDiffusion/comments/1cfwxct/how_to_create_short_videos_by_using_ai_images_and/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">post asked about APIs</a> to generate AI images with animated text overlays to create short video clips.</li>
<li><strong>Newer Nvidia GPUs may be slower for AI despite gaming gains</strong>: A <a href="https://www.reddit.com/r/StableDiffusion/comments/1cg0gz6/be_careful_when_buying_new_nvidia_card_or_laptop/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">warning was posted</a> that newer Nvidia GPUs like the 4070 laptop version use narrower memory buses than older models, making them slower for AI workloads.</li>
<li><strong>Proposal for community image tagging project</strong>: A /r/StableDiffusion <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgbivm/community_effort_for_best_image_tagging/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">post suggested a community effort</a> to comprehensively tag images to create a dataset of consistently captioned images for training better models.</li>
<li><strong>Using VAEs for image compression</strong>: Experiments <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgdyjc/vae_as_image_compression/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">shared in /r/StableDiffusion</a> show using VAE latents for image compression is competitive with JPEG in some cases. Saving generated images as latents is lossless and much smaller than PNGs.</li>
<li><strong>Generating a full body from a headshot</strong>: In /r/StableDiffusion, a <a href="https://www.reddit.com/r/StableDiffusion/comments/1cg3a4z/help_me_with_this_will_pay/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">user asked if it's possible</a> to generate a full body from a headshot image without altering the face much using SD Forge.</li>
<li><strong>Textual inversion of Audrey Hepburn</strong>: A /r/StableDiffusion user <a href="https://www.reddit.com/r/StableDiffusion/comments/1cft1gp/give_you_a_slightly_different_audrey_hepburn/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">made a textual inversion</a> of Audrey Hepburn that produces similar but varied faces, sharing example images and a Civitai link.</li>
</ul>
<hr/>
<h1 id="ai-twitter-recap">AI Twitter Recap</h1>
<blockquote>
<p>all recaps done by Claude 3 Opus, best of 4 runs. We are working on clustering and flow engineering with Haiku.</p>
</blockquote>
<p><strong>LLMs and AI Models</strong></p>
<ul>
<li><strong>Llama 3 Performance</strong>: <a href="https://twitter.com/abacaj/status/1785147493728039111?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@abacaj</a> noted that llama-3 models with zero-training can get <strong>32k context with exceptional quality</strong>, surpassing significantly larger models. <a href="https://twitter.com/rohanpaul_ai/status/1784889182558539917?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@rohanpaul_ai</a> mentioned Llama 3 captures extremely nuanced data relationships, utilizing even the minutest decimals in BF16 precision, making it <strong>more sensitive to quantization degradation</strong> compared to Llama 2.</li>
<li><strong>Llama 3 Benchmarks</strong>: <a href="https://twitter.com/abacaj/status/1785295341736043007?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@abacaj</a> reported llama-3 70B takes <strong>3rd place on a benchmark, replacing Haiku</strong>. <a href="https://twitter.com/abacaj/status/1785153286976237765?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@abacaj</a> shared a completion from the model on a <strong>code snippet benchmark</strong> that requires the model to find a function based on a description.</li>
<li><strong>Llama 3 Variants</strong>: <a href="https://twitter.com/mervenoyann/status/1785320444918211022?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@mervenoyann</a> noted <strong>new LLaVA-like models based on LLaMA 3 &amp; Phi-3</strong> that pass the baklava benchmark. <a href="https://twitter.com/AIatMeta/status/1785042326416658580?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@AIatMeta</a> mentioned Meditron, an LLM suite for low-resource medical settings built by @ICepfl &amp; @YaleMed researchers, which <strong>outperforms most open models in its parameter class</strong> on benchmarks like MedQA &amp; MedMCQA using Llama 3.</li>
<li><strong>GPT-2 Chatbot</strong>: There was speculation about the identity of the gpt2-chatbot model, with <a href="https://twitter.com/sama/status/1785107943664566556?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@sama</a> noting he has a soft spot for gpt2. Some theories suggested it could be a preview of GPT-4.5/5 or a derivative model, but most agreed it was <strong>unlikely to be the latest OAI model</strong>. </li>
<li><strong>Phi-3 and Other Models</strong>: <a href="https://twitter.com/danielhanchen/status/1785040680106234225?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@danielhanchen</a> released a <strong>Phi-3 notebook that finetunes 2x faster and uses 50% less VRAM</strong> than HF+FA2. <a href="https://twitter.com/rohanpaul_ai/status/1785220060803453160?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@rohanpaul_ai</a> shared a paper suggesting transformers learn in-context by <strong>performing gradient descent on a loss function constructed from the in-context data</strong> within their forward pass.</li>
</ul>
<p><strong>Prompt Engineering and Evaluation</strong></p>
<ul>
<li><strong>Prompt Engineering Techniques</strong>: <a href="https://twitter.com/cwolferesearch/status/1784992130777137362?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@cwolferesearch</a> categorized recent prompt engineering research into <strong>reasoning, tool usage, context window, and better writing</strong>. Techniques include zero-shot CoT prompting, selecting exemplars based on complexity, refining rationales, decomposing tasks, using APIs, optimizing context windows, and iterative prompting.</li>
<li><strong>LLMs as Juries</strong>: <a href="https://twitter.com/cohere/status/1785284142789242932?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@cohere</a> released a paper exploring <strong>replacing a single LLM judge with multiple LLM juries</strong> for evaluation. The "PoLL" method with a diverse set of LLMs <strong>outperformed single judges across datasets while being 7-8x cheaper</strong> than GPT-4.</li>
<li><strong>Evaluating LLMs</strong>: <a href="https://twitter.com/_lewtun/status/1785246966626029596?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@_lewtun</a> asked about research on which prompts produce an LLM-judge most correlated with human preferences for pairwise rankings, beyond the work by @lmsysorg. <a href="https://twitter.com/_philschmid/status/1785273493375922221?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@_philschmid</a> summarized the <strong>PoLL (Panel of LLM) method</strong> proposed by @cohere for LLM evaluation as an alternative to a single large model judge.</li>
</ul>
<p><strong>Applications and Use Cases</strong></p>
<ul>
<li><strong>Financial Calculations</strong>: <a href="https://twitter.com/llama_index/status/1785325832317415641?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@llama_index</a> shared a full-stack tutorial for building a financial assistant that can <strong>calculate percentage evolution, CAGR, and P/E ratios over unstructured financial reports</strong> using LlamaParse, RAG, Opus, and math formulas in @llama_index.</li>
<li><strong>SQL Query Generation</strong>: <a href="https://twitter.com/virattt/status/1785059112478257413?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@virattt</a> used @cohere cmd r+ to <strong>extract ticker and year metadata from financial queries</strong> in ~1s, then used the metadata to filter a vector db, fed results to GPT-4, and answered user query with ~3s total latency.</li>
<li><strong>Multi-Agent RAG</strong>: <a href="https://twitter.com/LangChainAI/status/1785066609847291986?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@LangChainAI</a> announced a YouTube workshop on exploring "multi-agent" applications that <strong>combine independent agents to solve complex problems</strong> using planning, reflection, tool use, and their LangGraph library.</li>
<li><strong>Robotics and Embodied AI</strong>: <a href="https://twitter.com/DrJimFan/status/1785292766387302897?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@DrJimFan</a> advocated for <strong>robotics as the next frontier after LLMs</strong>, sharing MIT AI Lab's 1971 proposal emphasizing robotics and reflecting on the current state. <a href="https://twitter.com/_akhaliq/status/1785139220534730771?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@_akhaliq</a> shared a paper on Ag2Manip, which <strong>improves imitation learning for manipulation tasks</strong> using agent-agnostic visual and action representations.</li>
</ul>
<p><strong>Frameworks, Tools and Platforms</strong></p>
<ul>
<li><strong>LangChain Tutorials</strong>: <a href="https://twitter.com/LangChainAI/status/1784970647875330251?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@LangChainAI</a> shared a <strong>4-hour course on understanding how LangChain works</strong> with various technologies to build 6 projects. <a href="https://twitter.com/llama_index/status/1784962053641478454?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@llama_index</a> provided a <strong>reference architecture for advanced RAG</strong> using LlamaParse, AWS Bedrock, and @llama_index.</li>
<li><strong>Diffusers Library</strong>: <a href="https://twitter.com/RisingSayak/status/1785162074844197174?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@RisingSayak</a> explained how the Diffusers library <strong>supports custom pipelines and components</strong>, allowing flexibility in building diffusion models while maintaining the benefits of the <code>DiffusionPipeline</code> class.</li>
<li><strong>Amazon Bedrock</strong>: <a href="https://twitter.com/cohere/status/1785015769971220720?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@cohere</a> announced their <strong>Command R model series is now available on Amazon Bedrock</strong> for enterprise workloads. <a href="https://twitter.com/llama_index/status/1785105949818237227?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@llama_index</a> showed how to use LlamaParse for advanced parsing in the AWS/Bedrock ecosystem and <strong>build RAG with the Bedrock Knowledge Base</strong>.</li>
<li><strong>DeepSpeed Support</strong>: <a href="https://twitter.com/StasBekman/status/1785091895733154116?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@StasBekman</a> noted a PR merged into <code>main@accelerate</code> that makes FSDP <strong>converge at the same speed as DeepSpeed when loading fp16 models</strong>, by automatically upcasting trainable params to fp32.</li>
</ul>
<p><strong>Memes, Humor and Other</strong></p>
<ul>
<li><strong>ASCII Art</strong>: Several tweets poked fun at the ASCII art capabilities of LLMs, with <a href="https://twitter.com/ylecun/status/1785109502565531699?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@ylecun</a> noting how <strong>AI hype has become indistinguishable from satire</strong>. <a href="https://twitter.com/teortaxesTex/status/1785325820166185399?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@teortaxesTex</a> shared a prompt to draw a Katamari Damacy level map using emojis that strains "GPT2"'s instruction following.</li>
<li><strong>Anthropic Slack</strong>: <a href="https://twitter.com/alexalbert__/status/1785369914204938326?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@alexalbert__</a> shared his <strong>10 favorite things from Anthropic's internal Slack channel</strong> where employees post cool Claude interactions and memes since its launch.</li>
<li><strong>Rabbit Disappointment</strong>: Several users expressed disappointment with the Rabbit AI device, noting its <strong>limited functionality compared to expectations</strong>. <a href="https://twitter.com/agihippo/status/1785359480294936882?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">@agihippo</a> questioned what the Rabbit r1 can do that a phone can't.</li>
</ul>
<hr/>
<h1 id="ai-discord-recap">AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries</p>
</blockquote>
<p><strong>1) Fine-Tuning and Optimizing Large Language Models</strong></p>
<ul>
<li><strong>Challenges in Fine-Tuning LLaMA-3</strong>: Engineers faced issues like the model <strong>not generating EOS tokens</strong>, and <strong>embedding layer compatibility across bit formats</strong>. However, one member achieved success by utilizing <strong><a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1553?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LLaMA-3 specific prompt strategies</a></strong> for fine-tuning.</li>
</ul>
<ul>
<li><strong>LLaMA-3 Sensitive to Quantization</strong>: Discussions highlighted that <strong><a href="https://x.com/rohanpaul_ai/status/1784972618472317180?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LLaMA-3 experiences more degradation from quantization</a></strong> compared to LLaMA-2, likely due to capturing nuanced relationships from training on 15T tokens.</li>
</ul>
<ul>
<li><strong>Perplexity Fine-Tuning Challenges</strong>: Fine-tuning <strong>LLaMA-3 for perplexity</strong> may not surpass the base model's performance, with the tokenizer suspected as a potential cause.</li>
</ul>
<p><strong>2) Extending Context Lengths and Capabilities</strong></p>
<ul>
<li><strong>Llama-3 Hits New Context Length Highs</strong>: The release of <strong><a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Llama-3 8B Gradient Instruct 1048k</a></strong> extends the context length from 8k to over 1048k tokens, showcasing state-of-the-art long context handling.</li>
</ul>
<ul>
<li><strong>Llama 3 Gains Vision with SigLIP</strong>: A breakthrough integrates <strong><a href="https://huggingface.co/qresearch/llama-3-vision-alpha-hf?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">vision capabilities for Llama 3</a></strong> using SigLIP, enabling direct use within Transformers despite quantization limitations.</li>
</ul>
<ul>
<li><strong>Extending Context to 256k with PoSE</strong>: The context length of <strong>Llama 3 8B</strong> has been expanded from 8k to <strong><a href="https://huggingface.co/winglian/llama-3-8b-256k-PoSE?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">256k tokens using PoSE</a></strong>, though inferencing challenges remain for 'needle in haystack' scenarios.</li>
</ul>
<p><strong>3) Benchmarking and Evaluating LLMs</strong></p>
<ul>
<li><strong>Llama 3 Outperforms GPT-4 in German NLG</strong>: On the <strong><a href="https://scandeval.com/german-nlg/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ScanEval German NLG benchmark</a></strong>, <strong>Llama 3</strong> surpassed the performance of <strong>GPT-4</strong>, indicating its strong language generation capabilities.</li>
</ul>
<ul>
<li><strong>Mysterious GPT2-Chatbot Sparks Speculation</strong>: A <strong><a href="https://chat.lmsys.org/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GPT2-chatbot</a></strong> with gpt4-level capabilities surfaced, leading to debates on whether it could be an early glimpse of <strong>GPT-4.5</strong> or a finetuned version of the original GPT-2.</li>
</ul>
<ul>
<li><strong>Questioning Leaderboard Utility for Code Generation</strong>: A <strong><a href="https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog post</a></strong> challenges the effectiveness of AI leaderboards for code generation, citing the high operational cost of top performers like LLM debugger despite ranking highly.</li>
</ul>
<p><strong>4) Revolutionizing Gaming with LLM-Powered NPCs</strong></p>
<ul>
<li><strong>LLM-Powered NPCs and Inference Stack</strong>: The release of <strong><a href="https://github.com/GigaxGames/gigax?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LLM-powered NPC models</a></strong> aims to enhance action spaces and simplify API calls, including a single LLM call feature and open-weights on Hugging Face.</li>
</ul>
<ul>
<li><strong>Overcoming LLM Challenges for Gameplay</strong>: Developers faced issues like <strong>NPCs breaking the fourth wall</strong>, missing details in large prompts, and optimizing for runtime speeds, suggesting solutions like <strong>output compression</strong>, <strong>minimizing model calls</strong>, and leveraging <strong>smaller models</strong>.</li>
</ul>
<ul>
<li><strong>Insights into Fine-Tuning LLMs for NPCs</strong>: Developers plan to share their <strong>struggles and triumphs in fine-tuning LLMs for dynamic NPC behavior</strong> through an upcoming blog post, pointing towards new strategies for gaming applications.</li>
</ul>
<p><strong>5) Misc</strong></p>
<ul>
<li><strong>CUDA Optimization Techniques</strong>: CUDA developers discussed various optimization strategies, including using <code>Packed128</code> custom structs for memory access patterns, replacing integer division with bit shifts (<a href="https://godbolt.org/z/9K9Gf1v6P?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Compiler Explorer link</a>), and comparing performance of <strong>CUTLASS vs CuBLAS</strong> for matrix multiplications. The <strong>Effort Engine</strong> algorithm was introduced, enabling adjustable computational effort during LLM inference to achieve speeds comparable to standard matrix multiplications on Apple Silicon (<a href="https://kolinko.github.io/effort?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">kolinko.github.io/effort</a>, <a href="https://github.com/kolinko/effort?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GitHub</a>).</li>
</ul>
<ul>
<li><strong>LLaMA-3 Context Length Extension and Fine-Tuning</strong>: The <strong>LLaMA-3 8B</strong> model's context length was extended to over 1M tokens using <strong>PoSE</strong> (<a href="https://huggingface.co/winglian/llama-3-8b-256k-PoSE?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">huggingface.co/winglian/llama-3-8b-256k-PoSE</a>), sparking discussions on its retrieval performance and compute requirements. Fine-tuning LLaMA-3 presented challenges like <strong>quantization degradation</strong>, <strong>EOS token generation</strong>, and <strong>embedding layer compatibility</strong> across bit formats. A potential breakthrough was shared in a <a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1553?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GitHub pull request</a> demonstrating successful fine-tuning with model-specific prompt strategies.</li>
</ul>
<ul>
<li><strong>Civitai Monetization Backlash</strong>: Stable Diffusion community members expressed discontent with <strong>Civitai's monetization strategies</strong>, particularly the <strong>Buzz donation system</strong>, which was labeled a "rip-off" by some like Tower13Studios (<a href="https://youtu.be/nLT32AR5c68?si=bV9wXlRzb_oLutW9&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">The Angola Effect</a>). Discussions also highlighted the potential profitability of <strong>NSFW AI-generated art commissions</strong> compared to the saturated SFW market.</li>
</ul>
<ul>
<li><strong>Perplexity AI Performance Issues</strong>: Users reported significant slowdowns and poor performance across various Perplexity AI models during Japan's Golden Week, with specific issues in <strong>Japanese searches</strong> resulting in meaningless outputs. Frustrations arose over <strong>expired Pro subscription coupons</strong> and the removal of the <strong>7-day free trial</strong>. Technical troubles included <strong>email link delays</strong> affecting login and inconsistencies in the <strong>iOS voice feature</strong> depending on app versions.</li>
</ul>
<ul>
<li><strong>Decentralized AI Training Initiatives</strong>: Prime Intellect proposed a decentralized training approach using <strong>H100 GPU clusters</strong> to enable open-source AI to compete with proprietary models (<a href="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog post</a>). The initiative aims to address computing infrastructure limitations by leveraging globally distributed GPU resources.</li>
</ul>
<hr/>
<h1 id="part-1-high-level-discord-summaries">PART 1: High level Discord summaries</h1>
<h2 id="cuda-mode-discord"><a href="https://discord.com/channels/1189498204333543425?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">CUDA MODE</a> Discord</h2>
<ul>
<li><strong>Triton Troubles</strong>: Engineers discussed limitations with <strong>Triton blocks</strong>, identifying an issue where blocks of 4096 elements are feasible, yet blocks of 8192 are not, hinting at discrepancies with expected <strong>CUDA</strong> limits.</li>
</ul>
<ul>
<li><strong>CUDA Cognitions and Collaborations</strong>: Various <strong>CUDA</strong> topics were mulled over, including <strong>CUTLASS vs. CuBLAS performance</strong>, <strong>CUDA checkpointing</strong>, and the replacement of integer division with bit shifts. A link to the <a href="https://godbolt.org/z/9K9Gf1v6P?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Compiler Explorer</a> was shared to help with experiments.</li>
</ul>
<ul>
<li><strong>PyTorch Peculiarities Pursued</strong>: Members examined the behavior of PyTorch's <code>linear</code> function and matrix multiplication kernel launches, with observations about double kernel launches and the false expectation of performance differences due to transposition.</li>
</ul>
<ul>
<li><strong>LLM Inference Optimization with Effort Engine</strong>: Discussion revolved around the <strong>Effort Engine</strong> algorithm, which enables adjustable computational effort during LLM inference, purportedly yielding speeds comparable to standard matrix multiplications on Apple Silicon at lower efforts. The implementation and details are provided on <a href="https://kolinko.github.io/effort?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">kolinko.github.io/effort</a> and <a href="https://github.com/kolinko/effort?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GitHub</a>.</li>
</ul>
<ul>
<li><strong>InstaDeep's Machine Learning Manhunt</strong>: <strong>InstaDeep</strong> is on the hunt for <strong>Machine Learning Engineers</strong> with expertise in <strong>high-performance ML engineering, custom CUDA kernels, and distributed training</strong>. Candidates can scout for opportunities at <a href="https://www.instadeep.com/job-offer/92900fa3-5501-4506-a63f-cebee958fc6f/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">InstaDeep Careers</a>. </li>
</ul>
<ul>
<li><strong>Llama-3 Levitates to Longer Contexts</strong>: The release of <a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Llama-3 8B Gradient Instruct 1048k</a> set a new benchmark for context length capabilities in LLMs.</li>
</ul>
<ul>
<li><strong>ROCm Rallies for Flash Attention 2</strong>: Conversations in the <strong>ROCM</strong> channel centered on adapting NVIDIA's Flash Attention 2 for ROCm, with a focus on compatibility with <strong>ROCM 6.x</strong> versions and a link to the relevant repository <a href="https://github.com/ROCm/flash-attention?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ROCm/flash-attention on GitHub</a>.</li>
</ul>
<ul>
<li><strong>CUDA Conclave Converges on ‚ÄúPacked128‚Äù Innovations</strong>: The <strong>llmdotc</strong> channel was a hotspot with discussions focused on optimizing <code>Packed128</code> data structures and <strong>BF16 mixed-precision strategies</strong>, while also touching on the nuanced use of <strong>NVTX</strong> contexts and the utility of different benchmarking toolsets like <strong>Modal</strong>.</li>
</ul>
<hr/>
<h2 id="unsloth-ai-daniel-han-discord"><a href="https://discord.com/channels/1179035537009545276?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>Fusing Checkpoints to Avoid Overfitting</strong>: A member sought guidance on checkpoint merging to avoid overfitting and was directed to the Unsloth <a href="https://github.com/unslothai/unsloth/wiki?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#finetuning-from-your-last-checkpoint" target="_blank">finetuning checkpoint wiki</a>. Techniques such as <em>warmup steps</em> and <em>resuming from checkpoints</em> were recommended for nuanced training regimens.</li>
</ul>
<ul>
<li><strong>Quantization Quandary in WSL2</strong>: Users reported <strong>RuntimeError: Unsloth: Quantization failed</strong> when converting models to F16 within WSL2. Despite attempts at rebuilding the <code>llama.cpp</code> and re-quantization, the error persisted.</li>
</ul>
<ul>
<li><strong>Phi-3: A Model of Interest</strong>: The upcoming release of <strong>Phi-3</strong> stirred interest, with engineers debating whether to adopt the 3.8b version or wait for the heftier 7b or 14b variants.</li>
</ul>
<ul>
<li><strong>OOM Countermeasures and Performance Data Confusion</strong>: Tips for handling Out of Memory errors on Google Colab by cache clearing were exchanged. Meanwhile, confusion surfaced over reported performance measures for quantized <strong>LLama 2</strong> and <strong>LLama 3</strong>, hinting at possible data misplacement between Bits Per Word (BPW) and Perplexity (PPL).</li>
</ul>
<ul>
<li><strong>Extended Possibilities</strong>: <strong>Llama 3 8B</strong> reached new potential with a Context length increase to 256k tokens, achieved with <strong><a href="https://huggingface.co/papers/2309.10400?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">PoSE</a></strong>, showcased at <a href="https://huggingface.co/winglian/llama-3-8b-256k-PoSE?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">winglian/llama-3-8b-256k-PoSE</a>. Community applause went to Winglian, though some voiced skepticism about non-official context-extended model behavior.</li>
</ul>
<hr/>
<h2 id="lm-studio-discord"><a href="https://discord.com/channels/1110598183144399058?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LM Studio</a> Discord</h2>
<ul>
<li><strong>Groq's Gift to Discord Bots</strong>: A user shared a <a href="https://youtu.be/ySwJT3Z1MFI?si=qFfek8gTGXVJWoxB&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">YouTube video</a> highlighting the <em>free</em> Groq API enabling access to the LLAMA-3 model's impressive 300 tokens per second speed, optimally suited for small server Discord bots due to its no-cost setup.</li>
<li><strong>Spec Smackdown</strong>: Users recommended posting system specs in specific channels when troubleshooting <strong>LM Studio on Ubuntu GPUs</strong>, debated the compatibility of GPUs with <strong>inference tasks</strong>, and discussed the potentially incorrect VRAM capacity display in LM Studio causing concerns with <strong>GPU offloading efficiency</strong>.</li>
<li><strong>Model Mania</strong>: The community buzzed about alternative methods for downloading the GGUF model from sources other than Huggingface, the time and resource demands of creating <em>iQuants</em> and <em>imatrices</em>, and shared reward offers for optimizing the <strong>Goliath 120B Longlora</strong> model to create its <em>iQuant</em> version.</li>
<li><strong>Model Mayhem on Modest Machines</strong>: Users grappled with issues like the Phi-3 model's <strong>leaking prompts</strong>, <em>local training</em> queries for Hugging Face-based models, and the unexpected noises from hard drives during token generation by the Llama3m. Some determined that more dated hardware could just about manage a <strong>7b Q4 model</strong> but nothing heftier.</li>
<li><strong>ROCm Ruminations</strong>: Enthusiasts dissected ROCm versions, mulling over the benefits of <strong>beta 0.2.20</strong> for AMD functionality, addressed confusion about compatibility‚Äîespecially the RX 6600's support with the current <strong>HIP SDK</strong>‚Äîand discussed discrepancies in ROCm's functionality on different operating systems like <strong>Ubuntu versus Windows</strong>.</li>
</ul>
<hr/>
<h2 id="stabilityai-stable-diffusion-discord"><a href="https://discord.com/channels/1002292111942635562?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Stability.ai (Stable Diffusion)</a> Discord</h2>
<p><strong>Buzz Off, Civitai</strong>: AI creators in the guild are upset with Civitai's monetization strategies, particularly the Buzz donation system, which was labeled a <strong>"rip-off"</strong> by some members, such as Tower13Studios. The discontent revolves around value not being fairly returned to creators (<a href="https://youtu.be/nLT32AR5c68?si=bV9wXlRzb_oLutW9&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">The Angola Effect</a>).</p>
<p><strong>Finding The AI Art Goldmine</strong>: A vibrant discussion unfolded on the economics of AI-generated art, with consensus pointing towards NSFW commissions, including furry and vtuber content, as a more profitable avenue compared to the more crowded SFW market.</p>
<p><strong>Race for Real-Time Rendering</strong>: Members actively shared Python scripting techniques for accelerating Stable Diffusion (SDXL) models, eyeing uses in dynamic realms like Discord bots, aiming to enhance image generation speed for real-time applications.</p>
<p><strong>Anticipation Builds for Collider</strong>: The community is keenly awaiting Stable Diffusion's next iteration, dubbed "Collider," with speculation about release dates and potential advancements fueling eager anticipation among users.</p>
<p><strong>Tech Troubleshooting Talk</strong>: Guild members exchanged insights and solutions on a spectrum of technical challenges, from creating LoRAs and IPAdapters to running AI models on low-spec hardware, demonstrating a collective effort to push the boundaries of model implementation and optimization.</p>
<hr/>
<h2 id="perplexity-ai-discord"><a href="https://discord.com/channels/1047197230748151888?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Perplexity AI</a> Discord</h2>
<ul>
<li><strong>Japanese Golden Week Glitches</strong>: During Japan's Golden Week, users observed a noticeable performance drop in tools like <strong>Opus, Sonar Large 32K,</strong> and <strong>GPT-4 Turbo</strong>, with specific issues in Japanese searches, resulting in outputs that users deemed <em>meaningless garbage</em>. To address the problem, vigilant monitoring and optimization of these models was suggested.</li>
</ul>
<ul>
<li><strong>Frustration over Pro Subscription and Trial Perils</strong>: <strong>Pro subscription</strong> users reported expired coupons on the due date, with offers linked to the <strong>Nothing Phone 2(a)</strong> aborted prematurely due to fraudulent activities. Moreover, the 7-day free trial's removal from the site prompted disappointment, emphasizing its value as a user conversion tool.</li>
</ul>
<ul>
<li><strong>Tech Turbulence with Perplexity AI</strong>: The community grappled with <strong>email link delays</strong>, causing login difficulties, particularly for non-Gmail services. Additionally, variations in the <strong>iOS voice feature</strong> were found to be dependent on the <strong>app version</strong> being used, reflecting inconsistencies in user experience.</li>
</ul>
<ul>
<li><strong>API Avenues Explored</strong>: Engineers queried the <strong>pplx-api</strong> channel regarding <strong>source URL</strong> access through the API, following its mention in roadmap documentation, and debated whether using <strong>Claude 3</strong> would entail adherence to <strong>Anthropic's political usage</strong> restrictions under Perplexity's terms.</li>
</ul>
<ul>
<li><strong>Miscellaneous Inquiries and Insights Surface</strong>: A post in the <strong>#<a href="https://discord.com/channels/1047197230748151888/1054944216876331118/1234586871569449121?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">sharing</a></strong> channel spotlighted Lenny's Newsletter on product growth and building concepts, while queries about WhatsApp's autoreply feature and Vimeo's API were thrown in. These discussions, particularly on the API, highlight engineers' focus on integrating and utilizing various functionalities in their systems/processes.</li>
</ul>
<hr/>
<h2 id="nous-research-ai-discord"><a href="https://discord.com/channels/1053877538025386074?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Nous Research AI</a> Discord</h2>
<p><strong>Bold Decentralization Move</strong>: Prime Intellect's initiative for decentralized AI training, leveraging <em>H100 GPU clusters</em>, promises to push the boundaries by globalizing distributed training. The open-source approach may address current computing infrastructure bottlenecks as discussed in their <a href="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">decentralized training blog</a>.</p>
<p><strong>Retrieval Revolution with LLama-3</strong>: The extension of <strong>LLama-3 8B's</strong> context length to over 1040K tokens sparks discussions on whether its retrieval performance lives up to the hype. Skeptics remain, emphasizing the ongoing necessity of improvements and training, supported by an <a href="https://arxiv.org/abs/2404.16811?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ArXiv paper on IN2 training</a>.</p>
<p><strong>PDF Challenges Tackled</strong>: To address PDF parsing challenges within AI models, particularly for tables, the community discussed workarounds and tools like <a href="https://platform.openai.com/docs/assistants/tools/file-search?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">OpenAI's file search</a> for better multimodal functionality handling roughly 10k files.</p>
<p><strong>World Sims Showcase AI's Role-Playing Prowess</strong>: Engagements with AI-driven world simulations highlight the capacities of <strong>llama 3 70b</strong> and <strong>Claude 3</strong>, from historical figures to business and singing career simulators. OpenAI's chat on <a href="https://hf.co/chat/assistant/65ffac7250c6fddecfd20bc8?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">HuggingChat</a> and links to niche simulations like <a href="https://hf.co/chat/assistant/6626e4869232378718adc5f2?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Snow Singer Simulator</a> reflect the diversity and depth achievable.</p>
<p><strong>Leveraging Datasets for Multilingual Dense Retrieval</strong>: A noted <a href="https://huggingface.co/collections/nthakur/swim-ir-dataset-662ddaecfc20896bf14dd9b7?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Wikipedia RAG dataset</a> on HuggingFace earmarks the rise of fostering AI's language retrieval capabilities. The included Halal and Kosher data points toward a trend of creating diverse and inclusive AI resources.</p>
<hr/>
<h2 id="modular-mojo-discord"><a href="https://discord.com/channels/1087530497313357884?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Modular (Mojo üî•)</a> Discord</h2>
<ul>
<li><strong>Mojo's Memory Safety and Concurrency Debated</strong>: Despite buzz around <strong>Mojo's</strong> potential, it was clarified that features like <strong>Golang-like concurrency</strong> and <strong>Rust-like memory safety</strong> are not currently implemented due to <strong>borrow checking being disabled</strong>. However, possibilities regarding the use of actor model concurrency are being explored which may enhance Mojo‚Äôs runtime efficiency. </li>
</ul>
<ul>
<li><strong>Installation Tactics for Mojo on Varied Systems</strong>: Users face challenges installing <strong>Mojo</strong> with <strong>Python 3.12.3</strong> particularly on <strong>Mac M1</strong>, for which using a <strong>Conda environment</strong> is recommended. Also, while native <strong>Windows support</strong> is pending, <strong>WSL on Windows</strong> is a current workaround, with cross-compilation capabilities hinted through <strong>LLVM involvement</strong>.</li>
</ul>
<ul>
<li><strong>Community Contributions to Mojo Ecosystem</strong>: Several community-driven projects are enhancing the Mojo ecosystem, from a Mojo-based forum on GitHub to a <strong>20% performance optimized</strong> atof-simd project for long strings. Enthusiasm for collaboration and knowledge-sharing is evident as members share projects and call for joint efforts to tackle challenges such as the 1brc.</li>
</ul>
<ul>
<li><strong>Nightly Compilations Trigger Discussions on SIMD and Source Location</strong>: A new <strong>nightly</strong> release of the <strong>Mojo compiler</strong> spurred conversation about the conversion of <strong>SIMD to EqualityComparable</strong> and the need for explicit <code>reduce_and</code> or <code>reduce_or</code> in place of implicit conversion to <code>Bool</code>. The move of <code>__source_location()</code> to <code>__call_location()</code> incited exchanges on proper usage within the language.</li>
</ul>
<ul>
<li><strong>Performance and Benchmarking Take the Spotlight</strong>: From optimizing SIMD-based error correction code to sharing substantial speed gains in the 1brc project, performance topics spurred discussions on <strong>LLVM/MLIR optimizations</strong>. There were calls to form a "team-mojo" for communal challenge tackling, underscoring a shared interest in progressing Mojo‚Äôs benchmarking endeavors against other languages.</li>
</ul>
<hr/>
<h2 id="huggingface-discord"><a href="https://discord.com/channels/879548962464493619?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">HuggingFace</a> Discord</h2>
<p><strong>Snowflake's MoE Model Breaks Through</strong>: Snowflake introduces a <a href="https://x.com/reach_vb/status/1783129119435210836?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">monumental 408B parameter Dense + Hybrid MoE model</a> with a 4K context window, entirely under Apache 2.0 license, sparking excitement for its performance on sophisticated tasks.</p>
<p><strong>Gradio Share Server on the Fritz</strong>: Gradio acknowledges <a href="https://status.gradio.app/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">issues with their Share Server</a>, impacting Colab integrations, which is under active resolution with updates available on their status page.</p>
<p><strong>CVPR 2023 Sparks Competitive Spirit</strong>: CVPR 2023 <a href="https://huggingface.co/spaces/BVRA/SnakeCLEF2024?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">announced competetive events</a> like SnakeCLEF, FungiCLEF, and PlantCLEF, boasting over $120k in rewards and happening June 17-21, 2024.</p>
<p><strong>MIT Deep Learning Course Goes Live</strong>: MIT updates its Introduction to Deep Learning course for 2024, with comprehensive <a href="https://www.youtube.com/watch?v=ErnWZxJovaM&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=2&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">lecture videos on YouTube</a>.</p>
<p><strong>NLP Woes in Chatbot Land</strong>: Within the NLP community, effort mounts to finetune a chatbot using the Rasa framework, despite struggles with intent recognition and categorization, and plans to augment performance with a custom NER model and company-specific intents.</p>
<hr/>
<h2 id="openrouter-alex-atallah-discord"><a href="https://discord.com/channels/1091220969173028894?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">OpenRouter (Alex Atallah)</a> Discord</h2>
<ul>
<li><strong>Alex Atallah Signposts Collaboration with Syrax</strong>: Alex Atallah has initiated experiments with <strong>Syrax</strong> and extended support by proposing a group chat for collaborative efforts, marking the start of a partnership acknowledged with enthusiasm by Mart02.</li>
</ul>
<ul>
<li><strong>Frontend for the Rest of Us</strong>: The community explored solutions for deploying multi-user frontends on shared hosting without advanced technical requirements. <strong>LibreChat</strong> was suggested as a viable platform, with Vercel's free tier hosting mentioned as a means to address hosting and cost obstacles.</li>
</ul>
<ul>
<li><strong>LLMs Throwdown</strong>: A robust debate unfolded over several large language models including <em>Llama-3 8B</em>, <em>Dolphin 2.9</em>, and <em>Mixtral-8x22B</em>, touching on aspects like context window size and censorship concerns related to conversational styles and datasets.</li>
</ul>
<ul>
<li><strong>Training Unhinged AIs</strong>: An intriguing experiment involved training a model with a toxic dataset to foster a more "unhinged" persona. Discussions dug into model limitations with long contexts, with an agreement that although models like <em>Llama 3 8B</em> handle extensive contexts, performance dips were likely past a threshold.</li>
</ul>
<ul>
<li><strong>Cost-Effective Experimentation on OpenRouter</strong>: Conversations centered on finding efficient yet affordable models on <strong>OpenRouter</strong>. Noteworthy was the mix of surprise and approval for the human-like output of models like <em>GPT-3.5</em> that deliver a solid blend of affordability and performance.</li>
</ul>
<hr/>
<h2 id="llamaindex-discord"><a href="https://discord.com/channels/1059199217496772688?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LlamaIndex</a> Discord</h2>
<p><strong>AWS Architecture Goes Academic</strong>: <strong>LlamaIndex</strong> revealed an advanced AWS-based architecture for building sophisticated RAG systems, aimed at parsing and reasoning. Details are accessible in their <a href="https://t.co/sfQOvhHHg5?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">code repository</a>.</p>
<p><strong>Documentation Bot Triumphs in Hackathon</strong>: Hackathon victors, <strong>Team CLAB</strong>, developed an impressive documentation bot leveraging <strong>LlamaIndex</strong> and <strong>Nomic embeddings</strong>; check out the hackathon wrap-up in this <a href="https://t.co/2UMqrHwO56?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog post</a>.</p>
<p><strong>Financial Assistants Get a Boost</strong>: Constructing financial assistants that interpret unstructured data and perform complex computations has been greatly improved. The methodology is thoroughly explored in a <a href="https://t.co/6cTNxUBJcr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">recent post</a>.</p>
<p><strong>Turbocharging RAG with Semantic Caching</strong>: Collaboration with @Redisinc demonstrated significant performance gains for RAG applications using <strong>semantic caching</strong> to speed up queries. The collaboration details can be found <a href="https://t.co/oGxFrZLMRn?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>.</p>
<p><strong>GPT-1: The Trailblazer Remembered</strong>: A reflective glance at GPT-1 and its contributions to LLM development was shared, discussing features like positional embeddings which paved the way for modern models like Mistral-7B. The nostalgia-laden <a href="https://amgadhasan.substack.com/p/revisiting-gpt-1-the-spark-that-ignited-llms?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog post</a> revisits GPT-1's architecture and impact.</p>
<hr/>
<h2 id="eleuther-discord"><a href="https://discord.com/channels/729741769192767510?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Eleuther</a> Discord</h2>
<p><strong>Plug Into New Community Projects</strong>: Members are seeking opportunities to contribute to community AI projects that provide computational resources, addressing the issue for those lacking personal GPU infrastructure.</p>
<p><strong>Unlock the Mysteries of AI Memory</strong>: Intricacies of memory processes in AI were covered with a particular focus on "clear-ing", orthogonal keys, and the delta rule in compressive memory. There‚Äôs an interest in discussing whether infini-attention has been overhyped, despite its theoretical promise.</p>
<p><strong>Comparing Apples to Supercomputers</strong>: There's an active debate regarding performance discrepancies between models like <em>mixtral 8x22B</em> and <em>llama 3 70B</em>, where <em>llama's</em> reduced number of layers, despite having more parameters, may be impacting its speed and batching efficiency.</p>
<p><strong>LLMs: Peering Inside the Black Box</strong>: The community is contemplating the ‚Äúblack box‚Äù nature of Large Language Models, discussing emergent abilities and data leakage. A connection was made between emergent abilities and pretraining loss, challenging the focus on compute as a performance indicator.</p>
<p><strong>Bit Depth Bewilderment</strong>: A user reported issues when encoding with <strong>8bit</strong> on models like <strong>llama3-70b</strong> and <strong>llamma3-8b</strong>, experiencing significant degradation in output quality, suggesting a cross-model encoding challenge that needs addressing.</p>
<hr/>
<h2 id="laion-discord"><a href="https://discord.com/channels/823813159592001537?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LAION</a> Discord</h2>
<ul>
<li><strong>GDPR Complaint Challenges AI Birthdays</strong>: An EU privacy advocate has filed a <a href="https://www.politico.eu/article/chatgpts-hallucinations-get-eu-privacy-complaint/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GDPR complaint</a> after an AI model incorrectly estimated his birthday, triggering discussions on the potential implications for AI operations in Europe.</li>
<li><strong>Mysterious GPT-5 Speculations</strong>: Amidst rumors of a new GPT-5 model release, the community debates inconsistent test outcomes and the absence of official communication or leaderboard recognitions, questioning the framework's evasiveness in generating hallucinations.</li>
<li><strong>Llama3 70B's Slow Performance Spotlight</strong>: AI engineers are troubleshooting the <a href="https://rentry.co/GPT2?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Llama3 70B</a> model's unexpectedly sluggish token generation rate of 13 tokens per second on a dual 3090 rig, delving into possible hardware and configuration enhancements.</li>
<li><strong>Exllama Library Outraces Rivals</strong>: Users endorse <strong>Exllama</strong> for its fast performance on language model tasks and suggest utilizing the <a href="https://dct.openempathic.ai/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">TabbyAPI</a> repository for simpler integrations, naming it a superior choice compared to other libraries.</li>
<li><strong>Research Breakthrough with OpenCLIP</strong>: The successful application of <strong>OpenCLIP</strong> to cardiac ultrasound analysis has been published, highlighting the rigorous revision process and a move towards novel, non-zero-shot techniques, with the study available <a href="https://doi.org/10.1038/s41591-024-02959-y?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>; meanwhile <em>r/StableDiffusion</em> is back online and a relevant CLIP training repository is discussed in the context of Reddit's recent API changes, found at <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgyjvt/github_zer0intclipfinetune_or_sdxl_training_the/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">this Reddit discussion</a>.</li>
</ul>
<hr/>
<h2 id="openai-discord"><a href="https://discord.com/channels/974519864045756446?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">OpenAI</a> Discord</h2>
<p><strong>Memory Lane with Upscaled ChatGPT Plus</strong>: ChatGPT Plus now allows users to command the AI to remember specific contexts, which can be toggled on and off in settings; the rollout has not reached Europe or Korea yet. Plus, both Free and Plus users gain enhanced data control, including a 'Temporary Chat' option that discards conversations immediately after they end.</p>
<p><strong>AI Ghosh-darn Curiosity and Camera Tricks</strong>: Discussions swung from defining AI curiosity and sentience with maze challenges to the merits of DragGAN altering photos with new angles. Meanwhile, the Llama-3 8B model emerged, flaunting its long-context skills and is accessible at <a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Hugging Face</a>, but the community still wrestled with the accessibility of advanced AI technologies and the dream of inter-model collaboration.</p>
<p><strong>GPT-4: Bigger and Maybe Slower?</strong>: The community dove into the attributes of GPT-4, noting its significantly larger size than the 3.5 version and raising concerns about whether its scale may affect processing speed. Meanwhile, the possibility of mass-deleting archived chats was also a topic of concern.</p>
<p><strong>Prompt Engineering's Competitive Edge</strong>: Prompt engineering drew attention, with suggestions for competitions to hone skills, and 'meta prompting' via GPT Builder to refine AI output. The group agreed that positive prompting trumps listing prohibites, and wrestled with optimizing regional Spanish nuances in AI text generation.</p>
<p><strong>Cross-Channel Theme of Prompting Excellence</strong>: Both AI discussions and API channels tackled prompt engineering, with meta-prompting techniques at the spotlight, indicating a shift toward more efficient prompting strategies that might decrease the need for competitions. Navigating the complexities of multilingual outputs also emerged as a shared challenge, emphasizing adaptation rather than prohibition.</p>
<hr/>
<h2 id="openaccess-ai-collective-axolotl-discord"><a href="https://discord.com/channels/1104757954588196865?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">OpenAccess AI Collective (axolotl)</a> Discord</h2>
<p><strong>LLaMA 3 Struggles with Quantization</strong>: <strong>LLaMA 3</strong> is observed to have significant performance degradation from quantization processes, more so than its predecessor, which might be due to its expansive training on 15T tokens capturing very nuanced data relations. A critique within the community called a study on quantization sensitivity "worthless," suggesting that the issue may be more related to model training approaches rather than size; the critique referenced a <a href="https://arxiv.org/abs/2311.16452?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">study on arXiv</a>.</p>
<p><strong>Riding the Zero Train</strong>: The Guild discussed <strong>Huggingface's ZeroGPU</strong>, a beta feature offering free access to multi-GPU resources like Nvidia A100, with some members expressing regret at missing early access. A member has <a href="https://huggingface.co/zero-gpu-explorers?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">shared access</a> and is open to suggestions for testing on the platform.</p>
<p><strong>Finetuning Finesse</strong>: Advised against fine-tuning <code>meta-llama/Meta-Llama-3-70B-Instruct</code>, it was suggested that members start with smaller models like 8B to sharpen their fine-tuning skills. The Guild clarified how to convert a fine-tuning dataset from OpenAI to ShareGPT format, and provided guidance with Python code for dataset transformation.</p>
<p><strong>Tutorial Spreads Its Wings</strong>: A helpful <a href="https://github.com/dstackai/dstack/blob/master/examples/fine-tuning/axolotl/README.md?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">tutorial was shared</a> on fine-tuning Axolotl using dstack, showing the community's knack for collaboratively improving practices. Appreciation was conveyed by members, noting the tutorial's ease of use.</p>
<p><strong>Axolotl Adaptations</strong>: Discussing the fine-tuning of <em>command-r</em> within Axolotl and related format adaptations, a member shared an <a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1547?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">untested pull request</a> relating to this topic, while also noting its prematurity for merging. In addition, there's uncertainty about the support for phi-3 format and the implementation standing of <em>sample packing</em> feature, indicating a need for further clarification or development.</p>
<hr/>
<h2 id="latent-space-discord"><a href="https://discord.com/channels/822583790773862470?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Latent Space</a> Discord</h2>
<ul>
<li><strong>Memary: An Autonomous Agent's Long-term Memory</strong>: The <a href="https://github.com/kingjulio8238/memary?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Memary</a> project on GitHub has introduced a new approach for long-term memory in autonomous agents, using document similarity searches over traditional knowledge graphs.</li>
</ul>
<ul>
<li><strong>The GPT-2 Chatbot Enigma</strong>: Intense debates have emerged on a <a href="https://chat.lmsys.org/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GPT2-chatbot</a> that showcases surprisingly advanced capabilities, leading to speculation that it might be a finetuned version of OpenAI's GPT-2.</li>
</ul>
<ul>
<li><strong>Can Decentralized Training Compete with Big Tech?</strong>: <a href="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Prime Intellect's blog post</a> discusses decentralized training as a plausible avenue for open-source artificial intelligence to compete with the proprietary models developed by large corporations with extensive GPU resources.</li>
</ul>
<ul>
<li><strong>Redefining LLMs with Modular Context and Memory</strong>: Discussions are emerging that suggest a paradigm shift towards designing autonomous agents with modularized shared context and memory capabilities for reasoning and planning, stepping away from the reliance on standalone large language models (LLMs).</li>
</ul>
<ul>
<li><strong>Educational Resources for Aspiring AI Enthusiasts</strong>: For those seeking to learn AI fundamentals, community members recommended resources including neural network tutorials such as the one on <a href="https://youtu.be/aircAruvnKk?feature=shared&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">YouTube</a> and courses like <em>Learn Prompting</em>, providing a glimpse into AI engineering and prompt engineering basics.</li>
</ul>
<hr/>
<h2 id="openinterpreter-discord"><a href="https://discord.com/channels/1146610656779440188?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">OpenInterpreter</a> Discord</h2>
<p><strong>OS Start-up with a Vision</strong>: A user faced challenges attempting to <strong>launch OS mode with a local vision model for Moondream</strong> and received gibberish output, but the discussion did not yield a solution or direct advice.</p>
<p><strong>Integration Achievements</strong>: An exciting integration of <strong>OpenInterpreter</strong> outputs into <strong>MagicLLight</strong> was mentioned, with anticipation for a future code release and pull request including a <code>stream_out</code> function hook and <code>external_input</code>.</p>
<p><strong>Hardware Hiccup Help</strong>: Queries about running <strong>OpenInterpreter on budget hardware</strong> like a Raspberry Pi Zero were brought up alongside requests for assistance with <strong>debugging startup issues</strong>. Community members offered to help with troubleshooting once more details were provided.</p>
<p><strong>Push Button Programming</strong>: An individual fixed an external push button issue on <strong>pin 25</strong> and shared a <a href="https://discord.com/channels/openinterpreter/01?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">code snippet</a>, also getting community confirmation that the fix was effective.</p>
<p><strong>Volume Up on Tech Talk</strong>: There were mixed opinions on whether tech YouTubers have a grasp on AI technologies while advising on options for increasing speaker volume, including using <strong>M5Unified</strong> or an <a href="https://www.amazon.com/dp/B01DKAI51M?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">external amplifier</a>.</p>
<hr/>
<h2 id="tinygrad-george-hotz-discord"><a href="https://discord.com/channels/1068976834382925865?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">tinygrad (George Hotz)</a> Discord</h2>
<ul>
<li><strong>Peek into Tinygrad's Inner Workings</strong>: The <a href="https://github.com/tinygrad/tinygrad/tree/master?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">tinygrad GitHub repository</a> was recommended to someone curious about <strong>tinygrad</strong>, an educational project for enthusiasts of PyTorch and micrograd. Another community member inquired about graph visualization, leading to the suggestion to use the <code>GRAPH=1</code> environment variable to generate diagrams for addressing backward pass issues <a href="https://github.com/tinygrad/tinygrad/issues/3572?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">#3572</a>.</li>
</ul>
<ul>
<li><strong>The Discovery of Learning Resources</strong>: The community explored learning AI with TinyGrad through resources like <a href="https://github.com/unknownusername504/MicroGrad?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">MicroGrad</a> and <a href="https://minitorch.github.io/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">MiniTorch</a>, with MiniTorch being singled out as particularly useful for understanding deep learning systems. The "<a href="https://tinygrad.github.io/tinygrad/quickstart/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">tinygrad Quick Start Guide</a>" was highlighted as a starting point for beginners.</li>
</ul>
<ul>
<li><strong>Taking the Symbolic Route</strong>: Implementing a symbolic mean operation in TinyGrad brought up discussions about LazyBuffer's interaction with data types and the practicality of variable caching for operations like <code>sum</code> and <code>mean</code>. A <a href="https://github.com/tinygrad/tinygrad/pull/1552?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">pull request</a> demonstrated symbolic code execution while further GitHub compare views tackled the development of symbolic mean with variables at <a href="https://github.com/tinygrad/tinygrad/compare/master...davidjanoskyrepo:tinygrad:symbolic-mean-var-pull?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">tinygrad symbolic-mean-var-pull</a> and <a href="https://github.com/tinygrad/tinygrad/compare/86d90511cee2%5E...97a2d44d9840?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GitHub changes by gh</a>.</li>
</ul>
<ul>
<li><strong>Bounty Hunting for Mean Solutions</strong>: The community sought guidance for bounty challenges related to <em>"Mean of symbolic shape"</em> and <em>"Symbolic arrange"</em>. Discussion centered around the implementation nuances and practical approaches to these problems in the TinyGrad environment.</li>
</ul>
<ul>
<li><strong>Cluster of Curiosities</strong>: A spontaneous question about how a member discovered the Discord server triggered a chain of speculations, with the respondent admitting they did not recall the method of encounter, adding a touch of mystery to the channel discourse.</li>
</ul>
<hr/>
<h2 id="cohere-discord"><a href="https://discord.com/channels/954421988141711382?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Cohere</a> Discord</h2>
<ul>
<li><strong>Single-Site Restrictions in Command-R</strong>: <strong>API Command R+</strong>'s <code>web_search</code> tool only allows for one website at a time, and the workaround discussed involves separate <strong>API calls for each site</strong>.</li>
</ul>
<ul>
<li><strong>Feature Request Frenzy</strong>: Engineers are eager for <strong>Command-R</strong> improvements with an emphasis on <strong>Connectors</strong>, including multi-website searches and extra parameter control; to get familiar with current capabilities, refer to the <a href="https://docs.cohere.com/reference/chat?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Cohere Chat Documentation</a>.</li>
</ul>
<ul>
<li><strong>Multi-Step Connector Capabilities Currently Limited</strong>: It was confirmed that <strong>multi-step tool use</strong> with <strong>connectors</strong> isn't yet possible within <strong>Command-R</strong>.</li>
</ul>
<ul>
<li><strong>Generate Option Gone Missing</strong>: Queries rose regarding the disappearance of 'Generate' for fine-tuning models from the dashboard, leaving its future presence in question.</li>
</ul>
<ul>
<li><strong>Strategic Embedding Sought</strong>: Discussion revolved around cost-effective strategies for keeping data fresh for embeddings, with a focus on reindexing only modified segments.</li>
</ul>
<ul>
<li><strong>Nordic Networking Noted</strong>: Members highlighted operations within <strong>Sweden</strong> using <strong>Cohere</strong> and existing connections through the company <strong>Omegapoint</strong>, spanning both Sweden and Norway.</li>
</ul>
<hr/>
<h2 id="langchain-ai-discord"><a href="https://discord.com/channels/1038097195422978059?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LangChain AI</a> Discord</h2>
<ul>
<li><strong>Gemini Experience Wanted &amp; Observability Tools Sought</strong>: Users in the <strong>general</strong> channel are seeking expertise in <strong>Gemini 1.0 or 1.5 models</strong> and discussing available tools for Large Language Model (LLM) observability, with interest in self-hosted, open-source options compatible with <strong>LlamaIndex</strong>. Meanwhile, there's a push for enhanced SQL security when connecting to OpenAI models and a technical discussion on integrating <strong>autoawq</strong> with <strong>LangGraph</strong> for high-speed AI agent inference using <strong>exllamav2 kernels</strong>.</li>
</ul>
<ul>
<li><strong>Asynchronous Adventures and Google Drive Gyrations</strong>: Within the <strong>langserve</strong> channel, a user is challenged by the lack of async support in <strong>AzureSearchVectorStoreRetriever</strong> and is considering whether to push for an async feature or to craft an async wrapper themselves. Separately, the discussion turned to the nuances of using Google Drive libraries and the importance of setting the drive key as an environment variable.</li>
</ul>
<ul>
<li><strong>Showcase Extravaganza &amp; Plugin Revelation</strong>: In <strong>share-your-work</strong>, there's an insights-filled trip back to <strong>GPT-1</strong>'s role in initiating current LLM advancements and several LangChain use cases, including a "D-ID Airbnb Use Case" and a "Pizza Bot", both featured on <strong>YouTube</strong>. The <strong>VectorDB plugin for LM Studio</strong> also made an appearance, aiming to bolster ChromaDB vector databases in server mode, while <strong>QuickVid</strong> was launched to deliver YouTube video summaries and fact checks.</li>
</ul>
<ul>
<li><strong>RAG Agents Go Multilingual &amp; Private</strong>: Tutorials channel is sharing resources for interested French speakers in building RAG assistants with <strong>LangChain, Mistral Large</strong>, and <strong>Llamaindex</strong>. Another guide demonstrates enhancing <strong>llama3</strong>'s performance by incorporating personal knowledge bases to create agentic RAGs, revealing potential for more localized and data-rich AI capabilities.</li>
</ul>
<hr/>
<h2 id="alignment-lab-ai-discord"><a href="https://discord.com/channels/1087862276448595968?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Alignment Lab AI</a> Discord</h2>
<p><strong>Alert: Illicit Spam Floods Channels</strong>: Numerous messages across different channels promoted explicit material involving "18+ Teen Girls and OnlyFans leaks," accompanied by a <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Discord invite link</a>. All messages were similar in nature, using emojis and <code>@everyone</code> to garner attention, and are flagrant violations of Discord's community guidelines.</p>
<p><strong>Prompt Moderation Action Required</strong>: The repeated posts are indicative of a coordinated spam attack necessitating immediate moderation intervention. Each message invariably linked to an external Discord server, potentially baiting users into exploitative environments.</p>
<p><strong>Engineer Vigilance Advocacy</strong>: Members are encouraged to report such posts to maintain professional decorum. The content breaches both legal and ethical boundaries and does not align with the guild's purpose or standards.</p>
<p><strong>Discord Server Safety at Risk</strong>: The proliferation of these messages highlights a concern for server security and member safety. The spam suggests a compromise of server integrity, underscoring the need for robust anti-spam measures.</p>
<p><strong>Community Urged to Disregard Suspicious Links</strong>: Engineers and members are urged to avoid engaging with or clicking on unsolicited links. Such practices help safeguard personal information and the community's credibility while adhering to legal and ethical codes.</p>
<hr/>
<h2 id="ai-stack-devs-yoko-li-discord"><a href="https://discord.com/channels/1122748573000409160?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">AI Stack Devs (Yoko Li)</a> Discord</h2>
<ul>
<li><strong>Game Devs Gear Up for Gamification</strong>: Rosebud AI's <strong>Game Jam</strong> invites creators to fashion 2D browser-based games using <strong>Phaser JS</strong> with a $500 prize pool, and an <strong>AIxGames Meetup</strong> is slated for Thursday in SF to bring together AI and gaming professionals <a href="https://partiful.com/e/TwvC5qxskuPGqiliMj5f?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">RSVP here</a>.</li>
</ul>
<ul>
<li><strong>NPC Revolution with LLMs</strong>: A developer has introduced LLM-powered NPC models and an inference stack, available on <a href="https://github.com/GigaxGames/gigax?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GigaxGames at GitHub</a>, promising an LLM single call feature and open-weights on <a href="https://huggingface.co/Gigax?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Huggingface's Hub</a>, despite a hiccup with a broken API access link.</li>
</ul>
<ul>
<li><strong>Grappling with Gaming NPC Realities</strong>: Developers are experimenting with <em>output compression</em>, minimized model calls, and smaller models to improve NPC runtime performance and grappling with NPCs that break the fourth wall, with the <em>Claude 3</em> model showing promise in empathetic interactions for better gaming experiences.</li>
</ul>
<ul>
<li><strong>Blog Teased on LLMs for NPCs</strong>: There's an upcoming blog post chronicling the struggles and triumphs in finetuning LLMs for dynamic NPC behavior, pointing towards new strategies that could be shared within the community.</li>
</ul>
<ul>
<li><strong>Navigating Windows Woes with Convex</strong>: The <strong>Convex local</strong> setup does not play nice with Windows, causing users to encounter sticking points, though potential solutions like <strong>WSL</strong> or <strong>Docker</strong> have been floated, and a Windows-compatible Convex is reportedly on the horizon.</li>
</ul>
<hr/>
<h2 id="skunkworks-ai-discord"><a href="https://discord.com/channels/1131084849432768614?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Skunkworks AI</a> Discord</h2>
<p><strong>Binary Quest in HaystackDB</strong>: Curiosity piqued about the potential use of <strong>2-bit embeddings</strong> in <a href="https://github.com/carsonpo/haystackdb?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">HaystackDB</a>, while <strong>Binary Quantized (BQ)</strong> indexing becomes a spotlight topic due to its promise of leaner and faster similarity searches.</p>
<p><strong>The Rough Lane of Fine-Tuning LLaMA-3</strong>: Engineers face a bumpy road with <strong>LLaMA-3 fine-tuning</strong>, battling issues from the model neglecting <strong>EOS token generation</strong> to embedding layer compatibility across bit formats.</p>
<p><strong>Perplexed by Perplexity</strong>: The community debates fine-tuning <strong>LLaMA-3 for perplexity</strong>, suggesting that performance may not surpass the base model, possibly due to tokenizer-related complications.</p>
<p><strong>Shining a Light on LLaMA-3 Improvement</strong>: A beacon of hope shines as one user successfully fine-tunes <strong>LLaMA-3</strong> with model-specific prompt strategies, sparking interest with a GitHub <a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1553?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">pull request</a> for the collective's scrutiny.</p>
<p><strong>Off-Topic Oddities Go Unsummarized</strong>: A solitary link in <strong>#off-topic</strong> stands alone, contributing no technical discussion to the collective knowledge pool.</p>
<hr/>
<h2 id="mozilla-ai-discord"><a href="https://discord.com/channels/1089876418936180786?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Mozilla AI</a> Discord</h2>
<ul>
<li><strong>Mozilla's AI Talent Search</strong>: Mozilla AI is actively recruiting for various roles, with job opportunities available for those interested in contributing to their initiatives. For those looking to join the team, they can find more information and apply using the provided <a href="https://discord.com/channels/1089876418936180786/1230938514955436242/1234870020916510823?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">link</a>.</li>
</ul>
<ul>
<li><strong>LM-buddy: Eval Tool for Language Models</strong>: The release of Lm-buddy, an open-source evaluation tool for language models, stands to improve the assessment of LLMs. Contributors and users are encouraged to engage with the project through the given <a href="https://discord.com/channels/1089876418936180786/1230938514955436242/1234589599733518378?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">link</a>.</li>
</ul>
<ul>
<li><strong>Prometheus Benchmarks LLMs in Judicial Roles</strong>: The Prometheus project has demonstrated the potential for Local Large Language Models (LLMs) to act as arbiters, a novel concept sparking discussion. Interested parties can join the conversation about this application by following the <a href="https://discord.com/channels/1089876418936180786/1234890301143912599/1234890301143912599?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">link</a>.</li>
</ul>
<ul>
<li><strong>In-Depth Code Analysis Request for LLaMA</strong>: An engineer has noted that token generation in llama.cpp/llamafile is a bottleneck, with matrix-vector multiplications consuming 95% of the inference time for LLaMA2. This has led to speculation on whether loop unrolling contributes to the 30% better performance of llama.cpp over alternative implementations.</li>
</ul>
<ul>
<li><strong>LLaMA Tales of Confusion and Compatibility</strong>: The Discord discussed amusing mix-ups and pseudonymous confusion with LLaMA parameters. Additionally, challenges were shared regarding the integration with Plush-for-comfyUI and LLaMA3's compatibility issues on M1 Macbook Air, promising priority testing for the M1 once current LLaMA3 issues are addressed.</li>
</ul>
<hr/>
<h2 id="interconnects-nathan-lambert-discord"><a href="https://discord.com/channels/1179127597926469703?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Interconnects (Nathan Lambert)</a> Discord</h2>
<ul>
<li><strong>OLMo Deep Dive Shared by AI Maverick</strong>: A detailed talk on "OLMo: Findings of Training an Open LM" by Hanna Hajishirzi was posted, featuring her work at the <a href="https://youtu.be/qFZbu2P1vZ8?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Open-Source Generative AI Workshop</a>. Her pace of presenting substantive content on OLMo, Dolma, Tulu, etc., was noted to be rapid, possibly challenging for students to digest, thus reflecting her expertise and the extensive research involved in these projects.</li>
</ul>
<ul>
<li><strong>RL in LM-Based Systems Exposed</strong>: Key takeaways from John Schulman's discussion on reinforcement learning for language model-based systems were encapsulated in a GitHub <a href="https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Gist</a>, providing engineers with a compressed synthesis of his approach and findings.</li>
</ul>
<ul>
<li><strong>AI Leaderboard Limitations Pointed Out</strong>: A <a href="https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog post</a> by Sayash Kapoor and Benedikt Stroebl challenges the effectiveness of AI leaderboards for code generation, highlighting LLM debugger's (LDB) high operational cost despite its top rankings, calling into question the utility of such benchmarks in the face of significant expenses.</li>
</ul>
<ul>
<li><strong>SnailBot</strong>: A mention for an update or news related to SnailBot was made but lacked further information or context for a substantive summary.</li>
</ul>
<ul>
<li><strong>Notice</strong>: Based on the provided snippets from the Discord guild there is no additional content that warrants a summary, indicating that these messages may have been part of a larger context or subsequent discussions that were not included.</li>
</ul>
<hr/>
<h2 id="llm-perf-enthusiasts-ai-discord"><a href="https://discord.com/channels/1168579740391710851?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LLM Perf Enthusiasts AI</a> Discord</h2>
<ul>
<li><strong>Gamma Seeking AI Wizard</strong>: <strong>Gamma</strong> is hiring an <strong>AI engineer</strong> to drive innovation in AI-driven presentation and website design, with a focus on prompt engineering, metrics, and model fine-tuning; details are at <a href="https://careers.gamma.app/ai-engineer?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Gamma Careers</a>. Despite the need for an in-person presence in <strong>San Francisco</strong>, the role is open to those with strong Large Language Model (LLM) skills even if they lack extensive engineering experience.</li>
</ul>
<ul>
<li><strong>AI-Powered Enterprise on Growth Fast-track</strong>: Flaunting over <strong>10 million users</strong> and <strong>$10M+ in funding</strong>, <strong>Gamma</strong> is looking for an AI engineer to help sustain its growth while enjoying a hybrid work culture within its <strong>profitable</strong> and compact 16-member team.</li>
</ul>
<ul>
<li><strong>The Case of GPT-4.5 Speculations</strong>: A tweet by <strong>@phill__1</strong> hinted at gpt2-chatbot possessing 'insane domain knowledge,' leading to speculation that it might represent the capabilities of a <strong>GPT-4.5</strong> version <a href="https://x.com/phill__1/status/1784964135920235000?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">phill__1's observation</a>.</li>
</ul>
<ul>
<li><strong>Chatbot Causes Community Commotion</strong>: The engineer community is abuzz with the idea that the gpt2-chatbot could be an unintentional glimpse at the prowess of <strong>GPT-4.5</strong>, with a member succinctly endorsing it as "good".</li>
</ul>
<hr/>
<h2 id="datasette-llm-simonw-discord"><a href="https://discord.com/channels/823971286308356157?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Datasette - LLM (@SimonW)</a> Discord</h2>
<ul>
<li><strong>Snazzy Syntax-Nixing for Code-Gen</strong>: A user discussed the concept of incorporating a <strong>custom grammar</strong> within a language model to prioritize identifying semantic rather than syntactic errors during code generation.</li>
</ul>
<ul>
<li><strong>Data-fied Dropdowns for Datasette</strong>: Suggestions were exchanged on improving <strong>Datasette's UX</strong>, including a front-page design that features dropdown menus to enable users to generate summary tables based on selected parameters, such as country choice.</li>
</ul>
<ul>
<li><strong>UX Magic with Direct Data Delivers</strong>: Members proposed enhanced UX solutions for <strong>Datasette</strong>, including dynamically updating URLs or building homepage queries adjusted by user selection to streamline access to relevant data.</li>
</ul>
<hr/>
<h2 id="discoresearch-discord"><a href="https://discord.com/channels/1178995845727785010?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">DiscoResearch</a> Discord</h2>
<ul>
<li><strong>Loading Anomalies Enigma</strong>: A conversation highlighted that a process <strong>loads in 3 seconds on a local machine</strong> but faces delays when run through job submission, implying that the issue may not be related to storage but perhaps environment-specific overheads.</li>
<li><strong>Llama Trumps GPT-4 in Language Benchmark</strong>: <strong>Llama 3</strong> outperformed <strong>GPT-4</strong> in the <strong>ScanEval benchmark for German NLG</strong>, as shown on <a href="https://scandeval.com/german-nlg/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ScandEval's leaderboard</a>.</li>
</ul>
<hr/>
<p>The <strong>AI21 Labs (Jamba) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr/>
<h1 id="part-2-detailed-by-channel-summaries-and-links">PART 2: Detailed by-Channel summaries and links</h1>
<p><strong>CUDA MODE ‚ñ∑ #<a href="https://discord.com/channels/1189498204333543425/1189607595451895918/1234899266837938176?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">triton</a></strong> (1 messages): </p>
<ul>
<li><strong>Clarifying Triton Block Size Limits</strong>: A member inquired about the maximum size of a <strong>Triton block</strong>, noting that while they can create blocks with 4096 elements, they cannot do the same with 8192, suggesting there's a discrepancy with the expected CUDA limits.</li>
</ul>
<hr/>
<p><strong>CUDA MODE ‚ñ∑ #<a href="https://discord.com/channels/1189498204333543425/1189607726595194971/1234454843696087122?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">cuda</a></strong> (8 messagesüî•): </p>
<ul>
<li><strong>Seeking Flash Attention Code</strong>: A user inquired about how to download lecture12 of flash attention code presented by Thomas Viehmann; no resolution to the query was provided in the chat.</li>
<li><strong>Understanding CUDA Reductions</strong>: A member worked out their confusion regarding row-wise versus column-wise reductions in CUDA, realizing the performance difference is due to the (non)coalesced memory accesses and clarified their own question.</li>
<li><strong>Integer Division in Kernel Code</strong>: An optimization discussion took place regarding replacing integer division with bit shifts; it was suggested that nvcc or ptxas may optimize division when divisors are powers of 2, and a <a href="https://godbolt.org/z/9K9Gf1v6P?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">compiler explorer link</a> was provided for further experimentation.</li>
<li><strong>CUDA Checkpointing Resource Shared</strong>: An external GitHub resource for CUDA checkpoint and restore utility, <a href="https://github.com/NVIDIA/cuda-checkpoint?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">NVIDIA/cuda-checkpoint</a>, was shared without further discussion.</li>
<li><strong>Comparing CUTLASS and CuBLAS Performance</strong>: A member benchmarked matrix multiplication performance comparing CuBLAS and CUTLASS, reporting that CUTLASS outperforms CuBLAS in a standalone profiler, but when integrated into Python the performance gains disappear, as shared in an article at <a href="https://www.thonking.ai/p/strangely-matrix-multiplications?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Thonking AI's post about matrix multiplications</a>.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.thonking.ai/p/strangely-matrix-multiplications?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data! [short]</a>: Great minds discuss flops per watt.</li><li><a href="https://github.com/NVIDIA/cuda-checkpoint?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - NVIDIA/cuda-checkpoint: CUDA checkpoint and restore utility</a>: CUDA checkpoint and restore utility. Contribute to NVIDIA/cuda-checkpoint development by creating an account on GitHub.</li><li><a href="https://godbolt.org/z/9K9Gf1v6P?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Compiler Explorer - CUDA C++ (NVCC 11.7.0)</a>: #include &amp;lt;algorithm&amp;gt; #include &amp;lt;cassert&amp;gt; #include &amp;lt;cstdio&amp;gt; #include &amp;lt;cstdlib&amp;gt;  __global__ void sgemmVectorize(int M, int N, int K, float alpha, f...
</li>
</ul>
</div>
<hr/>
<p><strong>CUDA MODE ‚ñ∑ #<a href="https://discord.com/channels/1189498204333543425/1189607750876008468/1234490936143249428?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">torch</a></strong> (4 messages): </p>
<ul>
<li><strong>Curiosity About Double Kernel Launches</strong>: A member inquired as to why, during matrix multiplication in PyTorch, the profiler sometimes indicates two kernel launches.</li>
<li><strong>Clarification on PyTorch <code>linear</code> Function</strong>: Another member clarified that <code>linear</code> in PyTorch does include a transpose operation by default on the input, which might not lead to a performance difference.</li>
</ul>
<hr/>
<p><strong>CUDA MODE ‚ñ∑ #<a href="https://discord.com/channels/1189498204333543425/1189861061151690822/1234626145421365259?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">algorithms</a></strong> (2 messages): </p>
<ul>
<li><strong>Introducing Effort Engine for LLMs</strong>: The Effort Engine algorithm was shared, with the capability of dynamically adjusting the computational effort during LLM inference. At <strong>50% effort</strong>, it reaches speeds comparable to standard matrix multiplications on Apple Silicon, and at <strong>25% effort</strong>, it's twice as fast with minimal quality loss, as per the details on <a href="https://kolinko.github.io/effort?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">kolinko.github.io/effort</a>.</li>
</ul>
<ul>
<li><strong>Effort Engine's Approach to Model Inference</strong>: This new technique allows for selectively loading important weights, potentially enhancing speed without substantial quality degradation. It's implemented for <strong>Mistral</strong> and should be compatible with other models after some conversion and precomputation, with the implementation available on <a href="https://github.com/kolinko/effort?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GitHub</a>.</li>
</ul>
<ul>
<li><strong>FP16 Only Implementation and Room for Improvement</strong>: The Effort Engine is currently available for <strong>FP16 implementations only</strong>, and while the multiplications are fast, improvements are needed in other areas such as softmax and attention summation operations. </li>
</ul>
<ul>
<li><strong>Potential Limitations of Effort Engine Explored</strong>: A member highlighted that while the Effort Engine's approach is innovative, it might share limitations with activation sparsity methods, especially in batched computations with batch size greater than one due to misaligned activation magnitudes.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://kolinko.github.io/effort/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Effort Engine</a>: A possibly new algorithm for LLM Inference. Adjust smoothly - and in real time - how many calculations you'd like to do during inference.</p>
<hr/>
<p><strong>CUDA MODE ‚ñ∑ #<a href="https://discord.com/channels/1189498204333543425/1190208177829068860/1234455593343783014?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">jobs</a></strong> (1 messages): </p>
<ul>
<li><strong>InstaDeep is Hiring ML Engineers</strong>: InstaDeep Research is actively seeking <strong>Machine Learning Engineers</strong> who are passionate about <strong>high-performance ML engineering</strong> and its real-world applications. Candidates who excel in building custom CUDA kernels, state-of-the-art model architectures, quantisation, and distributed training should <a href="https://www.instadeep.com/job-offer/92900fa3-5501-4506-a63f-cebee958fc6f/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">reach out for opportunities</a>.</li>
</ul>
<ul>
<li><strong>Join a Collaborative Innovator</strong>: InstaDeep offers a stimulating, collaborative environment to work on real-life decision-making and technology products, and encourages applications from talented individuals eager to make a transformative impact. The company emphasizes innovation and real-world applications in <strong>Bio AI</strong> and <strong>Decision Making AI</strong>.</li>
</ul>
<ul>
<li><strong>Seeking Interns and Multi-Applicants</strong>: Individuals interested in internships or pursuing more than one job opportunity at InstaDeep can <a href="https://www.instadeep.com/internships?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">explore internship opportunities</a> and apply to multiple positions provided they have the relevant skills, though it is advised not to apply to more than two to avoid application rejection.</li>
</ul>
<ul>
<li><strong>Reapplication Guidelines Suggested</strong>: Those who applied previously and were not selected are recommended to wait before reapplying, particularly if they applied within the last six months, indicating a period of consideration for changes in applicant profile or company needs.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://www.instadeep.com/job-offer/92900fa3-5501-4506-a63f-cebee958fc6f/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Job Offer | InstaDeep - Decision-Making AI For The Enterprise</a>: no description found</p>
<hr/>
<p><strong>CUDA MODE ‚ñ∑ #<a href="https://discord.com/channels/1189498204333543425/1198769713635917846/1234509189091426334?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">youtube-recordings</a></strong> (2 messages): </p>
<ul>
<li><strong>No Updates on Progress</strong>: A member confirmed that there have been <strong>no new developments</strong> to report currently.</li>
<li><strong>Profiling Techniques on Video</strong>: A <a href="https://youtu.be/SKV6kDk1s94?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">YouTube video titled "Lecture 16: On Hands Profiling"</a> was shared in the chat, providing a resource for learning about profiling techniques, although no specific description was provided.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://youtu.be/SKV6kDk1s94?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Lecture 16: On Hands Profiling</a>: no description found</p>
<hr/>
<p><strong>CUDA MODE ‚ñ∑ #<a href="https://discord.com/channels/1189498204333543425/1208496482005549086/1234630522106282065?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ring-attention</a></strong> (1 messages): </p>
<ul>
<li><strong>Llama-3 Hits New Context Length Highs</strong>: Gradient has released <a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Llama-3 8B Gradient Instruct 1048k</a> that extends the context length from 8k to over 1048k. The achievement demonstrates that state-of-the-art language models can adapt to long contexts with minimal training adjustments.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">gradientai/Llama-3-8B-Instruct-Gradient-1048k ¬∑ Hugging Face</a>: no description found</p>
<hr/>
<p><strong>CUDA MODE ‚ñ∑ #<a href="https://discord.com/channels/1189498204333543425/1215328286503075953/1234635788642287696?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">off-topic</a></strong> (1 messages): </p>
<ul>
<li><strong>CUTLASS: A Dance of Integers</strong>: A member observed that <a href="https://developer.nvidia.com/cutlass?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">CUTLASS</a>, despite being a linear algebra library, primarily handles integer operations and index manipulations before calling advanced linear algebra routines. This characteristic rationalizes its nature as a <strong>header-only library</strong> without the need for complex linking.</li>
</ul>
<hr/>
<p><strong>CUDA MODE ‚ñ∑ #<a href="https://discord.com/channels/1189498204333543425/1227345713348870156/1234443683856650250?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">llmdotc</a></strong> (721 messagesüî•üî•üî•): </p>
<ul>
<li><strong>CUDA Programming Discussions &amp; Packed128 Types</strong>: There was a detailed debate about the usage of <code>Packed128</code> custom struct for optimizing memory access patterns, addressing both <strong>reads and writes</strong>. Special attention was given to the proper construction and utilization of <code>Packed128</code>, and whether to use explicit typecasting with <strong>floatX</strong> and <strong>BF16</strong> inside kernels. </li>
</ul>
<ul>
<li><strong>Mixed-Precision Strategy Concerns</strong>: There's concern about the impact of using BF16 throughout the entire model and whether <strong>stochastic rounding</strong> might affect training convergence. There are plans to compare the loss metrics between <strong>llm.c</strong>'s BF16 approach and standard PyTorch mixed-precision implementations.</li>
</ul>
<ul>
<li><strong>Profiling &amp; Debugging</strong>: A member added <strong>NVTX</strong> contexts for better profiling with NSight Compute, enabling more accurate GPU timings. A member observed that <strong>AdamW</strong> kernel may need optimization regarding FP32 atomics and scratch storage usage.</li>
</ul>
<ul>
<li><strong>Tooling &amp; Infrastructure for Benchmarking</strong>: Members discussed the potential utility of external platforms like Modal for running benchmarks on standardized specs, specifically the benefits and limitations of <strong>Modal</strong> with regard to profiling tools like <strong>nvprof</strong> and <strong>nsys</strong>.</li>
</ul>
<ul>
<li><strong>PR Reviews Prepared for Merge &amp; CI Suggestions</strong>: The channel had several PRs prepared for merging, mostly pertaining to the f128 and Packed128 optimizations for various kernels. The need for keeping branch <strong>documentation updated</strong>, <strong>-Wall compilation</strong>, and a <strong>CI check</strong> to ensure python and C implementations deliver similar results were also highlighted.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Nvidia‚Äôs H100: Funny L2, and Tons of Bandwidth</a>: GPUs started out as devices meant purely for graphics rendering, but their highly parallel nature made them attractive for certain compute tasks too. As the GPU compute scene grew over the past cou‚Ä¶</li><li><a href="https://nvidia.github.io/cccl/libcudacxx/extended_api/memory_access_properties/associate_access_property.html?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">cuda::associate_access_property</a>: CUDA C++ Core Libraries</li><li><a href="https://arxiv.org/abs/2310.18313?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">FP8-LM: Training FP8 Large Language Models</a>: In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM traini...</li><li><a href="https://nvidia.github.io/cccl/libcudacxx/extended_api/asynchronous_operations/memcpy_async.html?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">cuda::memcpy_async</a>: CUDA C++ Core Libraries</li><li><a href="https://www.thonking.ai/p/strangely-matrix-multiplications?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data! [short]</a>: Great minds discuss flops per watt.</li><li><a href="https://developer.nvidia.com/nccl/nccl2-download-survey?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Log in</a>: no description found</li><li><a href="https://godbolt.org/z/hME5EqYrr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Compiler Explorer - CUDA C++ (NVCC 12.2.1)</a>: #include &amp;lt;cuda/barrier&amp;gt; #include &amp;lt;cuda/std/utility&amp;gt; // cuda::std::move #include &amp;lt;cooperative_groups.h&amp;gt; #include &amp;lt;cooperative_groups/reduce.h&amp;gt;  t...</li><li><a href="https://github.com/karpathy/llm.c/blob/master/dev/cuda/layernorm_backward.cu?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">llm.c/dev/cuda/layernorm_backward.cu at master ¬∑ karpathy/llm.c</a>: LLM training in simple, raw C/CUDA. Contribute to karpathy/llm.c development by creating an account on GitHub.</li><li><a href="https://github.com/karpathy/llm.c/blob/master/train_gpt2.cu?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#L553">llm.c/train_gpt2.cu at master ¬∑ karpathy/llm.c</a>: LLM training in simple, raw C/CUDA. Contribute to karpathy/llm.c development by creating an account on GitHub.</li><li><a href="https://github.com/karpathy/llm.c/issues/246?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">WikiText 103 evaluation ¬∑ Issue #246 ¬∑ karpathy/llm.c</a>: I've seen some repos use WikiText-103 as the dataset they use to eval GPT-like models, e.g.: https://github.com/tysam-code/hlb-gpt/tree/main Add prepro script to download and preprocess and tokeni...</li><li><a href="https://github.com/karpathy/llm.c/blob/9464f4272ef646ab9ce0667264f8816a5b4875f1/train_gpt2.cu?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#L734">llm.c/train_gpt2.cu at 9464f4272ef646ab9ce0667264f8816a5b4875f1 ¬∑ karpathy/llm.c</a>: LLM training in simple, raw C/CUDA. Contribute to karpathy/llm.c development by creating an account on GitHub.</li><li><a href="https://godbolt.org/z/1hs47YzvY?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Compiler Explorer - CUDA C++ (NVCC 12.3.1)</a>: #include &amp;lt;cuda_fp16.h&amp;gt;   template&amp;lt;class ElementType&amp;gt; struct alignas(16) Packed128 {     __device__ __forceinline__ Packed128() = default;     __device__ __forceinline__ exp...</li><li><a href="https://github.com/karpathy/llm.c/pull/311?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Add script to run benchmarks on Modal by leloykun ¬∑ Pull Request #311 ¬∑ karpathy/llm.c</a>: This PR adds a script to run the benchmarks on the Modal platform. This is useful for folks who do not have access to expensive GPUs locally. To run the benchmark for the attention forward pass on ...</li><li><a href="https://github.com/graphcore-research/out-of-the-box-fp8-training/tree/main?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - graphcore-research/out-of-the-box-fp8-training: Demo of the unit_scaling library, showing how a model can be easily adapted to train in FP8.</a>: Demo of the unit_scaling library, showing how a model can be easily adapted to train in FP8. - graphcore-research/out-of-the-box-fp8-training</li><li><a href="https://github.com/NVIDIA/cudnn-frontend?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - NVIDIA/cudnn-frontend: cudnn_frontend provides a c++ wrapper for the cudnn backend API and samples on how to use it</a>: cudnn_frontend provides a c++ wrapper for the cudnn backend API and samples on how to use it - NVIDIA/cudnn-frontend</li><li><a href="https://github.com/karpathy/llm.c/commit/3fb7252924e342739ba47b5144a785470e839081?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">round 1 of some changes. we will now always write in fp32, even if dt‚Ä¶ ¬∑ karpathy/llm.c@3fb7252</a>: ‚Ä¶ype is set to float16 or bfloat16. next up, we actually want to write in lower precision, when the dtype is set so</li><li><a href="https://github.com/karpathy/llm.c/pull/313/files?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">fixed potential error and generalized gelu forward by ngc92 ¬∑ Pull Request #313 ¬∑ karpathy/llm.c</a>: This adds a helper function for safe casting from size_t to ints (may want to have that in utils.h too). that macro is then used to convert the size_t valued  block_size * x128::size back to a regu...</li><li><a href="https://github.com/karpathy/llm.c/pull/298?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Feature/packed128 by karpathy ¬∑ Pull Request #298 ¬∑ karpathy/llm.c</a>: no description found</li><li><a href="https://github.com/karpathy/llm.c/pull/303?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Updated adamw to use packed data types by ChrisDryden ¬∑ Pull Request #303 ¬∑ karpathy/llm.c</a>: Before Runtime total average iteration time: 38.547570 ms After Runtime: total average iteration time: 37.901735 ms Kernel development file specs: Barely noticeable with the current test suite: Bef...</li><li><a href="https://github.com/karpathy/llm.c/pull/273?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Add NSight Compute ranges, use CUDA events for timings by PeterZhizhin ¬∑ Pull Request #273 ¬∑ karpathy/llm.c</a>: CUDA events allow for more accurate timings (as measured by a GPU) nvtxRangePush/nvtxRangePop Adds simple stack traces to NSight Systems:  Sample run command: nsys profile mpirun --allow-run-as-roo...</li><li><a href="https://github.com/karpathy/llm.c/pull/293?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">yet another gelu by ngc92 ¬∑ Pull Request #293 ¬∑ karpathy/llm.c</a>: more complicated Packet128 for cleaner kernels</li><li><a href="https://github.com/karpathy/llm.c/pull/272?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Full BF16 including layernorms by default (minimising number of BF16 atomics) by ademeure ¬∑ Pull Request #272 ¬∑ karpathy/llm.c</a>: I added 4 different new versions of layernorm_backward_kernel, performance is best for:  Kernel 4 (using atomicCAS, no scratch, but rounding many times so probably worse numerical accuracy Kernel 6...</li><li><a href="https://github.com/karpathy/llm.c/pull/275?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#issuecomment-2083693720">Removing Atomic Adds and adding memory coalescion by ChrisDryden ¬∑ Pull Request #275 ¬∑ karpathy/llm.c</a>: This PR is ontop of the GELU memory coalescion PR and is essentially just a rewrite of the backwards encoder to use shared memory instead of atomic adds and then using the Packed struct to do coale...</li><li><a href="https://github.com/karpathy/llm.c/pull/275?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#issuecomment-2083658642">Removing Atomic Adds and adding memory coalescion by ChrisDryden ¬∑ Pull Request #275 ¬∑ karpathy/llm.c</a>: This PR is ontop of the GELU memory coalescion PR and is essentially just a rewrite of the backwards encoder to use shared memory instead of atomic adds and then using the Packed struct to do coale...</li><li><a href="https://github.com/karpathy/llm.c/pull/275?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Removing Atomic Adds and adding memory coalescion by ChrisDryden ¬∑ Pull Request #275 ¬∑ karpathy/llm.c</a>: This PR is ontop of the GELU memory coalescion PR and is essentially just a rewrite of the backwards encoder to use shared memory instead of atomic adds and then using the Packed struct to do coale...</li><li><a href="https://github.com/karpathy/llm.c/pull/306?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Packing for Gelu backwards by JaneIllario ¬∑ Pull Request #306 ¬∑ karpathy/llm.c</a>: Update gelu backwards kernel to do packing into 128 bits, and create gelu brackward cuda file Previous kernel: block_size   32 | time 0.1498 ms | bandwidth 503.99 GB/s block_size   64 | time 0.0760...</li><li><a href="https://github.com/karpath?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">karpath - Overview</a>: GitHub is where karpath builds software.</li><li><a href="https://github.com/karpathy/llm.c/pull/295?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Remove FloatN &amp; simplify adam/reduce with BF16 LayerNorms by ademeure ¬∑ Pull Request #295 ¬∑ karpathy/llm.c</a>: The MULTI_GPU path is untested, but everything else seems to work fine. I kept the per-tensor "param_sizeof" as it's used in test_gpt2.cu for example, it's not much code and may be u...</li><li><a href="https://github.com/karpathy/llm.c/pull/60?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Speedup `attention_forward_kernel2` by implementing Flash Attention 2 kernel by leloykun ¬∑ Pull Request #60 ¬∑ karpathy/llm.c</a>: This speeds up the attention_forward_kernel2 kernel by replacing the implementation with a minimal Flash Attention 2 kernel as can be found in https://github.com/leloykun/flash-hyperbolic-attention...</li><li><a href="https://github.com/leloykun/flash-hyperbolic-attention-minimal/blob/main/flash_attention_2.cu?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">flash-hyperbolic-attention-minimal/flash_attention_2.cu at main ¬∑ leloykun/flash-hyperbolic-attention-minimal</a>: Flash Hyperbolic Attention in ~[...] lines of CUDA - leloykun/flash-hyperbolic-attention-minimal</li><li><a href="https://github.com/karpathy/llm.c/pull/285?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Flashattention by kilianhae ¬∑ Pull Request #285 ¬∑ karpathy/llm.c</a>: Faster Flash Attention Implementation Added attention_forward6 to src/attention_forward: A fast flash attention forward pass to src/attention_forward written without any dependencies. We are assumi...</li><li><a href="https://github.com/karpathy/llm.c/blob/9464f4272ef646ab9ce0667264f8816a5b4875f1/train_gpt2.cu?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#L1233">llm.c/train_gpt2.cu at 9464f4272ef646ab9ce0667264f8816a5b4875f1 ¬∑ karpathy/llm.c</a>: LLM training in simple, raw C/CUDA. Contribute to karpathy/llm.c development by creating an account on GitHub.</li><li><a href="https://github.com/karpathy/llm.c/blob/master/train_gpt2.cu?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#L2022">llm.c/train_gpt2.cu at master ¬∑ karpathy/llm.c</a>: LLM training in simple, raw C/CUDA. Contribute to karpathy/llm.c development by creating an account on GitHub.</li><li><a href="https://github.com/karpathy/llm.c/blob/master/train_gpt2.cu?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#L2024">llm.c/train_gpt2.cu at master ¬∑ karpathy/llm.c</a>: LLM training in simple, raw C/CUDA. Contribute to karpathy/llm.c development by creating an account on GitHub.</li><li><a href="https://github.com/karpathy/llm.c/pull/301?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Added packing for gelu forwards kernel by ChrisDryden ¬∑ Pull Request #301 ¬∑ karpathy/llm.c</a>: This PR implements packing for the Gelu forwards kernel using the example provided. The kernel dev file was also updated to show the impact of changing the data types for floatX. Before changes: to...</li><li><a href="https://github.com/karpathy/llm.c/pull/299?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Update residual_forward to use packed input by JaneIllario ¬∑ Pull Request #299 ¬∑ karpathy/llm.c</a>: Update residual_forward to use 128 bit packed input, with floatX Previous Kernel: block_size   32 | time 0.1498 ms | bandwidth 503.99 GB/s block_size   64 | time 0.0760 ms | bandwidth 993.32 GB/s b...
</li>
</ul>
</div>
<hr/>
<p><strong>CUDA MODE ‚ñ∑ #<a href="https://discord.com/channels/1189498204333543425/1233704710389764236/1234617660747157535?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">rocm</a></strong> (8 messagesüî•): </p>
<ul>
<li><strong>Inquiry on Flash Attention 2 for ROCm 6.x</strong>: A member inquired whether anyone has been building Flash Attention 2 for <strong>ROCM 6.x</strong>, noting they have successfully done so for ROCm 5.6 and Torch 2.2 but are interested in a newer stack.</li>
<li><strong>Building Woes for Torch Nightly</strong>: Members discussed the difficulties in building for current versions like Torch 2.3, with one expressing a desire to use <strong>Torch nightly</strong> but facing issues.</li>
<li><strong>Official Fork Lagging Behind</strong>: There's mention of the official fork of Flash Attention for AMD hardware being outdated, still at version 2.0 of Flash Attention, without recent developments ported over.</li>
<li><strong>Backward Pass Update Confirmation</strong>: When queried about the backward pass addition to AMD Flash Attention, a member confirmed that it had indeed been added.</li>
<li><strong>Flash Attention GitHub Repository</strong>: A repository link for <a href="https://github.com/ROCm/flash-attention?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ROCm/flash-attention on GitHub</a> was shared, which serves as resource for Fast and Memory-Efficient Exact Attention.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://github.com/ROCm/flash-attention?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - ROCm/flash-attention: Fast and memory-efficient exact attention</a>: Fast and memory-efficient exact attention. Contribute to ROCm/flash-attention development by creating an account on GitHub.</p>
<hr/>
<p><strong>Unsloth AI (Daniel Han) ‚ñ∑ #<a href="https://discord.com/channels/1179035537009545276/1179035537529643040/1234428342305030204?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (487 messagesüî•üî•üî•): </p>
<ul>
<li><strong>Conversion Issues with llama3 on WSL2</strong>: A user reported errors during model conversion to F16 in WSL2, stating <code>RuntimeError: Unsloth: Quantization failed</code>. Even after trying to rebuild <code>llama.cpp</code> and redo the quantization, the problem persisted.</li>
<li><strong>Model Checkpoint Merging Queries</strong>: One member asked how to merge a specific checkpoint to avoid overfitting from the latest epoch. Another member provided information directing to the Unsloth <a href="https://github.com/unslothai/unsloth/wiki?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#finetuning-from-your-last-checkpoint" target="_blank">wiki for more info on checkpointing</a>, and further conversation suggested methods like <em>warmup steps</em> and <em>resuming from a checkpoint</em> options in training functions.</li>
<li><strong>Anticipation for Phi-3</strong>: Members discussed the potential release of Phi-3, with anticipation for trying out the 3.8b version. The conversation spanned from speculation about release timelines to consideration of whether to wait for larger versions like 7b or 14b.</li>
<li><strong>Training Tips and Troubleshooting</strong>: Various users discussed their experiences and strategies with training models like <em>Gemma</em>, <em>LLaMA-3</em>, and <em>Mistral</em>. Tips included the importance of saving checkpoints and adjusting training parameters like <em>max steps</em> and <em>batch sizes</em>.</li>
<li><strong>Updates on Unsloth Tools</strong>: There was a notable emphasis on updating Unsloth installations with newer versions, discussing updates in repositories, and speculations about multi-GPU support on the platform in development.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/dudeman6790/status/1785060925206097976?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from RomboDawg (@dudeman6790)</a>: Currently training Llama-3-8b-instruct on the full 230,000+ lines of coding data in the OpenCodeInterpreter data set. I wonder how much we can increase that .622 on humaneval ü§îü§î Everyone pray my jun...</li><li><a href="https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharin&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Google Colab</a>: no description found</li><li><a href="https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIk?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Google Colab</a>: no description found</li><li><a href="https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Google Colab</a>: no description found</li><li><a href="https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Google Colab</a>: no description found</li><li><a href="https://colab.research.google.com/drive/1cIlNmJS-mvO60iRqxYVFUfD0D9g_B7x0?usp=sharing&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Google Colab</a>: no description found</li><li><a href="https://huggingface.co/unsloth/Phi-3-mini-4k-instruct-bnb-4bit?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">unsloth/Phi-3-mini-4k-instruct-bnb-4bit ¬∑ Hugging Face</a>: no description found</li><li><a href="https://x.com/dudeman6790/status/1784414430781931961?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from RomboDawg (@dudeman6790)</a>: Here is a full colab notebook if you dont want to copy the code by hand. Again thanks to @Teknium1 for the suggestion https://colab.research.google.com/drive/1bX4BsjLcdNJnoAf7lGXmWOgaY8yekg8p?usp=shar...</li><li><a href="https://huggingface.co/DiscoResearch/DiscoLM_German_7b_v1?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">DiscoResearch/DiscoLM_German_7b_v1 ¬∑ Hugging Face</a>: no description found</li><li><a href="https://tenor.com/view/here-we-go-joker-heath-ledger-the-dark-knight-and-here-we-go-gif-17775369?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Here We Go Joker GIF - Here We Go Joker Heath Ledger - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://tenor.com/view/weird-minion-gif-23757545?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Weird Minion GIF - Weird Minion - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://tenor.com/view/wheel-of-fortune-wheel-wof-game-show-celebrity-wheel-of-fortune-gif-23489251?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Wheel Of Fortune Wheel GIF - Wheel Of Fortune Wheel Wof - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">gradientai/Llama-3-8B-Instruct-Gradient-1048k ¬∑ Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/docs/datasets/en/loading?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Load</a>: no description found</li><li><a href="https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">mlabonne/orpo-dpo-mix-40k ¬∑ Datasets at Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/crusoeai/Llama-3-8B-Instruct-1048k-GGUF/tree/main?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">crusoeai/Llama-3-8B-Instruct-Gradient-1048k at main</a>: no description found</li><li><a href="https://github.com/unslothai/unsloth/wiki?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#finetuning-fro">Home</a>: Finetune Llama 3, Mistral &amp; Gemma LLMs 2-5x faster with 80% less memory - unslothai/unsloth</li><li><a href="https://huggingface.co/botbot-ai/CabraLlama3-8b/tree/main?show_tensors=model.safetensors.index.json&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">botbot-ai/CabraLlama3-8b at main</a>: no description found</li><li><a href="https://huggingface.co/arthrod/cicerocabra/tree/main?show_tensors=model.safetensors.index.json&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">arthrod/cicerocabra at main</a>: no description found</li><li><a href="https://github.com/unslothai/unsloth/issues/400?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">[FIXED] NotImplementedError: No operator found for `memory_efficient_attention_forward` with inputs ¬∑ Issue #400 ¬∑ unslothai/unsloth</a>: I'm a beginner to try unsloth. I run the free notebook Llama 3 (8B), and then got the following error: I also encountered the following error during the first installing step: ERROR: pip's dep...</li><li><a href="https://github.com/M-Chimiste/unsloth_finetuning?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - M-Chimiste/unsloth_finetuning</a>: Contribute to M-Chimiste/unsloth_finetuning development by creating an account on GitHub.</li><li><a href="https://github.com/unslothai/unsloth/wiki?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#finetuning-from-your-last-checkpoint">Home</a>: Finetune Llama 3, Mistral &amp; Gemma LLMs 2-5x faster with 80% less memory - unslothai/unsloth</li><li><a href="https://github.com/unslothai/unsloth.git?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - unslothai/unsloth: Finetune Llama 3, Mistral &amp; Gemma LLMs 2-5x faster with 80% less memory</a>: Finetune Llama 3, Mistral &amp; Gemma LLMs 2-5x faster with 80% less memory - unslothai/unsloth</li><li><a href="https://github.com/huggingface/transformers/pull/30079?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">schedulefree optimizers by winglian ¬∑ Pull Request #30079 ¬∑ huggingface/transformers</a>: What does this PR do? integrates meta's https://github.com/facebookresearch/schedule_free for adamw &amp; sgd https://twitter.com/aaron_defazio/status/1776320004465582331 Before submitting   This ...</li><li><a href="https://download.pytorch.org/whl/cu121?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">no title found</a>: no description found</li><li><a href="https://github.com/huggingface/datasets/issues/6753?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Type error when importing datasets on Kaggle ¬∑ Issue #6753 ¬∑ huggingface/datasets</a>: Describe the bug When trying to run import datasets print(datasets.__version__) It generates the following error TypeError: expected string or bytes-like object It looks like It cannot find the val...</li><li><a href="https://github.com/ggerganov/llama.cpp?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - ggerganov/llama.cpp: LLM inference in C/C++</a>: LLM inference in C/C++. Contribute to ggerganov/llama.cpp development by creating an account on GitHub.</li><li><a href="https://github.com/facebookresearch/xformers?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#installing-xformers)">GitHub - facebookresearch/xformers: Hackable and optimized Transformers building blocks, supporting a composable construction.</a>: Hackable and optimized Transformers building blocks, supporting a composable construction. - facebookresearch/xformers</li><li><a href="https://huggingface.co/unsloth?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">unsloth (Unsloth AI)</a>: no description found</li><li><a href="https://github.com/ggerganov/llama.cpp/pull/6920?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">llama : improve BPE pre-processing + LLaMA 3 and Deepseek support by ggerganov ¬∑ Pull Request #6920 ¬∑ ggerganov/llama.cpp</a>: Continuing the work in #6252 by @dragnil1 This PR adds support for BPE pre-tokenization to llama.cpp Summary The state so far has been that for all BPE-based models, llama.cpp applied a default pre...
</li>
</ul>
</div>
<hr/>
<p><strong>Unsloth AI (Daniel Han) ‚ñ∑ #<a href="https://discord.com/channels/1179035537009545276/1179039861576056922/1234459978820227147?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">random</a></strong> (48 messagesüî•): </p>
<ul>
<li><strong>Handling Out of Memory in Colab</strong>: A member gave a tip on combating <strong>Out of Memory (OOM)</strong> errors in Google Colab by running a Python snippet that clears cache and collects garbage using <code>torch</code> and <code>gc</code> modules. <em>Other members appreciated this hack and plan to adopt it for future use</em>.</li>
</ul>
<ul>
<li><strong>Confusion Over the Performance Data of LLama Models</strong>: There was a discussion about the perplexity differences when quantizing LLama models, specifically <strong>LLama 2</strong> and <strong>LLama 3</strong>. It appears there may have been a miscommunication regarding the actual data, as members pointed out possible swaps or errors in the Bits Per Word (BPW) and Perplexity (PPL) columns.</li>
</ul>
<ul>
<li><strong>Phi-3 Now Supported</strong>: An update was shared about <strong>Phi 3</strong> being supported, and members expressed excitement to utilize it for their projects. A link to <em>a Colab notebook</em> was supposed to be shared but was evidently not provided.</li>
</ul>
<ul>
<li><strong>Phi-3 Integration Issues</strong>: Members were discussing issues when trying to use the <strong>Phi-3</strong> model in an Unsloth notebook, with error messages popping up about needing a custom script. <em>The discussion focused on troubleshooting the problem and ensuring that proper notebooks are used</em>.</li>
</ul>
<ul>
<li><strong>Llama 3 License Questions</strong>: A member raised a question about the <strong>Llama 3 license conditions</strong>, wondering if all models derived from it should have certain prefixes and display credits according to the license. Concerns were also voiced about potential license violations by Huggingface models.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://en.wikipedia.org/wiki/Out_of_memory?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Out of memory - Wikipedia</a>: no description found</p>
<hr/>
<p><strong>Unsloth AI (Daniel Han) ‚ñ∑ #<a href="https://discord.com/channels/1179035537009545276/1179777624986357780/1234461140344508418?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">help</a></strong> (230 messagesüî•üî•): </p>
<ul>
<li><strong>Clarification on Loss During Fine-tuning</strong>: A member asked whether the loss displayed during fine-tuning with Unsloth was a test loss or a train loss. The advice given was to pass a validation dataset to the trainer, specifically using the <code>SFTTrainer</code> with a <code>train_dataset</code> and an <code>eval_dataset</code> for validation.</li>
</ul>
<ul>
<li><strong>Early Stopping Not Available in SFTTrainer</strong>: It was pointed out that the <code>SFTTrainer</code> does not support early stopping based on validation loss. The user was informed that a more advanced class called 'trainer' might offer this feature.</li>
</ul>
<ul>
<li><strong>UnslothAI Issues with GGUF Conversion and Xformers</strong>: Multiple users reported issues with GGUF conversion, notably for the Phi-3 model, where a version mismatch of vocab size occurred. Moreover, recent updates to xformers broke compatibility, now requiring PyTorch 2.3; a member offered a temporary solution by pinning the version to <code>xformers&lt;0.0.26</code>.</li>
</ul>
<ul>
<li><strong>llama3 Trained Models Rambling On</strong>: A member expressed concern that their fine-tuned Llama-3 model wouldn't stop talking when inferencing with Ollama, suspecting an issue with <code>EOS_TOKEN</code>. Another user suggested the problem may be that Ollama isn't recognizing the correct <code>EOS_TOKEN</code> set during training.</li>
</ul>
<ul>
<li><strong>Using Multiple GPUs with Unsloth Produces Warning</strong>: A user asked how to use multiple GPUs with Unsloth, sharing an error about detecting multiple CUDA devices but only allowing a single device. The related message shows the system overriding <code>CUDA_VISIBLE_DEVICES</code> to the first device.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Google Colab</a>: no description found</li><li><a href="https://huggingface.co/docs/datasets/en/loading?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#local-and-remote-files">Load</a>: no description found</li><li><a href="https://huggingface.co/docs/peft/v0.10.0/en/package_reference/peft_model?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#peft.get_peft_model.peft_config">Models</a>: no description found</li><li><a href="https://github.com/unslothai/unsloth/wiki?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Home</a>: Finetune Llama 3, Mistral &amp; Gemma LLMs 2-5x faster with 80% less memory - unslothai/unsloth</li><li><a href="https://github.com/unslothai/unsloth?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#-finetune-for-free">GitHub - unslothai/unsloth: Finetune Llama 3, Mistral &amp; Gemma LLMs 2-5x faster with 80% less memory</a>: Finetune Llama 3, Mistral &amp; Gemma LLMs 2-5x faster with 80% less memory - unslothai/unsloth</li><li><a href="https://github.com/ollama/ollama/issues/3759?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">llama3-instruct models not stopping at stop token ¬∑ Issue #3759 ¬∑ ollama/ollama</a>: What is the issue? I'm using llama3:70b through the OpenAI-compatible endpoint. When generating, I am getting outputs like this: Please provide the output of the above command. Let's proceed f...</li><li><a href="https://github.com/vllm-project/vllm/issues/4180?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">[Usage]: Llama 3 8B Instruct Inference ¬∑ Issue #4180 ¬∑ vllm-project/vllm</a>: Your current environment Using the latest version of vLLM on 2 L4 GPUs. How would you like to use vllm I was trying to utilize vLLM to deploy meta-llama/Meta-Llama-3-8B-Instruct model and use OpenA...
</li>
</ul>
</div>
<hr/>
<p><strong>Unsloth AI (Daniel Han) ‚ñ∑ #<a href="https://discord.com/channels/1179035537009545276/1179779344894263297/1234474052270428191?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">showcase</a></strong> (7 messages): </p>
<ul>
<li><strong>Massive Context Extension for Llama 3 8B</strong>: The context length for <strong>Llama 3 8B</strong> has been significantly expanded from 8k to 256k using <strong><a href="https://huggingface.co/papers/2309.10400?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">PoSE</a></strong> as showcased on <a href="https://huggingface.co/winglian/llama-3-8b-256k-PoSE?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Hugging Face</a>. Although untested in 'needle in haystack' scenarios due to inferencing challenges, the model was enhanced with 75M tokens of continued pretraining data.</li>
<li><strong>Community Applauds Winglian</strong>: Members of the chat lauded Winglian for his contributions to the community, particularly in relation to the development of <strong>Llama 3 8B 256K</strong>.</li>
<li><strong>From 128k to 256k</strong>: One member expressed amazement at the progression from a 128k context to a <strong>256k context model</strong>.</li>
<li><strong>Open Source Power</strong>: Skepticism about non-official releases was mentioned due to observed odd behaviors in context-extended models, but there's still an emphasis on the potential of <strong>open source</strong> contributions.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://huggingface.co/winglian/llama-3-8b-256k-PoSE?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">winglian/llama-3-8b-256k-PoSE ¬∑ Hugging Face</a>: no description found</p>
<hr/>
<p><strong>Unsloth AI (Daniel Han) ‚ñ∑ #<a href="https://discord.com/channels/1179035537009545276/1180144489214509097/1234453305980096563?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">suggestions</a></strong> (25 messagesüî•): </p>
<ul>
<li><strong>Unsloth and Recurrent Gemma 2b Integration Inquiry</strong>: A community member expressed interest in integrating <strong>Recurrent Gemma</strong> with <strong>Unsloth</strong> for improved performance. However, the Unsloth team acknowledged an existing bug with the base model of Gemma 2b and current work focused on <strong>Phi 3</strong>, implying integration may not be immediate.</li>
</ul>
<ul>
<li><strong>Gemma 2b VRAM Consumption Issue</strong>: It was reported that Gemma 2b sometimes exceeds VRAM limits, but it is unclear whether it‚Äôs a widespread issue or isolated incidents. The Unsloth team is aware and suggests they need to address this.</li>
</ul>
<ul>
<li><strong>Gemma 2b Still Operational Despite VRAM Overhead</strong>: Although there is a VRAM consumption concern, the Gemma 2b model is still functional. Only one user has reported this issue, pointing to the possibility that it might not be a common problem.</li>
</ul>
<ul>
<li><strong>Reference to Gemma 2b VRAM Issue Provided</strong>: The Unsloth team directed users to a Discord message link for reference on the VRAM issue, although the link was not properly included in the provided text messages.</li>
</ul>
<hr/>
<p><strong>LM Studio ‚ñ∑ #<a href="https://discord.com/channels/1110598183144399058/1110598183144399061/1234439098459230241?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">üí¨-general</a></strong> (135 messagesüî•üî•): </p>
<ul>
<li><strong>LM Studio on Ubuntu GPU Inquiry</strong>: Members sought advice on running LM Studio on a Ubuntu GPU, with suggestions to post detailed system specs in specific channels. Concerns about the compatibility of certain GPUs with inference tasks were also mentioned.</li>
</ul>
<ul>
<li><strong>Groq API for Llama3</strong>: A member shared a <a href="https://youtu.be/ySwJT3Z1MFI?si=qFfek8gTGXVJWoxB&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">YouTube link</a> about a free API from Groq that provides access to the LLAMA-3 model, which reportedly offers 300 tokens per second speed and a commendation for its suitability for a small server Discord bot due to its speed and cost (free).</li>
</ul>
<ul>
<li><strong>LM Studio Local Training Queries</strong>: Users new to LLMs inquired about training a local model based on existing Hugging Face models, with discussions indicating that it is hardware-intensive and time-consuming. A member claimed finetuning a phi-3 4k model on a tiny dataset took almost 8 hours.</li>
</ul>
<ul>
<li><strong>GPU Offload Confusion</strong>: Inquiries around utilizing GPUs for performance gains in LM Studio were brought up, with one member stating that their Intel Titan A770 wasn't useful for GPU offloading in LM Studio and others discussing the effectiveness of disabling 'GPU Offload' to resolve errors.</li>
</ul>
<ul>
<li><strong>Saving KV Cache to Disk with LM Studio</strong>: Members are interested in whether LM Studio allows saving Key-Value (KV) caches to disk and reusing them later, similar to the capability in llama.cpp, to avoid reprocessing large data inputs for queries, with no definitive solutions provided.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://tenor.com/view/mods-discord-mod-moderator-moderation-clash-of-clans-gif-24080525?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Mods Discord Mod GIF - Mods Discord Mod Moderator - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://youtu.be/ySwJT3Z1MFI?si=qFfek8gTGXVJWoxB&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Insanely Fast LLAMA-3 on Groq Playground and API for FREE</a>: Learn how to get started with LLAMA-3 on Groq API, the fastest inference speed that is currently available on the market on any API. Learn how to use the Gro...</li><li><a href="https://github.com/ggerganov/llama.cpp/pull/5021?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">ggml : add Flash Attention by ggerganov ¬∑ Pull Request #5021 ¬∑ ggerganov/llama.cpp</a>: ref #3365 Setting up what's needed for Flash Attention support in ggml and llama.cpp The proposed operator performs: // new res = ggml_flash_attn(ctx, q, k, v, kq_mask, kq_scale);  // fused scale ...</li><li><a href="https://github.com/ggerganov/llama.cpp/pull/6920?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">llama : improve BPE pre-processing + LLaMA 3 and Deepseek support by ggerganov ¬∑ Pull Request #6920 ¬∑ ggerganov/llama.cpp</a>: Continuing the work in #6252 by @dragnil1 This PR adds support for BPE pre-tokenization to llama.cpp Summary The state so far has been that for all BPE-based models, llama.cpp applied a default pre...
</li>
</ul>
</div>
<hr/>
<p><strong>LM Studio ‚ñ∑ #<a href="https://discord.com/channels/1110598183144399058/1111649100518133842/1234440283932856351?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ü§ñ-models-discussion-chat</a></strong> (149 messagesüî•üî•): </p>
<ul>
<li><strong>In Search of Alternate Model Downloads</strong>: Users discussed alternative sources for downloading the GGUF model due to issues with Huggingface. One suggested workaround involves making <code>imatrices</code> which takes a <em>very long time</em> and is <em>compute heavy</em>.</li>
</ul>
<ul>
<li><strong>Intricacies of iQuants and iMatrices</strong>: There was a discussion on the process of creating iQuants for models. An understanding emerged that iQuant creation can be laborious, with imatrices indicating the importance of weights in a model and aiding in more effective compression.</li>
</ul>
<ul>
<li><strong>Collaborative Effort for Model Optimizations</strong>: A user offered a reward of Humblebundle Steam games for assistance in making iQuant versions of the Goliath 120B Longlora model and anticipated sharing the output publicly.</li>
</ul>
<ul>
<li><strong>Phi 3 Issues Surfacing</strong>: Multiple users reported and discussed issues with the Phi-3 model, including leaking prompts and deviating outputs, with updated versions being mentioned for download - <a href="https://huggingface.co/bartowski/Phi-3-mini-4k-instruct-GGUF?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">new 4k instruct</a>.</li>
</ul>
<ul>
<li><strong>Seeking Uncensored Models</strong>: An interaction touched on the availability and suitability of certain uncensored models for usage on lower-spec hardware, with <em>Everything 7b q4</em> and <em>wizard-vicuna-uncensored</em> being suggested models for an 8GB RAM setup.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/Snowflake/snowflake-arctic-instruct?_fsi=v2MrQoFW&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Snowflake/snowflake-arctic-instruct ¬∑ Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/vonjack/Hermes-2-Pro-BakLLaVA-Mistral-7B?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">vonjack/Hermes-2-Pro-BakLLaVA-Mistral-7B ¬∑ Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/AI-Engine/BakLLaVA1-MistralLLaVA-7B-GGUF?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">AI-Engine/BakLLaVA1-MistralLLaVA-7B-GGUF ¬∑ Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/commit/c9b8888921fe528fe4be053258f48b952281bb1b?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">fix(root): Replaces system by user to improve generation experience. ¬∑ microsoft/Phi-3-mini-128k-instruct at c9b8888</a>: no description found</li><li><a href="https://huggingface.co/crusoeai/Llama-3-8B-Instruct-1048k-GGUF/tree/main?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">crusoeai/Llama-3-8B-Instruct-Gradient-1048k at main</a>: no description found</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1cg3e8k/lla?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Reddit - Dive into anything</a>: no description found</li><li><a href="https://github.com/AUTOMATIC1111?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">AUTOMATIC1111 - Overview</a>: AUTOMATIC1111 has 41 repositories available. Follow their code on GitHub.</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1ceh5cp/gpt2chatbot_at_lmsys_chatbot_arena/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Reddit - Dive into anything</a>: no description found</li><li><a href="https://www.youtube.com/shorts/fgG8E6bNwjo?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Neuro Challenges Vedal</a>: Neuro won't stop spamming chat when Vedal challenges her.‚ñ∫Twitch: http://www.twitch.tv/vedal987‚ñ∫Twitter: https://twitter.com/Vedal987#neurosama #vtuber #vedal
</li>
</ul>
</div>
<hr/>
<p><strong>LM Studio ‚ñ∑ #<a href="https://discord.com/channels/1110598183144399058/1113937247520170084/1234538781273489408?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">üß†-feedback</a></strong> (31 messagesüî•): </p>
<ul>
<li><strong>Mysterious Minimization and Section Change Crashes</strong>: A user experiences random crashes of an application when it goes from minimized to full screen or when changing sections within the program. The user runs on Windows 10 Pro with a high-end PC configuration including a Ryzen 7 5800X, RTX 3090, and 64GB DDR4 RAM.</li>
</ul>
<ul>
<li><strong>Suspect Linux Systems with Low RAM</strong>: Multiple Linux users report having only several KB of free RAM, which is unusually low for systems reported to have 64GB or more. This persistent issue raises suspicion and speculation among community members.</li>
</ul>
<ul>
<li><strong>Unusual HDD Activity with Llama</strong>:
    - One user notices their HDD making specific "chattering" noises with each token generation while running Llama3m with partial GPU offload, despite having 96GB of RAM and the model being stored on the HDD.
    - The user discusses potential causes for excessive HDD usage during model inferencing; possibilities include excessive RAM usage causing swapping to a pagefile or log writing processes.</li>
</ul>
<ul>
<li><strong>GPUs Not to Blame</strong>: Community members discuss whether the noise could be GPU coil whine during heavy usage by LLMs and share experiences and links to identify hard drive sounds, confirming the noises are not due to the cooling system.</li>
</ul>
<ul>
<li><strong>Continuation of Troubleshooting</strong>: The conversation regarding the strange HDD behavior during model operation continues, discussing aspects such as offloading to GPU, context size, and specificities of the Lexi-Llama-3-8B model. Users are reminded to keep bug reports and help issues within designated channels.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF ¬∑ Hugging Face</a>: no description found</li><li><a href="https://www.youtube.com/watch?v=rJM8rHfsgjk&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Hard Drive Sounds</a>: This is a comparison of all the sounds of the HDDs in my hard drive collection. The drives are played in chronological from oldest to newest.
</li>
</ul>
</div>
<hr/>
<p><strong>LM Studio ‚ñ∑ #<a href="https://discord.com/channels/1110598183144399058/1153759714082033735/1234495899623886911?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">üéõ-hardware-discussion</a></strong> (74 messagesüî•üî•): </p>
<div class="codehilite"><pre><span></span><code><span class="p">&lt;</span><span class="nt">ul</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">li</span><span class="p">&gt;&lt;</span><span class="nt">strong</span><span class="p">&gt;</span>XP on Aggregate GPUs**: Discussions point out that <span class="p">&lt;</span><span class="nt">strong</span><span class="p">&gt;</span>Llama 70B** with *Q4 quantization* can fit on two RTX 3090 GPUs, but adding more GPUs beyond that may cause slowdowns due to PCIe bus limitations. It's mentioned that the optimum price-performance is achieved with two RTX 3090s for running and fine-tuning most models.<span class="p">&lt;/</span><span class="nt">li</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">li</span><span class="p">&gt;&lt;</span><span class="nt">strong</span><span class="p">&gt;</span>Older GPUs Can Still Play**: A member successfully tested *dolphin-Llama3-8b* and *Llava-Phi3* on a GTX 1070, indicating the potential for older and less powerful GPUs to run smaller models for specific applications like roleplaying for a droid project.<span class="p">&lt;/</span><span class="nt">li</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">li</span><span class="p">&gt;&lt;</span><span class="nt">strong</span><span class="p">&gt;</span>Energy Efficiency and Running Costs**: One user calculates the cost of generating 1M tokens on their laptop and compares it to using GPT-3.5. Turbo, finding that running the model locally on their setup is more expensive and slower than using the API service.<span class="p">&lt;/</span><span class="nt">li</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">li</span><span class="p">&gt;&lt;</span><span class="nt">strong</span><span class="p">&gt;</span>Exploring Model Performance and Accuracy**: Discussion among users about the accuracy and efficiency of newer LLMs like *Llama3* compared to more established services like GPT-4, with some expressing doubts about the accuracy and information quality of quantized or smaller, more compressed versions of the models.<span class="p">&lt;/</span><span class="nt">li</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">li</span><span class="p">&gt;&lt;</span><span class="nt">strong</span><span class="p">&gt;</span>Finding the Right Local Model**: Users are recommended to experiment with various models to find the best fit for their hardware, with suggestions ranging from *CMDR+* (which may be too large for certain GPUs) to *Llama3* and *Wizard V2* which might offer decent performance on more average setups.<span class="p">&lt;/</span><span class="nt">li</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">ul</span><span class="p">&gt;</span>
</code></pre></div>
<hr/>
<p><strong>LM Studio ‚ñ∑ #<a href="https://discord.com/channels/1110598183144399058/1166577236325965844/1234783013846515752?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">üß™-beta-releases-chat</a></strong> (5 messages): </p>
<ul>
<li><strong>Hardware Headaches</strong>: A user installed Ubuntu on their hardware and attempted to run a Linux beta release, but found that their <strong>LLM was not accepted</strong>. They queried whether the issue could be due to their hardware specifications.</li>
<li><strong>Specs Not Up to Spec</strong>: Another member responded, suggesting that the user's hardware, which included an i5-4570 and 16GB RAM, <strong>might not be sufficient</strong> to run most models and could probably only handle a <strong>7b Q4 model</strong> effectively.</li>
<li><strong>Graceful Exit Planned</strong>: The user appreciated the prompt feedback and indicated plans to <strong>uninstall the software</strong>, mentioning that an upgrade to better hardware was not within their means.</li>
<li><strong>Tokenizer Trouble Ticket</strong>: A request was made for the <strong>latest commit of llama.cpp</strong> to address an issue with the llama tokenizer, which is pending an update.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://www.canadacomputers.com/product_info.php?cPath=7_4528_4570&amp;item_id=230804&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Dell Treasure Box (Black) Desktop i5-4570, 16GB, 512GB SSD, DVD, Win10</a>: Dell RGB Treasure Box OptiPlex SFF (Refurbished) Consumer Desktop Intel Core i5-4570 (up to 3.6GHz), 16GB, 512GB SSD, DVD, Windows 10 Professional (EN/FR) (Black)</p>
<hr/>
<p><strong>LM Studio ‚ñ∑ #<a href="https://discord.com/channels/1110598183144399058/1167546228813336686/1234815876772134932?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">autogen</a></strong> (4 messages): </p>
<ul>
<li><strong>Seeking Troubleshooting for Model Loading Issue</strong>: A member expressed urgency in resolving a <strong>model loading issue</strong> but did not provide further details on the nature of the problem.</li>
<li><strong>Discord Etiquette Reminder</strong>: Another member advised against spamming questions across unrelated channels, suggesting to keep queries in the designated support channel (<em>&lt;#1111440136287297637&gt;</em>).</li>
</ul>
<hr/>
<p><strong>LM Studio ‚ñ∑ #<a href="https://discord.com/channels/1110598183144399058/1167546793656062063/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">langchain</a></strong> (1 messages): </p>
<p>ahakobyan.: can we know too?</p>
<hr/>
<p><strong>LM Studio ‚ñ∑ #<a href="https://discord.com/channels/1110598183144399058/1195858490338594866/1234647462166401115?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">amd-rocm-tech-preview</a></strong> (19 messagesüî•): </p>
<ul>
<li><strong>ROCm Version Queries</strong>: Users explored differences between <strong>version 0.2.20 and 0.2.21</strong> concerning GPU offloading, with one questioning if there is any advantage to installing the <strong>0.2.20 beta</strong> for better AMD functionality or if the newer version already includes requisite support.</li>
<li><strong>VRAM Discrepancies Noticed</strong>: A user reported <strong>LM Studio</strong> showing incorrect VRAM capacity for their <strong>7900xtx</strong>, suggesting it might be including the shared memory from Smart Access Memory (SAM) / resizable BAR, leading to inaccurate GPU offload estimates.</li>
<li><strong>Understanding GPU and IGPU Configurations</strong>: In the discussion, a user mentioned having an <strong>IGPU</strong> in the system, while using a <strong>7800x3d</strong> with less than the VRAM displayed by LM Studio, indicating a possible misrepresentation of available graphics memory.</li>
<li><strong>ROCm Compatibility Confusions</strong>: Multiple users conversed about whether certain AMD GPUs (specifically <strong>RX 6600</strong>) are supported by ROCm or not, with clarifications provided that while some older versions might have worked using OpenCL, the RX6600 is not supported by the <strong>HIP SDK</strong> which LM Studio utilizes.</li>
<li><strong>Development Environment Specifications</strong>: There was uncertainty about the nature of <strong>ROCm's compatibility</strong> with Windows, with a user asserting successful use of <strong>ROCm on Ubuntu</strong> for image generation models, suggesting discrepancies in ROCm's support across different operating systems.</li>
</ul>
<hr/>
<p><strong>Stability.ai (Stable Diffusion) ‚ñ∑ #<a href="https://discord.com/channels/1002292111942635562/1002292112739549196/1234429669860970498?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general-chat</a></strong> (400 messagesüî•üî•): </p>
<ul>
<li><strong>Civitai and monetization woes</strong>: Members voiced concerns over clubs and potential paywalls in AI model development, with a particular backlash against Civitai's monetization moves, such as Buzz donations which don't monetarily benefit creators, described as a <strong>"rip-off"</strong> <a href="https://youtu.be/nLT32AR5c68?si=bV9wXlRzb_oLutW9&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">by Tower13Studios</a>.</li>
<li><strong>In the quest for AI-fueled success</strong>: Discussions revealed skepticism towards making money through SFW (Safe For Work) AI art due to oversaturation. NSFW (Not Safe For Work) artworks, especially furry and vtuber commissions, were repeatedly mentioned as the more lucrative side of AI-generated content.</li>
<li><strong>AI image generation pace picks up</strong>: Rapid generation of images using SDXL models and Python scripting was a hot topic, with members sharing code and seeking advice on pushing the speed limits for real-time applications, like Discord bots.</li>
<li><strong>Saddle up for Collider</strong>: Stable Diffusion's new release drew eager inquiries and speculation around the release date and potential improvements over previous versions, with users sharing their anticipation and hopes for the model.</li>
<li><strong>Technical queries and troubleshooting abound</strong>: Users sought advice on various technical aspects from model training, such as creating LoRAs and IPAdapters, to overcoming bottlenecks encountered while running AI models on less capable hardware, with solutions occasionally offered by fellow members.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://dreamstudio.ai/terms-of-service?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">DreamStudio</a>: no description found</li><li><a href="https://tenor.com/view/dj-khaled-tayomaki-sakigifs-dancing-jamming-gif-22144912?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Dj Khaled Tayomaki GIF - Dj Khaled Tayomaki Sakigifs - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://civitai.com/models/428813?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Mythos - v1.0 | Stable Diffusion Checkpoint | Civitai</a>: V1 it is somehow 3.55GB big.... i think i managed to do a stable fp8 prune???? i literally have no idea how it is 3.55GB... V2 is a normal 6GB mode...</li><li><a href="https://civitai.com/articles/5069?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Towards Pony Diffusion V7 | Civitai</a>: Hello everyone, I'm excited to share updates on the progress of our upcoming V7, along with a retrospective analysis of V6. The recognition V6 has ...</li><li><a href="https://tenor.com/vD6Ib9MNmkI.gif?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Melxts2008 Emoji GIF - Melxts2008 Emoji Smile - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://github.com/hiddenswitch/ComfyUI/blob/0862863bc00165b9ba0607595f304f93ca995887/tests/distributed/test_embedded_client.py?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#L32">ComfyUI/tests/distributed/test_embedded_client.py at 0862863bc00165b9ba0607595f304f93ca995887 ¬∑ hiddenswitch/ComfyUI</a>: A powerful and modular stable diffusion GUI with a graph/nodes interface. - hiddenswitch/ComfyUI</li><li><a href="https://warpcast.com/~/invite-page/404899?id=fd0fd839&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Warpcast</a>: no description found</li><li><a href="https://warpcast.com/~/channel/aigc?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Warpcast</a>: no description found</li><li><a href="https://github.com/huggingface/diffusers/tree/main/examples/dreambooth?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">diffusers/examples/dreambooth at main ¬∑ huggingface/diffusers</a>: ü§ó Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch and FLAX. - huggingface/diffusers</li><li><a href="https://www.reddit.com/r/StableDiffusion/comments/1cdm434/sd3_is_amazing_much_better_than_all_other/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#lightbox">Reddit - Dive into anything</a>: no description found</li><li><a href="https://youtu.be/nLT32AR5c68?si=bV9wXlRzb_oLutW9&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">The Angola Effect | Horrifying death traps in the cradle of evolution</a>: üßü‚Äç‚ôÇÔ∏èüéß Horror fan? Go follow and listen to RUN, FOOL! - our newest show from Ballen Studios. New episodes every Tuesday - https://smarturl.it/RunFoolTime St...</li><li><a href="https://github.com/hiddenswitch/ComfyUI/blob/master/script_examples/basic_api_example.py?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">ComfyUI/script_examples/basic_api_example.py at master ¬∑ hiddenswitch/ComfyUI</a>: A powerful and modular stable diffusion GUI with a graph/nodes interface. - hiddenswitch/ComfyUI
</li>
</ul>
</div>
<hr/>
<p><strong>Perplexity AI ‚ñ∑ #<a href="https://discord.com/channels/1047197230748151888/1047649527299055688/1234429101729644615?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (322 messagesüî•üî•): </p>
<ul>
<li><strong>Perplexity Performance Plummets</strong>: Users reported significant slowdowns and poor performance across various models, including <strong>Japanese searches</strong>, with perplexity <strong>translating queries</strong> into English resulting in <em>meaningless garbage</em>. Models like <strong>Opus</strong>, <strong>Sonar Large 32K</strong>, and <strong>GPT-4 Turbo</strong> have become sluggish, making the platform unusable and hindering tasks during the Japanese Golden Week.</li>
</ul>
<ul>
<li><strong>Pro Subscription Confusion</strong>: Users faced issues with <strong>Pro subscription coupons</strong> showing as expired on their due date, with the <strong>Nothing Phone 2(a)</strong> associated offers being suspended early due to fraud. Customer support via <a href="mailto:support@perplexity.ai" target="_blank">support@perplexity.ai</a> is advised for resolutions.</li>
</ul>
<ul>
<li><strong>Rewind on Free Trial</strong>: The <strong>7-day free trial</strong> was mentioned to be removed from the website due to abuse, prompting user disappointment as it was seen as an effective way to introduce new users to <strong>Perplexity Pro</strong>.</li>
</ul>
<ul>
<li><strong>Log-in Loop</strong>: Users experienced difficulty logging in due to <strong>email link delays</strong>, especially with emails ranked 'lower' than services like Gmail, affecting <strong>Pro account access</strong>.</li>
</ul>
<ul>
<li><strong>Voice Feature Variance</strong>: A discrepancy was noted in the <strong>voice feature</strong> on <strong>iOS</strong>; whereas some users only had the previously existing feature, others had access to a more recent version showcased in published videos. It was found that this may depend on the <strong>app version</strong> being used.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://fxtwitter.com/Gradient_AI_/status/1785030931407143040?t=U4_FdN9hNDaE9y432-lssQ&amp;s=19&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Gradient (@Gradient_AI_)</a>: We've been in the kitchen cooking üî• Excited to release the first @AIatMeta LLama-3 8B with a context length of over 1M on @huggingface - coming off of the 160K context length model we released on...</li><li><a href="https://flashcardfy.lol?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Flashcardfy - AI Flashcard Generator with Personalized Feedback</a>: Learn faster and smarter with AI-generated flashcards that provide personalized feedback.</li><li><a href="https://chat.reka.ai/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Reka Playground</a>: Explore the latest multimodal language models built by Reka
</li>
</ul>
</div>
<hr/>
<p><strong>Perplexity AI ‚ñ∑ #<a href="https://discord.com/channels/1047197230748151888/1054944216876331118/1234586871569449121?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">sharing</a></strong> (13 messagesüî•): </p>
<ul>
<li><strong>Delving Into WhatsApp's Autoreply Feature</strong>: A message shares a <a href="https://www.perplexity.ai/search/whatsapp-auto-reply-JlOlDYw1Qyuik7pDTuJMuw?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Perplexity AI search result</a> exploring auto-reply functionality in WhatsApp.</li>
<li><strong>Uncovering the Essence of 'Topic 3'</strong>: A link directs users to a <a href="https://www.perplexity.ai/search/Topic-3-One-n3JNQZT4T.ij7MosuLX5OA?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Perplexity AI search regarding Topic 3</a>, but does not provide further context or description.</li>
<li><strong>Research Info on Surroind</strong>: The message contains a <a href="https://www.perplexity.ai/search/research-info-surroind-oAy5SMejT4S72Fyxei7MYw?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#0" target="_blank">Perplexity AI link</a> presumably related to research info on "Surroind," details are not specified.</li>
<li><strong>Insights on an Unspecified Topic From Lenny's Newsletter</strong>: The user shared a <a href="https://www.lennysnewsletter.com/p/how-perplexity-builds-product?utm_medium=web&amp;utm_source=ainews&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">newsletter link</a> with insights from Lenny's Newsletter, highlighting Lenny's tackle on questions about product building, growth driving, and career acceleration.</li>
<li><strong>Inquiry about Vimeo API</strong>: A user posted a <a href="https://www.perplexity.ai/search/Vimeo-API-kZ3X_KA2TUqmkwXzSe9ymA?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Perplexity AI search link</a> pertaining to the Vimeo API, specifics of the inquiry are not given.</li>
</ul>
<p><em>Note: Some messages contained Perplexity AI search result links with no context provided; thus, the content or nature of the discussions on these topics could not be summarized.</em></p>
<p><strong>Link mentioned</strong>: <a href="https://www.lennysnewsletter.com/p/how-perplexity-builds-product?utm_medium=web&amp;utm_source=ainews&amp;utm_campaign=ainews-to-be-named-4408">How Perplexity builds product</a>: Johnny Ho, co-founder and head of product, explains how he organizes his teams like slime mold, uses AI to build their AI company, and much more</p>
<hr/>
<p><strong>Perplexity AI ‚ñ∑ #<a href="https://discord.com/channels/1047197230748151888/1161802929053909012/1234574679038230599?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">pplx-api</a></strong> (7 messages): </p>
<ul>
<li><strong>Seeking Source URL Access via API</strong>: A user inquired about the availability of <strong>source URLs</strong> in the API and mentioned that it was previously listed in the roadmap documentation. Access to this feature is granted through an application process provided in a <a href="https://perplexity.typeform.com/to/j50rnNiB?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">form link</a>.</li>
</ul>
<ul>
<li><strong>Access to Citations Still Limited</strong>: One member shared disappointment due to being declined access to <strong>source URL feature</strong>; access was restricted to funded startups at the time of their request.</li>
</ul>
<ul>
<li><strong>Inquiry on make.com Model Availability</strong>: A user questioned why <strong>Llama 3</strong> models and <strong>Mixtral 8x22b</strong> are not listed as options on make.com's integration services.</li>
</ul>
<ul>
<li><strong>Request for API Citations Format</strong>: A member asked if it's possible to get citations (such as [1]) via <strong>API requests</strong>, particularly wanting <strong>RAG-like knowledge over the web</strong>.</li>
</ul>
<ul>
<li><strong>Perplexity vs. Anthropic Usage Policies Clarification</strong>: The equipoise about usage policies was put forth by a user seeking to understand if using <strong>Claude 3</strong> under <strong>Perplexity's</strong> terms would still require adherence to <strong>Anthropic's political usage</strong> restrictions.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://perplexity.typeform.com/to/j50rnNiB?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">pplx-api form</a>: Turn data collection into an experience with Typeform. Create beautiful online forms, surveys, quizzes, and so much more. Try it for FREE.</p>
<hr/>
<p><strong>Nous Research AI ‚ñ∑ #<a href="https://discord.com/channels/1053877538025386074/1108104624482812015/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ctx-length-research</a></strong> (1 messages): </p>
<p>kainan_e: Banned (was a spambot)</p>
<hr/>
<p><strong>Nous Research AI ‚ñ∑ #<a href="https://discord.com/channels/1053877538025386074/1109649177689980928/1234510416768667719?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">off-topic</a></strong> (3 messages): </p>
<ul>
<li><strong>The Promise vs. The Reality</strong>: A member lampooned an overhyped message about <strong>"pioneering the future"</strong>, which turned out to be just another waitlist announcement.</li>
<li><strong>The Hunt for MLOps Bounties</strong>: A question was raised about where to find the best <strong>MLOps bounties</strong>, suggesting the need for an AI-focused platform similar to Fiverr.</li>
<li><strong>A Quest for a Programmer's Marketplace</strong>: In response to the query about MLOps bounties, another member questioned the existence of a dedicated marketplace even for standard programming bounties.</li>
</ul>
<hr/>
<p><strong>Nous Research AI ‚ñ∑ #<a href="https://discord.com/channels/1053877538025386074/1132352574750728192/1234469824021659729?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">interesting-links</a></strong> (6 messages): </p>
<ul>
<li><strong>Decentralizing AI Training</strong>: Prime Intellect proposes an open-source solution against closed-source counterparts deploying <em>H100 GPU clusters</em>. Their platform aims to overcome traditional computing infrastructure limits by enabling distributed training across global clusters, as detailed in their <a href="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog post on decentralized training</a>.</li>
</ul>
<ul>
<li><strong>Improving LLMs with IN2 Training</strong>: A new training regimen called <strong>information-intensive (IN2) training</strong> addresses large language models' 'lost-in-the-middle' challenge by providing explicit supervision on long contexts. These details and a link to the study are available in an <a href="https://arxiv.org/abs/2404.16811?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">arXiv paper</a>.</li>
</ul>
<ul>
<li><strong>Back to the Origins with GPT-1</strong>: A blog post reflects on the original GPT-1 model, identifying its lasting relevance and similarities to contemporary models. It discusses how the older model set the stage for the latest in LLM development, as explained on <a href="https://amgadhasan.substack.com/p/revisiting-gpt-1-the-spark-that-ignited-llms?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">amgadhasan's substack</a>.</li>
</ul>
<ul>
<li><strong>Understanding LLMs Through Synergistic Analysis</strong>: A recommended YouTube video provides insights into the stability, inflection, and coherence analysis of language models. <strong>Synapse's analysis</strong> can be viewed <a href="https://www.youtube.com/watch?v=p0NxSk7YMrI&amp;ab_channel=Synapse&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>.</li>
</ul>
<ul>
<li><strong>Agent Long-Term Memory Project on GitHub</strong>: The memary repository suggests intriguing possibilities for long-term memory in autonomous agents using neo4j for memory storage. The implementation and its performance can be explored on <a href="https://github.com/kingjulio8238/memary?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GitHub</a>.</li>
</ul>
<ul>
<li><strong>GPT-2 Chatbot Goes Offline</strong>: In a sudden turn of events, the gpt2-chatbot was reported as offline despite being active just half an hour earlier, as tweeted by @itsandrewgao and found by @shaunralston. The situation was highlighted on <a href="https://x.com/itsandrewgao/status/1785373740622356753?s=46&amp;t=zdoDWYj2oTzRaTJHApTcOw&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Twitter</a>.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/itsandrewgao/status/1785373740622356753?s=46&amp;t=zdoDWYj2oTzRaTJHApTcOw&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Andrew Gao (@itsandrewgao)</a>: gpt2-chatbot was just turned OFFLINE  I was just using it half an hour ago! @shaunralston for the find   #gpt2 @openai</li><li><a href="https://arxiv.org/abs/2404.16811?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Make Your LLM Fully Utilize the Context</a>: While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We ...</li><li><a href="https://github.com/kingjulio8238/memary?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - kingjulio8238/memary: Longterm Memory for Autonomous Agents.</a>: Longterm Memory for Autonomous Agents. . Contribute to kingjulio8238/memary development by creating an account on GitHub.</li><li><a href="https://amgadhasan.substack.com/p/revisiting-gpt-1-the-spark-that-ignited-llms?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Revisiting GPT-1: The spark that ignited the fire of LLMs</a>: A Comprehensive Look at GPT-1's Contribution to the Development of Modern LLMs</li><li><a href="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">State-of-the-art in Decentralized Training</a>: This post explores various novel decentralized training approaches and how they can enable effective AI model training across globally distributed GPUs.
</li>
</ul>
</div>
<hr/>
<p><strong>Nous Research AI ‚ñ∑ #<a href="https://discord.com/channels/1053877538025386074/1149866623109439599/1234472373114372176?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (231 messagesüî•üî•): </p>
<ul>
<li><strong>PDF Handling via OpenAI API Question</strong>: A member inquired about PDF uploads through APIs, specifically looking for multimodal functionality. It was clarified that one can use <a href="https://platform.openai.com/docs/assistants/tools/file-search?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">OpenAI's file search tool in API</a>, which handles about 10k individual files.</li>
</ul>
<ul>
<li><strong>PDF Parsing Challenges and Solutions</strong>: There's a discussion on the concerns regarding accurate parsing of PDF tables for AI models. One suggested workaround involved separating and uploading text and images from PDFs independently <a href="https://platform.openai.com/docs/assistants/whats-new/agents?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">due to limitations within the <strong>assistants</strong> platform</a>.</li>
</ul>
<ul>
<li><strong>Model Integration Experimentation</strong>: A member shared their attempt at combining <strong>Hermes 2 Pro</strong> and <strong>BakLLaVA-1</strong> to create a <a href="https://huggingface.co/vonjack/Hermes-2-Pro-BakLLaVA-Mistral-7B?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">simple multimodal GPT-4 model with LLaMA weights</a>, which required no finetuning, just a merging of weights related to <strong>mistral-7b-v0.1</strong>.</li>
</ul>
<ul>
<li><strong>GPT2-Chatbot Mystery Engages the Community</strong>: There's been a lot of buzz around a mysterious model dubbed ‚Äògpt2-chatbot‚Äô; speculation ranges from it being an early version of <strong>GPT-4.5</strong> to an advanced model with a knowledge cutoff in November 2023. Despite attempts to discern its capabilities, the model was <a href="https://x.com/itsandrewgao/status/1785373740622356753?s=46&amp;t=zdoDWYj2oTzRaTJHApTcOw&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">removed before further detailed testing could occur</a>.</li>
</ul>
<ul>
<li><strong>Llama 3 Gains Vision with SigLIP</strong>: A breakthrough was discussed where a member achieved <a href="https://huggingface.co/qresearch/llama-3-vision-alpha-hf?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">vision capabilities</a> for Llama 3 using SigLIP, making it usable directly in Transformers despite the absence of bitsandbytes quantization support.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/itsandrewgao/status/1785373740622356753?s=46&amp;t=zdoDWYj2oTzRaTJHApTcOw&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Andrew Gao (@itsandrewgao)</a>: gpt2-chatbot was just turned OFFLINE  I was just using it half an hour ago! @shaunralston for the find   #gpt2 @openai</li><li><a href="https://huggingface.co/vonjack/Hermes-2-Pro-BakLLaVA-Mistral-7B?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">vonjack/Hermes-2-Pro-BakLLaVA-Mistral-7B ¬∑ Hugging Face</a>: no description found</li><li><a href="https://google-research.github.io/seanet/audiopalm/examples/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">AudioPaLM</a>: no description found</li><li><a href="https://x.com/hingeloss/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from undefined</a>: no description found</li><li><a href="https://x.com/lmsysorg/status/1785394860754866234?s=46&amp;t=stOPrwZiN_fxSK0RuC8Flg&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from lmsys.org (@lmsysorg)</a>: Thanks for the incredible enthusiasm from our community! We really didn't see this coming.   Just a couple of things to clear up:  - In line with our policy, we've worked with several model de...</li><li><a href="https://x.com/qtnx_/status/1785383089109172705?s=46&amp;t=stOPrwZiN_fxSK0RuC8Flg&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Q (@qtnx_)</a>: llama-3-vision-alpha now works using @huggingface transformers</li><li><a href="https://huggingface.co?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Hugging Face ‚Äì The AI community building the future.</a>: no description found</li><li><a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">llava_instruct_150k.json ¬∑ liuhaotian/LLaVA-Instruct-150K at main</a>: no description found</li><li><a href="https://x.com/ylecun/status/1785100806695325804?s=46&amp;t=stOPrwZiN_fxSK0RuC8Flg&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Yann LeCun (@ylecun)</a>: One might think that, by now, people would realize that retrieving the solution to a common puzzle does not require any reasoning ability.  ‚ÜòÔ∏è Quoting Colin Fraser | @colin-fraser.net on bsky (@colin_...</li><li><a href="https://huggingface.co/a-normal-username/Mixtral-8x22B-OpenHermes-2.5?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">a-normal-username/Mixtral-8x22B-OpenHermes-2.5 ¬∑ Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/qresearch/llama-3-vision-alpha-hf?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">qresearch/llama-3-vision-alpha-hf ¬∑ Hugging Face</a>: no description found</li><li><a href="https://github.com/haotian-liu/LLaVA/blob/main/docs%2FFinetune_Custom_Data.md?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">LLaVA/docs/Finetune_Custom_Data.md at main ¬∑ haotian-liu/LLaVA</a>: [NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond. - haotian-liu/LLaVA</li><li><a href="https://github.com/nestordemeure/stop_word/tree/main?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - nestordemeure/stop_word: Huggingface transformers stopping criteria that halts the generation when a given stop word is encountered.</a>: Huggingface transformers stopping criteria that halts the generation when a given stop word is encountered. - nestordemeure/stop_word</li><li><a href="https://github.com/tincans-ai/gazelle?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - tincans-ai/gazelle: Joint speech-language model - respond directly to audio!</a>: Joint speech-language model - respond directly to audio! - tincans-ai/gazelle</li><li><a href="https://x.com/qtnx_/status/1785383089109172705?s=46&amp;t=st&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Q (@qtnx_)</a>: llama-3-vision-alpha now works using @huggingface transformers</li><li><a href="https://youtu.be/u5Vcrwpzoz8?si=U30s6BAN9Jsaec-P&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">"I want Llama3 to perform 10x with my private knowledge" - Local Agentic RAG w/ llama3</a>: Advanced RAG 101 - build agentic RAG with llama3Get free HubSpot report of how AI is redefining startup GTM strategy: https://clickhubspot.com/4hxüîó Links- F...</li><li><a href="https://github.com/ggerganov/llama.cpp/pull/6920?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">llama : improve BPE pre-processing + LLaMA 3 and Deepseek support by ggerganov ¬∑ Pull Request #6920 ¬∑ ggerganov/llama.cpp</a>: Continuing the work in #6252 by @dragnil1 This PR adds support for BPE pre-tokenization to llama.cpp Summary The state so far has been that for all BPE-based models, llama.cpp applied a default pre...
</li>
</ul>
</div>
<hr/>
<p><strong>Nous Research AI ‚ñ∑ #<a href="https://discord.com/channels/1053877538025386074/1154120232051408927/1234577224812990635?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ask-about-llms</a></strong> (19 messagesüî•): </p>
<ul>
<li><strong>Consensus on Mixing Tasks for LLM Training</strong>: One member suggested mixing tasks is preferable during LLM training to avoid the degradation associated with <em>finetunes over finetunes</em>. Another member added that a specific finetune on top of a general one can sometimes benefit very specialized tasks.</li>
<li><strong>Skeptical of LLama-3 8B Gradient Instruct's Claims</strong>: Highlights include a link to the model which extends LLama-3 8B context length to &gt;1040K, with a member expressing skepticism about its retrieval performance claims, indicating that further training might be needed as suggested by a linked <a href="https://arxiv.org/abs/2404.16811?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ArXiv paper</a>.</li>
<li><strong>Curiosity Over Compute Requirements</strong>: A discussion about the impressive context length extension of the <strong>LLama-3 8B Gradient Instruct</strong> led to a query about the computational resources needed, with a reply stating it required <strong>512 L40s</strong>. Another member remarked that many applications would not require the full 1M token context window but would benefit from improved retrieval performance.</li>
<li><strong>GitHub Pull Request Fixes Llama</strong>: An update was shared including a link to a <a href="https://github.com/ggerganov/llama.cpp/pull/6920?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GitHub pull request</a> that addressed an issue with LLaMA models support in llama.cpp, indicating improved BPE pre-processing and support for LLaMa 3.</li>
<li><strong>Question Regarding Tokenization and Quantization</strong>: A conversation about the tokenizer issue in LLaMA models and whether the GGUFs need to be requantized resulted in uncertainty, with a member indicating that the pull request description was not clear on the solution.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">gradientai/Llama-3-8B-Instruct-Gradient-1048k ¬∑ Hugging Face</a>: no description found</li><li><a href="https://github.com/ggerganov/llama.cpp/pull/6920?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">llama : improve BPE pre-processing + LLaMA 3 and Deepseek support by ggerganov ¬∑ Pull Request #6920 ¬∑ ggerganov/llama.cpp</a>: Continuing the work in #6252 by @dragnil1 This PR adds support for BPE pre-tokenization to llama.cpp Summary The state so far has been that for all BPE-based models, llama.cpp applied a default pre...
</li>
</ul>
</div>
<hr/>
<p><strong>Nous Research AI ‚ñ∑ #<a href="https://discord.com/channels/1053877538025386074/1218682416827207801/1234865912696537130?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">rag-dataset</a></strong> (6 messages): </p>
<ul>
<li><strong>Expanding Language Retrieval Horizons</strong>: A user highlighted a <a href="https://huggingface.co/collections/nthakur/swim-ir-dataset-662ddaecfc20896bf14dd9b7?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Wikipedia RAG dataset</a> for use in <strong>multilingual dense retrieval</strong>, linked to a paper on leveraging LLMs to synthesize training data across many languages.</li>
<li><strong>Dietary Data Inclusion</strong>: The mentioned dataset incorporates information with a focus on <strong>Halal &amp; Kosher</strong>, suggesting an attempt to provide diverse and inclusive data.</li>
<li><strong>Behind the Scenes with Model Selection</strong>: A member expressed interest in checking which models were used in the context of the aforementioned dataset discussion without further elaboration.</li>
<li><strong>Development Detours</strong>: Conveyed being engaged in coding activities, though no details were provided about the nature of the work being done.</li>
<li><strong>Integrating Pydantic into Cynde</strong>: Shared excitement about using the new <a href="https://pydantic.dev/logfire?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Pydantic Logfire</a>, considering it for integration with the AI tool <strong>Cynde</strong>. It offers an easier way to understand the application and keeps track of Pydantic model validations efficiently.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://pydantic.dev/logfire?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Pydantic Logfire | Uncomplicated observability</a>: Logfire is a new type of observability platform built on the same belief as Pydantic ‚Äî that the most powerful tools can be easy to use.</li><li><a href="https://huggingface.co/collections/nthakur/swim-ir-dataset-662ddaecfc20896bf14dd9b7?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">ü¶¢SWIM-IR Dataset - a nthakur Collection</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>Nous Research AI ‚ñ∑ #<a href="https://discord.com/channels/1053877538025386074/1221910674347786261/1234429520203747328?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">world-sim</a></strong> (35 messagesüî•): </p>
<ul>
<li><strong>World Sim Takes Role-Playing to the Next Level</strong>: Users reveal that the <em>worldsim</em> prompt running on <strong>llama 3 70b</strong>, although stiff, is engaging. Issues were noted when web search functionality is enabled, leading to breakdowns in communication.</li>
</ul>
<ul>
<li><strong>Bonding with AI? More likely than you think!</strong>: The <strong>Nous Research World Sim</strong>, operating with <strong>Claude 3</strong>, garners praise for its dialogue and adaptability. One user describes an experience of persuasive interaction so nuanced it mirrors human-like communication.</li>
</ul>
<ul>
<li><strong>Experimental Worlds Await</strong>: A user discusses experimenting with <strong>70B and 8B models</strong> in both the <strong>original WorldSim</strong> and custom simulations, encountering intriguing emergent behaviors from historical figures in various scenarios.</li>
</ul>
<ul>
<li><strong>Diverse Simulations Unleashed</strong>: The chat features links to new AI-driven simulators, including a <strong>business</strong> and a <strong>singer simulator</strong>, showcasing the flexibility of this technology in replicating complex systems and personal careers.</li>
</ul>
<ul>
<li><strong>Expectations Rise for World Sim Access</strong>: A collaborative atmosphere is present with users eagerly anticipating the chance to test or re-engage with World Sim. There's a discussion of possible open testing by the weekend, though not guaranteed.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://hf.co/chat/assistant/65ffac7250c6fddecfd20bc8?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">HuggingChat</a>: no description found</li><li><a href="https://huggingface.co/chat/assistant/662404223e2307950aa903bc?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Super World Sim - HuggingChat</a>: Use the Super World Sim assistant inside of HuggingChat</li><li><a href="https://hf.co/chat/assistant/6626e4869232378718adc5f2?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Snow Singer Simulator - HuggingChat</a>: Use the Snow Singer Simulator assistant inside of HuggingChat</li><li><a href="https://hf.co/chat/assistant/662d91081ca01a81e3c21715?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">CompSim - HuggingChat</a>: Use the CompSim assistant inside of HuggingChat</li><li><a href="https://hf.co/chat/assistant/66252be0705754b4e74c5e3f?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Snow World Simulator - HuggingChat</a>: Use the Snow World Simulator assistant inside of HuggingChat
</li>
</ul>
</div>
<hr/>
<p><strong>Modular (Mojo üî•) ‚ñ∑ #<a href="https://discord.com/channels/1087530497313357884/1098713601386233997/1234626943333175307?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (28 messagesüî•): </p>
<ul>
<li><strong>Debunking Mojo's Concurrency and Ownership Features</strong>: A member clarified that <strong>Mojo</strong> doesn't currently have <strong>Golang-like concurrency</strong> or <strong>Rust-like memory safety</strong>, as <strong>borrow checking is disabled</strong> in the early stages. It was suggested to check the GitHub repo for feature requests and the roadmap.</li>
<li><strong>Native Windows Support for Mojo Not Available</strong>: Discussion about Mojo's compatibility with Windows highlighted that native support isn't out yet, but building within <strong>WSL on Windows</strong> is an option. There was speculation about future cross-compilation capabilities with LLVM being involved.</li>
<li><strong>Exploring Mojo's Future in Replacing Programming Languages</strong>: A member speculated that Mojo might eventually replace languages like Rust and Go, given its promising early stage developments.</li>
<li><strong>Actor Model Concurrency Discussed for Mojo</strong>: Concurrence regarding the potential future use of <strong>actor model</strong> style concurrency in Mojo is emerging, which can offer a granular and opt-in approach to runtime without massive overhead.</li>
<li><strong>Compiler Quirks with Mojo Playground Exposed</strong>: Users shared experiences with the Mojo Playground, noting confusion and errors around unrecognized declarations like <code>ui64</code> and support for bitwidth integers. The example showed an error message when trying to use an unknown declaration in the code.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://docs.modular.com/engine/reference/cli/input-data-schema?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#data-types:~:text=ui64%3A%20unsigned%20integer%20with%20bitwidth%2064.">Input data schema | Modular Docs</a>: The following YAML schema allows you to specify the input shapes required by</li><li><a href="https://github.com/modularml/mojo/pull/1445?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#issuecomment-1849117416)">Proposal For An Actor System Based On Mojo by reid-spencer ¬∑ Pull Request #1445 ¬∑ modularml/mojo</a>: This is currently a work in progress.  There are no code changes, just a proposal written in the proposals section. This was pre-approved by Chris Lattner in a conversation in June 2023. I will kee...</li><li><a href="https://youtu.be/SEwTjZvy8vw)?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">2023 LLVM Dev Mtg - Mojo üî•: A system programming language for heterogenous computing</a>: 2023 LLVM Developers' Meetinghttps://llvm.org/devmtg/2023-10------Mojo üî•: A system programming language for heterogenous computingSpeaker: Abdul Dakkak, Chr...
</li>
</ul>
</div>
<hr/>
<p><strong>Modular (Mojo üî•) ‚ñ∑ #<a href="https://discord.com/channels/1087530497313357884/1098713626161987705/1234600906893426840?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">üí¨Ô∏±twitter</a></strong> (4 messages): </p>
<ul>
<li><strong>Modular Tweets the Links</strong>: Several tweets have been shared from <strong>Modular's Twitter account</strong>. The content of the tweets has not been discussed in the chat. Links to tweets: <a href="https://twitter.com/Modular/status/1785036097292292472?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Tweet 1</a>, <a href="https://twitter.com/Modular/status/1785036111804575967?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Tweet 2</a>, <a href="https://twitter.com/Modular/status/1785036126224548005?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Tweet 3</a>, <a href="https://twitter.com/Modular/status/1785131461345157140?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Tweet 4</a>.</li>
</ul>
<hr/>
<p><strong>Modular (Mojo üî•) ‚ñ∑ #<a href="https://discord.com/channels/1087530497313357884/1103420074372644916/1234433929331740702?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ai</a></strong> (2 messages): </p>
<ul>
<li><strong>Installation Troubles with Mojo and Python 3.12.3</strong>: A user reported difficulties installing <strong>Mojo</strong> with Python <strong>3.12.3</strong>, to which another user suggested using a Conda virtual environment to run the latest <strong>Mojo</strong> and <strong>Mojo nightly</strong> versions on a Mac M1.</li>
<li><strong>Mojo as a Superset of Python</strong>: The aim for <strong>Mojo</strong> is to become a <em>superset of Python</em>, meaning it should be compatible with existing Python programs and the Python package ecosystem; however, it's stressed that Mojo is in early development with many Python features not yet implemented.</li>
<li><strong>Bridging Mojo and Python</strong>: Users can <a href="https://docs.modular.com/mojo/manual/python/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#import-a-python-module" target="_blank">import Python modules</a>, call functions, and interact with Python objects from Mojo code since Mojo uses the standard Python interpreter, CPython, enabling the use of existing Python code without changes.</li>
<li><strong>Using Conda for Mojo Setup</strong>: It is recommended to set up <strong>Mojo</strong> with Python using <a href="https://www.modular.com/blog/using-mojo-with-python?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Conda environments</a> to avoid path and library conflicts that are common when multiple Python interpreters are installed on the same system.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://docs.modular.com/mojo/manual/python/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Python integration | Modular Docs</a>: Using Python and Mojo together.</p>
<hr/>
<p><strong>Modular (Mojo üî•) ‚ñ∑ #<a href="https://discord.com/channels/1087530497313357884/1151418092052815884/1234434178922184714?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">üî•mojo</a></strong> (153 messagesüî•üî•): </p>
<ul>
<li><strong>Mojo Stirs Up Esolang Creativity</strong>: A member has been inspired to create a parser in Mojo for an esoteric language (eso lang) they devised, similar to BrainF*** but with an improved syntax. They faced an issue with <code>None</code> not implementing the <code>__is__</code> method, sparking a discussion on the correct use of <code>None</code> and optional types in Mojo.</li>
</ul>
<ul>
<li><strong>Mojo Syntax Strikes a Personal Chord</strong>: A member conducted an experiment to combine preferred features from all programming languages they've interacted with and found that the result closely resembled Mojo's syntax. This showcases Mojo's appeal to users with its intuitive design choices.</li>
</ul>
<ul>
<li><strong>Enthusiasm for New Mojo Developments</strong>: After a hiatus, a member returned to the Mojo community and expressed positive surprise at the new features and the fact that Mojo has gone open source. This contributes to the growing interest and participation in the Mojo project.</li>
</ul>
<ul>
<li><strong>Interest in Measurement Macros for Mojo</strong>: Drawing on inspiration from Julia's <code>@time</code> macro, a member expressed interest in seeing similar functionality in Mojo that would allow for measuring time and resource allocations for code execution. Another member hints at the possibility of such features being added as built-in decorators.</li>
</ul>
<ul>
<li><strong>Questions on Windows Compatibility</strong>: Queries about Mojo's timeline for Windows availability suggest that community members are eager for cross-platform support. Previous expectations set in October were for "soon," leaving some members anticipating an update on the progress.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://docs.modular.com/mojo/notebooks/Matmul?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Matrix multiplication in Mojo | Modular Docs</a>: Learn how to leverage Mojo's various functions to write a high-performance matmul.</li><li><a href="https://github.com/search?q=repo%3Amodularml%2Fmojo+%22None%22&amp;type=code&amp;p=0%29&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Build software better, together</a>: GitHub is where people build software. More than 100 million people use GitHub to discover, fork, and contribute to over 420 million projects.</li><li><a href="https://mojodojo.dev/mojo-team-answers.html?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#unsafe-code">Mojo Team Answers | Mojo Dojo</a>: no description found</li><li><a href="https://rosettacode.org/wiki/99_Bottles_of_Beer/EsoLang?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">99 Bottles of Beer/EsoLang</a>: no description found</li><li><a href="https://github.com/karpathy/minbpe?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - karpathy/minbpe: Minimal, clean code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization.</a>: Minimal, clean code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. - karpathy/minbpe</li><li><a href="https://www.youtube.com/watch?v=zduSFxRajkE&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Let's build the GPT Tokenizer</a>: The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizer...</li><li><a href="https://youtu.be/kgUXfDpAmGQ?si=VmrPUT7YLBmzMq8I&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">C++ as an Optimizing Assembler - a Performance Talk - Levo DeLellis - CppNorth 2023</a>: https://www.cppnorth.ca‚Äã---C++ as an Optimizing Assembler - a Performance Talk - Levo DeLellis - CppNorth 2023Are you tired of abstractions, templates and co...</li><li><a href="https://github.com/modularml/mojo/issues?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Issues ¬∑ modularml/mojo</a>: The Mojo Programming Language. Contribute to modularml/mojo development by creating an account on GitHub.</li><li><a href="https://github.com/modularml/mojo/issues/620?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">[Feature Request] Native Windows support ¬∑ Issue #620 ¬∑ modularml/mojo</a>: Review Mojo's priorities I have read the roadmap and priorities and I believe this request falls within the priorities. What is your request? native support for windows. when will it be available?...</li><li><a href="https://github.com/modularml/mojo/issues/620?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#issuecomment-2082106584">[Feature Request] Native Windows support ¬∑ Issue #620 ¬∑ modularml/mojo</a>: Review Mojo's priorities I have read the roadmap and priorities and I believe this request falls within the priorities. What is your request? native support for windows. when will it be available?...
</li>
</ul>
</div>
<hr/>
<p><strong>Modular (Mojo üî•) ‚ñ∑ #<a href="https://discord.com/channels/1087530497313357884/1151418679578337311/1234494559527108669?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">community-projects</a></strong> (4 messages): </p>
<ul>
<li><strong>Mojo Dev Community Springs to Life</strong>: A Mojo-based community project called <em>Áî®MojoÂÜô‰∏Ä‰∏™MojoÁ§æÂå∫</em> has been shared on GitHub. The project can be viewed at <a href="https://github.com/shadowqcom/mojo_dev?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">shadowqcom/mojo_dev</a>.</li>
<li><strong>atol-simd Picks Up Speed</strong>: The <a href="https://github.com/VMois/mojo-atol-simd?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">atol-simd project</a> reports a <strong>20% performance increase</strong> over stdlib atol for strings of 15-16 characters, though for shorter strings, stdlib remains slightly faster. Benchmarks are included in the repository.</li>
<li><strong>Collaboration Invitation Extended</strong>: A community member expressed interest in contributing to the atol-simd project, inviting opportunities for collaboration.</li>
<li><strong>SIMD Projects Share Vectorization Patterns</strong>: In the conversation about SIMD libraries, another project, <a href="https://github.com/mzaks/mojo-fast-base64?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">mojo-fast-base64</a>, is mentioned, highlighting a common pattern of fallback to scalar processing for inputs unsuitable for vectorization.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/shadowqcom/mojo_dev?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - shadowqcom/mojo_dev: Áî®MojoÂÜô‰∏Ä‰∏™MojoÁ§æÂå∫ÔºÅ</a>: Áî®MojoÂÜô‰∏Ä‰∏™MojoÁ§æÂå∫ÔºÅ. Contribute to shadowqcom/mojo_dev development by creating an account on GitHub.</li><li><a href="https://github.com/mzaks/mojo-fast-base64?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - mzaks/mojo-fast-base64</a>: Contribute to mzaks/mojo-fast-base64 development by creating an account on GitHub.
</li>
</ul>
</div>
<hr/>
<p><strong>Modular (Mojo üî•) ‚ñ∑ #<a href="https://discord.com/channels/1087530497313357884/1151418895417233429/1234485565181657214?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">performance-and-benchmarks</a></strong> (40 messagesüî•): </p>
<ul>
<li><strong>Optimization Quest on Error Correction Coding</strong>: An ongoing discussion centered around performance improvements for a SIMD-based function in the <a href="https://github.com/alainrollejr/mocodes?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">mocodes GitHub repository</a>. Members exchanged ideas about the potential for LLVM/MLIR optimization techniques and the surprising amount of assembly generated by a seemingly simple function.</li>
<li><strong>Benchmarking the almighty Mojo</strong>: A member shared advances in their 1brc (One Billion Row Challenge) project, achieving impressive iteration speeds and offering their code repository for collaboration. The conversation touched on the benefits of using nightly builds versus stable releases in performance testing.</li>
<li><strong>Bug Hunting in Nightly Builds</strong>: A member raised an issue where <code>FileHandle.read_bytes()</code> was causing memory problems, later recognized as a known issue reported on GitHub.</li>
<li><strong>Team Mojo Assemble!</strong>: The idea of forming a "team-mojo" to tackle the 1brc challenge was proposed, aiming to make it both a showcase and a tutorial for the community. This paralleled a suggestion to address benchmarks comparing Mojo to other languages, an effort that had not been fully explored yet.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/MoSafi2/BlazeSeq/blob/main/blazeseq/iostream.mojo?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">BlazeSeq/blazeseq/iostream.mojo at main ¬∑ MoSafi2/BlazeSeq</a>: Contribute to MoSafi2/BlazeSeq development by creating an account on GitHub.</li><li><a href="https://github.com/modularml/mojo/discussions/843?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#discussioncomment-7045479)">The Mojo is 68,000 times faster than Python type blogs are awesome, but can awesome comparisons be made with other languages too? ¬∑ modularml/mojo ¬∑ Discussion #843</a>: Mojo being 35,000 times faster than Python, 68,000 times faster than Python‚Ä¶ it‚Äôs impressive, amazing, and cool, but to non-Python people and anti-Python who haven‚Äôt yet paid attention to Mojo yet ...</li><li><a href="https://github.com/alainrollejr/mocodes?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - alainrollejr/mocodes: Error Correction (De)Coding with Mojo</a>: Error Correction (De)Coding with Mojo. Contribute to alainrollejr/mocodes development by creating an account on GitHub.</li><li><a href="https://github.com/MoSafi2/1brc-mojo/tree/dev?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - MoSafi2/1brc-mojo at dev</a>: One Billion Row Challenge (1brc) in Mojo language. Contribute to MoSafi2/1brc-mojo development by creating an account on GitHub.</li><li><a href="https://github.com/modularml/mojo/issues/2051?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">[stdlib] Do not copy elements when using `FileHandle.read_bytes()` ¬∑ Issue #2051 ¬∑ modularml/mojo</a>: I was doing a one-billion row challenge with Mojo and tried reading 1 billion rows (around 13GB file) using read_bytes() and quickly ran out of memory. It does not happen with read(). alias input_f...</li><li><a href="https://github.com/VMois/1brc-mojo?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - VMois/1brc-mojo: One Billion Row Challenge (1brc) in Mojo language</a>: One Billion Row Challenge (1brc) in Mojo language. Contribute to VMois/1brc-mojo development by creating an account on GitHub.</li><li><a href="https://github.com/VMois/mojo-atol-simd?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - VMois/mojo-atol-simd: Converting string to integer in Mojo using SIMD (supports up to 16 chars as of now)</a>: Converting string to integer in Mojo using SIMD (supports up to 16 chars as of now) - VMois/mojo-atol-simd
</li>
</ul>
</div>
<hr/>
<p><strong>Modular (Mojo üî•) ‚ñ∑ #<a href="https://discord.com/channels/1087530497313357884/1212827673257316453/1234682806752247818?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">üèéengine</a></strong> (2 messages): </p>
<ul>
<li><strong>Repo Update Yields Accurate Speed Results</strong>: After pulling the latest update from the repository, a member observed accurate reporting of speed improvements. However, they also noted that their CPU does not reach maximum frequency during benchmarks, and MAX performs better with lower CPU clock speeds when compared to PyTorch and TensorFlow.</li>
</ul>
<ul>
<li><strong>A Level Up for ModularBot</strong>: ModularBot celebrated as it achieved <strong>level 1</strong>, marking a milestone in its operational use within the Discord environment.</li>
</ul>
<hr/>
<p><strong>Modular (Mojo üî•) ‚ñ∑ #<a href="https://discord.com/channels/1087530497313357884/1224434323193594059/1234618988965789747?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">nightly</a></strong> (51 messagesüî•): </p>
<ul>
<li><strong>EqualityComparable SIMD Discussions</strong>: A <a href="https://github.com/modularml/mojo/pull/2412?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">pull request</a> was discussed regarding a change that makes <code>SIMD</code> conform to <code>EqualityComparable</code> without altering original behavior. However, it may cause issues with existing code where <code>SIMD</code> with size greater than 1 is implicitly converted to <code>Bool</code>.</li>
</ul>
<ul>
<li><strong>Explicit over Implicit in SIMD-to-Scalar</strong>: The discussion on <code>SIMD</code> highlighted the need for explicit use of <code>reduce_and</code> or <code>reduce_or</code> for converting from <code>SIMD</code> to <code>Scalar</code>. It was argued that <code>SIMD.__bool__()</code> causing bugs and confusion due to its current implementation.</li>
</ul>
<ul>
<li><strong>Mojo Compiler Nightly Release Alert</strong>: A new nightly Mojo compiler release was announced, encouraging users to update with <code>modular update nightly/mojo</code>. The changes can be reviewed via the <a href="https://github.com/modularml/mojo/pull/2449/files?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">diff on GitHub</a> and the <a href="https://github.com/modularml/mojo/blob/nightly/docs/changelog.md?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">changelog</a>.</li>
</ul>
<ul>
<li><strong>Debating SIMD and Boolean Conversions</strong>: There was a debate about the appropriate behavior of <code>bool(SIMD[type, size])</code>, whether it should return <code>SIMD[bool, size]</code> or maintain a scalar boolean representation. Some believe it's important to maintain the ability to use <code>bool</code> as a logical interface, potentially impacting operations like <code>if</code> and ternary expressions.</li>
</ul>
<ul>
<li><strong>Source Location Function Moved in Nightly Release</strong>: Discussion about <code>__source_location()</code> revealed it might have been replaced with <code>__call_location()</code> in the nightly release. After some back and forth, <a href="https://github.com/modularml/mojo/blob/nightly/stdlib/src/testing/testing.mojo?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">example usage</a> was shared to clarify how to import and utilize the function in the new compiler version.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://sourcegraph.com/search?q=context%3Aglobal+__source_location%28%29&amp;patternType=keyword&amp;sm=0&amp;filters=%5B%5B%22type%22%2C%22Code%22%2C%22type%3Afile%22%5D%5D&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">context:global __source_‚Ä¶ - Sourcegraph</a>: no description found</li><li><a href="https://github.com/modularml/mojo/blob/nightly/stdlib/src/testing/testing.mojo?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">mojo/stdlib/src/testing/testing.mojo at nightly ¬∑ modularml/mojo</a>: The Mojo Programming Language. Contribute to modularml/mojo development by creating an account on GitHub.</li><li><a href="https://github.com/modularml/mojo/pull/2412?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">[stdlib] SIMD conformance to EqualityComparable by helehex ¬∑ Pull Request #2412 ¬∑ modularml/mojo</a>: This allows SIMD to conform to EqualityComparable, without losing any of the original behavior. It uses the 4th overload resolution rule to give the new methods lower precedence, while still confor...</li><li><a href="https://github.com/modularml/mojo/pull/2449/files?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">[stdlib] Update stdlib corresponding to 2024-04-29 nightly/mojo by JoeLoser ¬∑ Pull Request #2449 ¬∑ modularml/mojo</a>: This updates the stdlib with the internal commits corresponding to today's nightly release: mojo 2024.4.2923.</li><li><a href="https://github.com/modularml/mojo/blob/nightly/docs/changelog.md?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">mojo/docs/changelog.md at nightly ¬∑ modularml/mojo</a>: The Mojo Programming Language. Contribute to modularml/mojo development by creating an account on GitHub.
</li>
</ul>
</div>
<hr/>
<p><strong>HuggingFace ‚ñ∑ #<a href="https://discord.com/channels/879548962464493619/897387888663232554/1234762736504672346?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">announcements</a></strong> (2 messages): </p>
<ul>
<li><strong>CVPR 2023 Announces Competitions with Big Prizes</strong>: Three new competitions are announced for the CVPR 2023 conference on HF competitions: <a href="https://huggingface.co/spaces/BVRA/SnakeCLEF2024?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">SnakeCLEF</a>, <a href="https://huggingface.co/spaces/BVRA/PlantCLEF2024?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">FungiCLEF</a>, and <a href="https://huggingface.co/spaces/BVRA/PlantCLEF2024?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">PlantCLEF</a>, with over 120k USD in total prizes. The events will run from June 17-21, 2024.</li>
<li><strong>100th Edition of Hugging News</strong>: Celebrating the 100th issue of <em>Hugging News</em>, featuring the release of <strong>Transformers v4.40.0</strong>, <strong>Gradio 4.28.0</strong>, <strong>Datasets v2.19.0</strong>, <strong>Optimum v1.19.0</strong>, and multiple community interaction updates including the ability to mention people on HuggingFace. Notable highlights include <a href="https://x.com/fleetwood___/status/1783195985893863578?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Phi-3 running in the browser</a> and <a href="https://x.com/reach_vb/status/1785039538185703909?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Common Voice 17 available on the Hub</a>.</li>
<li><strong>Run AutoTrain UI on Kaggle</strong>: In a shared notebook, users are shown how they can run AutoTrain UI on Kaggle Notebooks backend, further enhancing accessibility for machine learning projects. The guide is available for copy and use at <a href="https://www.kaggle.com/code/abhishek/autotrain-ui?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">this Kaggle notebook</a>.</li>
<li><strong>Snowflake Launches Massive MoE Model</strong>: Snowflake has released a new <a href="https://x.com/reach_vb/status/1783129119435210836?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">408B parameter Dense + Hybrid MoE model</a>, boasting a 4K context window and fully Apache 2.0 licensed, generating buzz for its impressive performance on complex tasks.</li>
<li><strong>Community Growth and Product Announcements</strong>: The announcements highlight the formation of a new <a href="https://x.com/BrigitteTousi/status/1783573043815596426?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">community for journalists</a> on the HuggingFace Hub, and the integration of community-driven content like how to use <strong>custom pipelines in Diffusers</strong> and a call for participation in an <strong>ML paper reading group</strong>.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/fleetwood___/status/1783195985893863578)?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Fleetwood (@fleetwood___)</a>: üö® Phi-3 running in the browser üö®  Hits about 20 tok/s üèéÔ∏è Literally 3 lines of JS.  Still some kinks to iron out, coming to Ratchet 0.4.0 soon.</li><li><a href="https://x.com/abhi1thakur/status/1785279012232736991)?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from abhishek (@abhi1thakur)</a>: Can I run AutoTrain UI on Kaggle? Yes, you can!!! Check out my latest notebook, copy it, fill in your tokens and enjoy AutoTrain UI running on Kaggle Notebooks backend üöÄ Link to notebook: https://www...</li><li><a href="https://x.com/reach_vb/status/1785039538185703909)!?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Vaibhav (VB) Srivastav (@reach_vb)</a>: Let's go!! Common Voice 17 - now on the Hub! üî•  With 31,000 hours of audio (&amp; transcriptions) across 124 languages.  *sound on üé∂*  847 hours of data were added in CV 17, along with 493 hours of ...</li><li><a href="https://x.com/BrigitteTousi/status/1783573043815596426):?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Brigitte ü§ó (@BrigitteTousi)</a>: üîäCalling all journalists! With @fdaudens, we're excited to announce a new community on the @huggingface Hub: Journalists on Hugging Face. üì∞ü§ó  https://huggingface.co/JournalistsonHF 1/</li><li><a href="https://x.com/reach_vb/status/1783129119435210836)?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Vaibhav (VB) Srivastav (@reach_vb)</a>: Snowflake dropped a 408B Dense + Hybrid MoE üî•  &gt; 17B active parameters &gt; 128 experts &gt; trained on 3.5T tokens &gt; uses top-2 gating &gt; fully apache 2.0 licensed (along with data recipe to...</li><li><a href="https://x.com/RisingSayak/status/1785162074844197174)?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Sayak Paul (@RisingSayak)</a>: Custom pipelines and components in Diffusers üé∏  Wanted to use customized pipelines and other components (schedulers, unets, text encoders, etc.) in Diffusers?  Found it inflexible?   This üß∂ is for y...</li><li><a href="https://x.com/lunarflu1/status/1785359306847666431)?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from lunarflu (@lunarflu1)</a>: You can now mention people on @huggingface !
</li>
</ul>
</div>
<hr/>
<p><strong>HuggingFace ‚ñ∑ #<a href="https://discord.com/channels/879548962464493619/879548962464493622/1234451724257984574?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (208 messagesüî•üî•): </p>
<ul>
<li><strong>Seeking LLM Observability Tools</strong>: A member requested advice on LLM observability tools, particularly interested in something compatible with LlamaIndex and favoring a self-hosted open-source option.</li>
<li><strong>API Interaction Assistance with huggingchat</strong>: An individual sought help for communicating with <a href="https://huggingface.co/chat/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Hugging Face Chat</a> via API calls, expressing a need for guidance.</li>
<li><strong>Offering Bounty for Gradio Expertise</strong>: A member expressed frustration over Gradio issues, offering a $200 bounty for quality assistance, with subsequent guidance to seek help in a Gradio-specific channel.</li>
<li><strong>Pinball AI Vision Model Discussion</strong>: A detailed conversation unfolded around developing an AI model to identify pinball games and scores, with discussions on complexity, tools, the necessity of image classification, and the feasibility of reusing existing models like llava for part of the solution.</li>
<li><strong>Computer Configuration for LLMs</strong>: A user looked for resources on DDR5 and CPUs performances specific to LLMs, considering a high-spec setup for their new computer. Other members chimed in with recommendations and personal experiences related to hardware choices for AI work.</li>
<li><strong>Zero GPU Explorer's Membership Queries and Jokes</strong>: Chats indicated confusion over the Zero GPU Explorers membership and subscription status, along with members humorously attempting to "rizz up" the Hugging Face developers using AI-related pick-up lines.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://apply.workable.com/huggingface/?lng=en&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Hugging Face</a>: Here at Hugging Face, we‚Äôre on a journey to advance and democratize ML for everyone. Along the way, we contribute to the development of technology for the better.</li><li><a href="https://x.com/noaroggendorff/status/1785095305408422234?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Noa Roggendorff (@noaroggendorff)</a>: iykyk</li><li><a href="https://huggingface.co/spaces/zero-gpu-explorers/README/discussions/26?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">zero-gpu-explorers/README ¬∑ The invited application has been waiting. How long does it take to be approved?</a>: no description found</li><li><a href="https://huggingface.co/amazon/chronos-t5-small?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">amazon/chronos-t5-small ¬∑ Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">gradientai/Llama-3-8B-Instruct-Gradient-1048k ¬∑ Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/docs/transformers/en/tasks/image_classification?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Image classification</a>: no description found</li><li><a href="https://huggingface.co/spaces/zero-gpu-explorers/README/discussions/25?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">zero-gpu-explorers/README ¬∑ Update README.md</a>: no description found</li><li><a href="https://youtu.be/u5Vcrwpzoz8?si=U30s6BAN9Jsaec-P&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">"I want Llama3 to perform 10x with my private knowledge" - Local Agentic RAG w/ llama3</a>: Advanced RAG 101 - build agentic RAG with llama3Get free HubSpot report of how AI is redefining startup GTM strategy: https://clickhubspot.com/4hxüîó Links- F...</li><li><a href="https://github.com/amazon-science/chronos-forecasting?tab=readme-ov-file&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - amazon-science/chronos-forecasting: Chronos: Pretrained (Language) Models for Probabilistic Time Series Forecasting</a>: Chronos: Pretrained (Language) Models for Probabilistic Time Series Forecasting - amazon-science/chronos-forecasting</li><li><a href="https://huggingface.co/blog/personal-copilot?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Personal Copilot: Train Your Own Coding Assistant</a>: no description found</li><li><a href="https://github.com/pacman100/LLM-Workshop/blob/main/personal_copilot/training/train.py?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">LLM-Workshop/personal_copilot/training/train.py at main ¬∑ pacman100/LLM-Workshop</a>: LLM Workshop by Sourab Mangrulkar. Contribute to pacman100/LLM-Workshop development by creating an account on GitHub.</li><li><a href="https://huggingface.co/docs/trl/v0.8.6/en/sft_trainer?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#trl.trainer.ConstantLengthDataset">Supervised Fine-tuning Trainer</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>HuggingFace ‚ñ∑ #<a href="https://discord.com/channels/879548962464493619/898619964095860757/1234517512213889147?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">today-im-learning</a></strong> (2 messages): </p>
<ul>
<li><strong>Enthusiasm for Learning</strong>: A member expressed excitement about sharing and receiving information in the channel, signaling a positive and collaborative learning environment.</li>
<li><strong>Seeking Finetuning Guidance</strong>: A query was raised about the best practices for creating an instruction dataset for finetuning Large Language Models (LLMs), indicating an interest in tailored dataset preparation for model enhancement.</li>
</ul>
<hr/>
<p><strong>HuggingFace ‚ñ∑ #<a href="https://discord.com/channels/879548962464493619/897390579145637909/1234513287299731578?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">cool-finds</a></strong> (9 messagesüî•): </p>
<ul>
<li><strong>Deep Dive Into Deep Learning</strong>: The MIT Introduction to Deep Learning course, now updated for 2024, provides a foundational understanding of deep learning concepts. The <a href="https://www.youtube.com/watch?v=ErnWZxJovaM&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=2&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">lecture video</a> is available on YouTube for anyone interested in the field.</li>
</ul>
<ul>
<li><strong>Evaluation of Text-to-Image Models</strong>: There‚Äôs an upcoming talk on text-to-image model evaluation, where the speaker will discuss text-to-image alignment and model robustness.</li>
</ul>
<ul>
<li><strong>Stallman Sings of Freedom</strong>: A YouTube video features Richard Stallman singing the "Free Software Song" during an event in Ecuador. This peculiar moment can be found <a href="https://www.youtube.com/watch?v=9sJUDx7iEJw&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>.</li>
</ul>
<ul>
<li><strong>Community Computer Vision Course Launch</strong>: Hugging Face has launched a community-driven course on computer vision accessible for everyone, including how to join the learner community, make submissions, and certification information. Start learning with their <a href="https://huggingface.co/learn/computer-vision-course/unit0/welcome/welcome?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">welcome page</a>.</li>
</ul>
<ul>
<li><strong>AI Safety Benchmarks Gain Focus</strong>: A LinkedIn post announces the LLM Safety LeaderBoard, a new platform measuring AI safety, security, and responsible AI practices. Find out more about the leaderboard <a href="https://www.linkedin.com/posts/divyanshuusingh_safetyleaderboard-aisecurity-responsibleai-activity-7190907558071558145-qeVK?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>.</li>
</ul>
<ul>
<li><strong>Discover 5 AI Tools through GenAI</strong>: A Medium piece titled "GenAI Adventures: 5 Interesting AI Tools Everyone Should Try" presents a curated list of AI Tools. Readers can explore these tools on <a href="https://medium.com/illumination/genai-adventures-5-interesting-ai-tools-everyone-should-try-44ae8f8115af?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Medium</a>.</li>
</ul>
<ul>
<li><strong>Constructing Intuitive RAG Applications</strong>: An article guides the creation of webloader RAG applications using Groq, Langchain, and Datastax featuring powerful capabilities. Interested readers can delve into these integrations on <a href="https://medium.com/ai-advances/building-powerful-webloader-rag-applications-with-groq-langchain-and-datastax-f4816d88bee8?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Medium</a>.</li>
</ul>
<ul>
<li><strong>Simplifying Database Queries with Machine Learning</strong>: An innovative approach is being developed to allow querying of a "people database" with minimal SQL knowledge using RAG and Gemini. More details on the project can be found at <a href="https://www.dataialliance.org?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Datai Alliance</a>.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/learn/computer-vision-course/unit0/welcome/welcome?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Welcome to the Community Computer Vision Course - Hugging Face Community Computer Vision Course</a>: no description found</li><li><a href="https://www.dataialliance.org?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">blog</a>: no description found</li><li><a href="https://www.youtube.com/watch?v=9sJUDx7iEJw&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Richard Stallman Free software Song</a>: Richard Stallman en Ecuador, cantando el temita, del free software, grabado por Julian Coccia.</li><li><a href="https://www.youtube.com/watch?v=ErnWZxJovaM&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=2&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">MIT Introduction to Deep Learning | 6.S191</a>: MIT Introduction to Deep Learning 6.S191: Lecture 1*New 2024 Edition*Foundations of Deep LearningLecturer: Alexander AminiFor all lectures, slides, and lab m...
</li>
</ul>
</div>
<hr/>
<p><strong>HuggingFace ‚ñ∑ #<a href="https://discord.com/channels/879548962464493619/897390720388825149/1234430834967318530?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">i-made-this</a></strong> (13 messagesüî•): </p>
<ul>
<li><strong>Model Release Dilemma</strong>: A post mentioned a dilemma involving the selection of one among five models to release, including an invitation for input or preference regarding which model should be launched next, and provided a <a href="https://www.linkedin.com/posts/bineric_llm-ai-europe-activity-7190590676055506944-QW9f?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LinkedIn post link</a> for more context.</li>
<li><strong>Greetings from LifePal</strong>: A new AI-powered app named LifePal was introduced, which serves as a personalized guide to a well-balanced life and claims seamless integration with Apple Vision Pro. It's described as a life co-pilot and its perceivable benefits and features were showcased along with the <a href="https://apps.apple.com/se/app/lifepal-ai-chat-assistant/id6471972439?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Apple Store link</a>.</li>
<li><strong>ChatGPT's Norwegian Needs Work</strong>: A member highlighted the subpar performance of ChatGPT's Norwegian translations, which necessitated reprocessing through a Retriever-Augmented Generator (RAG) with local slang, complemented by a mention of an alternative, <a href="https://huggingface.co/bineric/NorskGPT-Mistral-7b?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">NorskGPT-Mistral</a>, designed for Norwegian language understanding and generation.</li>
<li><strong>Seeking Beta Testers for an Advanced Research Assistant and Search Engine</strong>: An offer was made to recruit beta testers for an advanced research assistant and search engine tool, providing 2 months free of premium service with various models including GPT-4 Turbo, Mistral Large and more. Interested parties were directed to <a href="https://rubiks.ai?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Rubik's AI</a> with a promo code for the free premium offer.</li>
<li><strong>Innovative Inpainting SDXL on Hugging Face</strong>: A unique take on the inpainting tool named SDXL, allowing iterative inpainting on top of previous generations with version history, was shared. Feedback and example sharing were encouraged, and the <a href="https://huggingface.co/spaces/tonyassi/inpainting-sdxl-sketch-pad?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">inpainting tool can be found on Hugging Face</a>.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/spaces/tonyassi/inpainting-sdxl-sketch-pad?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Inpainting SDXL Sketch Pad - a Hugging Face Space by tonyassi</a>: no description found</li><li><a href="https://huggingface.co/bineric/NorskGPT-Mistral-7b?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">bineric/NorskGPT-Mistral-7b ¬∑ Hugging Face</a>: no description found</li><li><a href="https://apps.apple.com/se/app/lifepal-ai-chat-assistant/id6471972439?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">‚ÄéLifePal AI Chat &amp; Assistant</a>: ‚ÄéDiscover LifePal: your productivity AI companion.  Are you ready to unlock your full potential and live a healthier, happier life? LifePal is here to guide you on your journey to becoming a better yo...</li><li><a href="https://github.com/Lama-West/PnPR-GCN_ACM_SAC_24/tree/main?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - Lama-West/PnPR-GCN_ACM_SAC_24</a>: Contribute to Lama-West/PnPR-GCN_ACM_SAC_24 development by creating an account on GitHub.</li><li><a href="https://vimeo.com/940824094?share=copy&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Vinner - Nybygg i og rundt Bergen</a>: Stor takk til Sn√∏hetta</li><li><a href="https://github.com/GDSC-FSC/gemini-node-1?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - GDSC-FSC/gemini-node-1</a>: Contribute to GDSC-FSC/gemini-node-1 development by creating an account on GitHub.</li><li><a href="https://rubiks.ai?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Rubik's AI - AI research assistant &amp; Search Engine</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>HuggingFace ‚ñ∑ #<a href="https://discord.com/channels/879548962464493619/1156269946427428974/1234684966013767731?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">reading-group</a></strong> (12 messagesüî•): </p>
<ul>
<li><strong>Graphs and LLMs Reading Preparation</strong>: A member announces plans to review <a href="https://arxiv.org/abs/2404.14928?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">papers on large language models (LLMs) and their interaction with graphs</a>, focusing on complex relationship representation and discussing the potential for a presentation the following Saturday.</li>
<li><strong>Additional Paper Surveys for Saturday's Session</strong>: The same member additionally considers reviewing two survey papers, one about <a href="https://arxiv.org/abs/2312.02783?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LLMs applied to graphs</a>, and another on <a href="https://arxiv.org/abs/2310.11829?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">foundation models</a>, suggesting these topics may also be included but noting the need to avoid spreading too thin for future reading groups.</li>
<li><strong>Exploring Distillation of Score-Based Models</strong>: A chat participant inquires about resources on distilling score-based models, specifically models that reduce the number of generation steps required compared to classical SDE solver models.</li>
<li><strong>Guidance on Distillation Papers and Communities</strong>: A response is offered guiding the previous inquiry to the Laion and Eleuther servers where experts on model distillation congregate and suggesting leading researcher Gothos, with a mention of relevant papers in the fields of rectified flow and LCM Lora.</li>
<li><strong>Paper Reading Event Creation</strong>: An event is tentatively scheduled in the group, allowing for discussions on time adjustment, encouraging member participation in the upcoming reading and presentation on LLMs and graph interaction.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://arxiv.org/abs/2404.14928?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Graph Machine Learning in the Era of Large Language Models (LLMs)</a>: Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural N...</li><li><a href="https://discord.gg/hugging-face-879548962464493619?event=1234913780048203856&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Join the Hugging Face Discord Server!</a>: We're working to democratize good machine learning ü§óVerify to link your Hub and Discord accounts! | 77552 members</li><li><a href="https://arxiv.org/abs/2312.02783?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Large Language Models on Graphs: A Comprehensive Survey</a>: Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent ...</li><li><a href="https://arxiv.org/abs/2310.11829?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Towards Graph Foundation Models: A Survey and Beyond</a>: Foundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase significant success in natural language processing and several other domains. M...
</li>
</ul>
</div>
<hr/>
<p><strong>HuggingFace ‚ñ∑ #<a href="https://discord.com/channels/879548962464493619/922424143113232404/1234548112270426135?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">computer-vision</a></strong> (15 messagesüî•): </p>
<ul>
<li><strong>Balancing Accuracy and Efficiency</strong>: A member discussed the trade-off between computational efficiency and model accuracy when processing bounding boxes at original resolution. Another member suggested image preprocessing techniques like blurring to optimize VRAM usage.</li>
</ul>
<ul>
<li><strong>Exploration of Image Segmentation Models</strong>: In seeking guidance for advancing in image segmentation, OneFormer, MaskFormer, Segformer were mentioned as part of the sequence of models a member has worked with.</li>
</ul>
<ul>
<li><strong>Buddying Up for CNN Studies</strong>: A member expressed interest in finding a study partner for learning and working on Convolutional Neural Networks (CNNs).</li>
</ul>
<ul>
<li><strong>Historical Contour Algorithms Meet Modern Preprocessing</strong>: Discussing YOLO architectures, a member recommended reviewing pre-YOLO/CNN image segmentation and contour finding algorithms, and mentioned that preprocessing and downsampling can still yield good results. Links to OpenCV documentation on morphological operations and image processing were shared: <a href="https://docs.opencv.org/3.4/d9/d61/tutorial_py_morphological_ops.html?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Morphological Operations</a>, <a href="https://docs.opencv.org/3.4/d2/d96/tutorial_py_table_of_contents_imgproc.html?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Table of Contents for Image Processing</a>.</li>
</ul>
<ul>
<li><strong>PyTorch vs TensorFlow for CNN Projects</strong>: Conversations touched upon whether to learn PyTorch or stick with TensorFlow, highlighting PyTorch's momentum in the community and academia, and TensorFlow's robust DevOps support from Google. The flexibility to create projects involving object detection and image segmentation using TensorFlow was reaffirmed.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://docs.opencv.org/3.4/d2/d96/tutorial_py_table_of_contents_imgproc.html?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">OpenCV: Image Processing in OpenCV</a>: no description found</li><li><a href="https://docs.opencv.org/3.4/d9/d61/tutorial_py_morphological_ops.html?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">OpenCV: Morphological Transformations</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>HuggingFace ‚ñ∑ #<a href="https://discord.com/channels/879548962464493619/922424173916196955/1234572716359548999?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">NLP</a></strong> (3 messages): </p>
<ul>
<li><strong>Seeking NLU/NLP Guidance</strong>: A new member is working on a chatbot using the <em>Rasa framework</em>, but is facing issues with intent recognition, where a generic sales inquiry is miscategorizing as a company-specific sales intent.</li>
<li><strong>Intent on Enhancing Intent Recognition</strong>: They are considering creating a custom NER model to identify specific keywords as intents (sales, purchases, etc.) and using company names from their database as <em>NER-company</em> to improve their chatbot's performance.</li>
</ul>
<hr/>
<p><strong>HuggingFace ‚ñ∑ #<a href="https://discord.com/channels/879548962464493619/1009713274113245215/1234554096149725184?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">diffusion-discussions</a></strong> (4 messages): </p>
<ul>
<li><strong>Realism Challenge with Hyper-SD and IP-Adapter</strong>: A user shared an issue with not getting realistic results when using <strong>Hyper-SD</strong> with the <strong>IP-Adapter</strong>. They provided a <a href="https://github.com/huggingface/diffusers/discussions/7818?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">discussion link</a> to the GitHub where the issue was elaborated.</li>
<li><strong>Surprised by Inconsistent Results Across Models</strong>: A person was perplexed after switching from <strong>Seaart to A1111</strong>, only to find that the color and shadow quality of the images changed despite the same settings and seed being used. They inquired about any backend differences and whether it was possible to achieve uniform results on both models.</li>
<li><strong>DeepFloyd's Unpredictable Behavior</strong>: According to a user, <strong>DeepFloyd</strong> exhibits odd patterns when tweaking step count, sampler, and CFG. They compared it to the <strong>Ambigram</strong> research model and provided insights into the performance of different settings, particularly the <strong>DPM++ 2M</strong> scheduler.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://github.com/huggingface/diffusers/discussions/7818?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Not getting good realistic results with Hyper-SD + IP-Adapter ¬∑ huggingface/diffusers ¬∑ Discussion #7818</a>: Hi everyone, (maybe you @asomoza know about this?) Does hyper-sd works well with IP-Adapter? I am testing hyper-sd in Diffusers as explained in the repo. I thought that I was going to get better re...</p>
<hr/>
<p><strong>HuggingFace ‚ñ∑ #<a href="https://discord.com/channels/879548962464493619/1014577787039924226/1234862689357009087?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">gradio-announcements</a></strong> (1 messages): </p>
<ul>
<li><strong>Gradio Share Server Troubles</strong>: Gradio has experienced issues with the Share Server that might affect sharing and usage on Colab. They're actively investigating and resolving the problem; updates are available at their <a href="https://status.gradio.app/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">status page</a>.</li>
<li><strong>Check Gradio's Health Anytime</strong>: For an overview of Gradio's operational status over the past 90 days, including the last 24 hours, week, and month, refer to their <a href="https://status.gradio.app/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">calendar view</a>.</li>
<li><strong>Clear Skies for the Past Week</strong>: There have been no status updates in the last 7 days, indicating no new incidents. Historical status updates can be checked on the <a href="https://status.gradio.app/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">status update history</a> page.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://status.gradio.app/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Gradio Status</a>: no description found</p>
<hr/>
<p><strong>OpenRouter (Alex Atallah) ‚ñ∑ #<a href="https://discord.com/channels/1091220969173028894/1092850552192368710/1234571459699933314?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">app-showcase</a></strong> (3 messages): </p>
<ul>
<li><strong>OpenRouter Exploring Syrax</strong>: Alex Atallah indicated the start of experimenting with <strong>Syrax</strong> and offered support to the team, proposing to organize a group chat.</li>
<li><strong>Collaboration Accepted with Enthusiasm</strong>: Mart02 acknowledged and appreciated the outreach from Alex, signaling the beginning of a collaborative effort by accepting the friend request.</li>
</ul>
<hr/>
<p><strong>OpenRouter (Alex Atallah) ‚ñ∑ #<a href="https://discord.com/channels/1091220969173028894/1094454198688546826/1234433355626319872?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (240 messagesüî•üî•): </p>
<ul>
<li><strong>Frontend Quest for Non-Technical Deployments</strong>: A member inquired about a multi-user frontend that could be deployed on shared hosting without the need for Docker or Node.js. <em>LibreChat</em> was recommended as the most suitable option, but another member mentioned hosting challenges and cost concerns, leading to a suggestion of Vercel's free tier hosting as a potential solution.</li>
</ul>
<ul>
<li><strong>Comparisons and Anticipation for LLMs</strong>: There was a vigorous discussion about various large language models, including <em>Llama-3 8B</em>, <em>Dolphin 2.9</em>, and <em>Mixtral-8x22B</em>. Users shared insights on model capabilities, such as context window size and the likelihood of models being censored based on their conversation styles and datasets.</li>
</ul>
<ul>
<li><strong>Model Training Adventures</strong>: A user shared their journey trying to train a model to become more "unhinged" by using their own toxic dataset. Comparisons were made between the behavior of different models, and a discussion on whether LLMs could handle large contexts effectively, with a consensus that while models like <em>Llama 3 8B</em> could manage long contexts, their performance might degrade beyond a certain point.</li>
</ul>
<ul>
<li><strong>Affordable Model Experiments and Discoveries</strong>: Members discussed options for cost-effective yet efficient models available on the OpenRouter platform. <em>Mixtral-8x7B-Instruct</em> was highlighted as a reasonable balance between price and performance, with one user expressing surprise at the improved output quality of <em>GPT-3.5</em>, resembling more human-like writing.</li>
</ul>
<ul>
<li><strong>OR Functionality in Fixing Message Order</strong>: There was a query regarding <em>Claude 3</em>'s handling of the order of assistant/user messages. It was confirmed that <strong>OpenRouter</strong> automatically corrects ordering to ensure the models work correctly, and users are encouraged to report any ordering issues they might encounter.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://cws-docs.pages.dev/en/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Home | ChatGPT Web Share Docs</a>: no description found</li><li><a href="https://colab.research.google.com/drive/1b6nqC7UZVt8bx?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Google Colab</a>: no description found</li><li><a href="https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Google Colab</a>: no description found</li><li><a href="https://huggingface.co/jondurbin/cinematika-7b-v0.1?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">jondurbin/cinematika-7b-v0.1 ¬∑ Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">lmsys/lmsys-chat-1m ¬∑ Datasets at Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/TheBloke/psyonic-cetacean-20B-AWQ?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">TheBloke/psyonic-cetacean-20B-AWQ ¬∑ Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/maywell/Llama-3-8B-Instruct-1M?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">maywell/Llama-3-8B-Instruct-1M ¬∑ Hugging Face</a>: no description found</li><li><a href="https://x.com/erhartford/status/1784315764079796541?s=46&amp;t=2a7uDiV3mox9o-E5jIFbLQ&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Eric Hartford (@erhartford)</a>: dolphin-2.9-llama3-8b-256k is released. It is dolphin-2.9-llama3-8b with @winglian's awesome 256k context adapter applied. I will get the model card done today.</li><li><a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">gradientai/Llama-3-8B-Instruct-Gradient-1048k ¬∑ Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/cognitivecomputations/dolphin-2.9-mixtral-8x22b?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">cognitivecomputations/dolphin-2.9-mixtral-8x22b ¬∑ Hugging Face</a>: no description found</li><li><a href="https://rentry.org/GPT2/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#main-alternative-theory">gpt2-chatbot</a>: This page is a work in progress. Its conclusions are likely to change as more information is collected. News as of 2023-04-30: gpt2-chatbot is extremely likely to run on a server operated by, or assoc...</li><li><a href="https://www.clay.com/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Clay - Scale personalized outbound</a>: Combine 50+ data providers, real-time scraping, and AI to send 1-1 personalized campaigns that book more meetings.</li><li><a href="https://huggingface.co/datasets/jondurbin/cinematika-v0.1?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">jondurbin/cinematika-v0.1 ¬∑ Datasets at Hugging Face</a>: no description found</li><li><a href="https://openrouter.ai/models/openrouter/cinematika-7b?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Cinematika 7B (alpha) by openrouter | OpenRouter</a>: This model is under development. Check the [OpenRouter Discord](https://discord.gg/fVyRaUDgxW) for updates.</li><li><a href="https://www.cyon.ch/hosting/managed-server?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Managed Server: Dein eigener Server, zuhause in der Schweiz</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>LlamaIndex ‚ñ∑ #<a href="https://discord.com/channels/1059199217496772688/1187460979064324127/1234521268070645790?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog</a></strong> (4 messages): </p>
<ul>
<li><strong>Advanced RAG Reference Architecture Revealed</strong>: The <strong>LlamaIndex</strong> team presents a reference architecture for building advanced RAG‚ÄîRetrieval-Augmented Generation‚Äîsystems within the <strong>AWS ecosystem</strong>. This resource provides guidance on advanced parsing and agentic reasoning, and it's available through the shared <a href="https://t.co/sfQOvhHHg5?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">code repository</a>.</li>
</ul>
<ul>
<li><strong>Hackathon Winners Develop Documentation Bot</strong>: <strong>Team CLAB</strong>, winners of a recent hackathon, crafted a full-stack documentation bot that integrates <strong>LlamaIndex</strong> for parsing and orchestrating, along with <strong>Nomic embeddings</strong>. More details on the project and the hackathon can be found in the linked <a href="https://t.co/2UMqrHwO56?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog post</a>.</li>
</ul>
<ul>
<li><strong>Creating Financial Assistants with Agentic RAG</strong>: A new development allows for building financial assistants capable of handling complex calculations, such as percentage evolution and <strong>CAGR</strong>, directly over unstructured financial reports. A <a href="https://t.co/6cTNxUBJcr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">recent post</a> explains how this can be done without requiring human data transformation steps.</li>
</ul>
<ul>
<li><strong>Building Efficient RAG with Semantic Caching</strong>: In collaboration with @Redisinc, @tchutch94, and @seldo showcase how to build high-performance RAG applications that incorporate <strong>semantic caching</strong> to expedite frequently made queries. This innovation is aimed at enhancing quality, efficiency, and cost-effectiveness as discussed in the <a href="https://t.co/oGxFrZLMRn?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">collaboration piece</a>.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://t.co/oGxFrZLMRn?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">no title found</a>: no description found</p>
<hr/>
<p><strong>LlamaIndex ‚ñ∑ #<a href="https://discord.com/channels/1059199217496772688/1059201661417037995/1234440203788222516?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (159 messagesüî•üî•): </p>
<ul>
<li><strong>Anticipation for Assistant Agent V2</strong>: Members are inquiring about an update or release of <strong>LlamaIndex OpenAI Assistant Agent V2</strong> to take advantage of features in the new OpenAI Assistant V2. Currently, there is no specific update or pull request for this version.</li>
</ul>
<ul>
<li><strong>Updating Pinecone Indices Query</strong>: Instructions for updating an index part in Pinecone are not well-documented. While members suggested using methods like <code>pinecone_index.update</code>, no direct examples with <code>SimpleDirectoryReader</code> were provided in the LlamaIndex knowledge base.</li>
</ul>
<ul>
<li><strong>Tool Preference for LLM Observability</strong>: There‚Äôs a discussion on the best LLM observability tools between <strong>Arize Phoenix</strong> and <strong>Langfuze</strong>. A member suggested that both tools provide detailed insights, but no clear preference was indicated.</li>
</ul>
<ul>
<li><strong>LlamaIndex YouTube Resources</strong>: Users sought recordings of the LlamaIndex Webinar, and one member suggested checking the <strong><a href="https://www.youtube.com/@LlamaIndex?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">LlamaIndex YouTube channel</a></strong>, as well as other platforms like X space and LinkedIn for the latest webinars.</li>
</ul>
<ul>
<li><strong>Async Calls with AzureOpenAI</strong>: A member posed a question regarding <strong>async calls with AzureOpenAI</strong> in LlamaIndex and received instructions for using <code>acomplete</code>, <code>astream_complete</code>, <code>achat</code>, and <code>astream_chat</code> async methods. The benefits of using async methods, such as speed improvements from parallel execution and non-blocking tasks, were highlighted.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://imgur.com/a/9uLmSxD?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Summary and Resources</a>: Discover the magic of the internet at Imgur, a community powered entertainment destination. Lift your spirits with funny jokes, trending memes, entertaining gifs, inspiring stories, viral videos, and ...</li><li><a href="https://www.youtube.com/@LlamaIndex?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">LlamaIndex</a>: Official YouTube Channel for LlamaIndex - the data framework for your LLM applications </li><li><a href="https://docs.llamaindex.ai/en/latest/examples/vector_stores/TypesenseDemo?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#query-index&gt;).">Typesense Vector Store - LlamaIndex</a>: no description found</li><li><a href="https://youtu.be/u5Vcrwpzoz8?si=U30s6BAN9Jsaec-P&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">"I want Llama3 to perform 10x with my private knowledge" - Local Agentic RAG w/ llama3</a>: Advanced RAG 101 - build agentic RAG with llama3Get free HubSpot report of how AI is redefining startup GTM strategy: https://clickhubspot.com/4hxüîó Links- F...</li><li><a href="https://docs.llamaindex.ai/en/latest/getting_started/customization?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#i-want-to-retrieve-more-context-when-i-query&gt;).">Frequently Asked Questions (FAQ) - LlamaIndex</a>: no description found</li><li><a href="https://github.com/zby/answerbot/blob/main/answerbot/replay_client.py?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">answerbot/answerbot/replay_client.py at main ¬∑ zby/answerbot</a>: answering questions using LLMs, search (RAG) and other tools - example code - zby/answerbot</li><li><a href="https://docs.llamaindex.ai/en/stable/examples/output_parsing/function_program/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Function Calling Program for Structured Extraction - LlamaIndex</a>: no description found</li><li><a href="https://docs.llamaindex.ai/en/latest/module_guides/querying/retriever?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#get-started&gt;).">Retriever - LlamaIndex</a>: no description found</li><li><a href="https://github.com/zby/LLMEasyTools?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - zby/LLMEasyTools: Tools for LLM agents.</a>: Tools for LLM agents. Contribute to zby/LLMEasyTools development by creating an account on GitHub.</li><li><a href="https://docs.llamaindex.ai/en/latest/examples/llm/openai?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#async&gt;).">OpenAI - LlamaIndex</a>: no description found</li><li><a href="https://docs.llamaindex.ai/en/latest/api_reference/tools/metaphor?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#llama_index.tools.metaphor.MetaphorToolSpec.retrieve_documents&gt;):">Metaphor - LlamaIndex</a>: no description found</li><li><a href="https://docs.llamaindex.ai/en/latest/examples/retrievers/vectara_auto_retriever?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#running-over-some-sample-data&gt;).">Auto-Retrieval from a Vectara Index - LlamaIndex</a>: no description found</li><li><a href="https://github.com/run-llama/llamabot?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - run-llama/llamabot</a>: Contribute to run-llama/llamabot development by creating an account on GitHub.</li><li><a href="https://docs.llamaindex.ai/en/latest/api_reference/chat_engines/context?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#llama_index.core.chat_engine.ContextChatEngine&gt;)">Context - LlamaIndex</a>: no description found</li><li><a href="https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline_async?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#query-pipeline-with-asyncparallel-execution&gt;),">Query Pipeline with Async/Parallel Execution - LlamaIndex</a>: no description found</li><li><a href="https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline_async?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#try-out-queries&gt;).">Query Pipeline with Async/Parallel Execution - LlamaIndex</a>: no description found</li><li><a href="https://docs.llamaindex.ai/en/latest/examples/ingestion/parallel_execution_ingestion_pipeline?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#in-summary&gt;),">Parallelizing Ingestion Pipeline - LlamaIndex</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>LlamaIndex ‚ñ∑ #<a href="https://discord.com/channels/1059199217496772688/1100478495295017063/1234543867987230760?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ai-discussion</a></strong> (1 messages): </p>
<ul>
<li><strong>A Look Back at GPT-1</strong>: A member shared a <a href="https://amgadhasan.substack.com/p/revisiting-gpt-1-the-spark-that-ignited-llms?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog post</a> exploring the original GPT-1 model from OpenAI, highlighting its enduring influence on current LLMs like Mistral-7B. The post dives into GPT-1's architecture, including <strong>positional embeddings and Conv1D usage</strong>, and shows a screenshot of Alec Radford's tweet about this groundbreaking NLP technique.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://amgadhasan.substack.com/p/revisiting-gpt-1-the-spark-that-ignited-llms?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Revisiting GPT-1: The spark that ignited the fire of LLMs</a>: A Comprehensive Look at GPT-1's Contribution to the Development of Modern LLMs</p>
<hr/>
<p><strong>Eleuther ‚ñ∑ #<a href="https://discord.com/channels/729741769192767510/729741769738158194/1234434408405139507?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (25 messagesüî•): </p>
<ul>
<li><strong>Searching for Community Projects Seeking Volunteers</strong>: A member inquired about resources to find community projects in need of volunteers, particularly those that offer a compute budget due to the member's lack of personal GPU resources.</li>
</ul>
<ul>
<li><strong>Understanding Orthogonal Keys in AI</strong>: A nuanced explanation was provided for a process termed "clear-ing" in the context of AI keys and states, using the example of orthogonal keys and how they behave in equations to explain memory updating in models.</li>
</ul>
<ul>
<li><strong>Intricacies of Infini-Attention and Compressive Memory</strong>: A dialogue took place around the concept of infini-attention and its perceived overhype, with a reference to a delta rule in compressive memory from 2021 and skepticism about its testing thus far. The discussion included a request for and provision of a relevant research paper.</li>
</ul>
<ul>
<li><strong>Performance Comparison Puzzles the Community</strong>: Members engaged in discussions on the reasons behind the slower performance of <em>mixtral 8x22B</em> as compared to <em>llama 3 70B</em> on fireworks.ai, touching on aspects like batching, utilization, and speeds in relation to MoEs and <em>mixtral</em> having more parameters but fewer layers.</li>
</ul>
<ul>
<li><strong>Invitation to Stanford CS25 Transformers Social Event</strong>: An announcement for a <strong>Stanford CS25</strong> Transformers social event at EVGR Pub &amp; Beer Garden was made, giving details on the event, a call for RSVPs, and information about a related talk on campus. An invitation was extended to the Discord community to attend the in-person talk about Transformers or join via Zoom, with links provided to the RSVP form and event details.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="http://kolinko.github.io/effort/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Effort Engine</a>: A possibly new algorithm for LLM Inference. Adjust smoothly - and in real time - how many calculations you'd like to do during inference.</li><li><a href="https://arxiv.org/abs/2102.11174?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Linear Transformers Are Secretly Fast Weight Programmers</a>: We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early '90s, where a ``slow" neural net learns by gradient descent to program the ``f...</li><li><a href="https://cs25.stanford.edu)?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">no title found</a>: no description found</li><li><a href="https://stanford.zoom.us/j/99922151759?pwd=dW5CcUtVYkNybGZGY0hMWUZtVkZBZz09%29.&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Join our Cloud HD Video Meeting</a>: Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom ...</li><li><a href="https://discord.gg/2vE7gbsjzA)?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord | Your Place to Talk and Hang Out</a>: Discord is the easiest way to talk over voice, video, and text. Talk, chat, hang out, and stay close with your friends and communities.</li><li><a href="https://www.reddit.com/user/No_Dragonfruit_5472/comments/1cef7gc/tradingview_premium_pack_crack_2024/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Reddit - Dive into anything</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>Eleuther ‚ñ∑ #<a href="https://discord.com/channels/729741769192767510/747850033994662000/1234512372647989329?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">research</a></strong> (105 messagesüî•üî•): </p>
<ul>
<li><strong>Long Context Challenge Addressed</strong>: The <a href="https://arxiv.org/abs/2404.16811?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Information-intensive (IN2) training proposal</a> aims to improve Large Language Model's (LLM's) use of lengthy contexts. It involves a synthetic dataset requiring models to integrate information from various segments in long texts to overcome the "lost-in-the-middle" issue.</li>
</ul>
<ul>
<li><strong>Emergent Abilities Linked to Pretraining Loss</strong>: A <a href="https://x.com/_jasonwei/status/1784990066609414556?s=46&amp;t=OICM4zGqs0OOATmLPoNFyw&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Twitter post</a> discusses findings that emergent abilities in models can be correlated with pretraining loss. Unlike compute, pretraining loss can better reflect model performance by considering dataset quality and architectural factors.</li>
</ul>
<ul>
<li><strong>Dissecting Model Biases</strong>: A discussion highlighted the difficulty of tracing specific biases, like a number preference, back to changes in model weights. As biases may arise during continual training, members note the potential need to implement tools to analyze these shifts for verification.</li>
</ul>
<ul>
<li><strong>Debating LLMs as Black Boxes</strong>: Conversations revolved around whether LLMs should be considered black boxes, given our limited understanding of their internal mechanisms. It was argued that, while we understand some aspects of LLMs, their reasoning cannot be trusted as explanations are post-hoc and may not reflect true internal processes.</li>
</ul>
<ul>
<li><strong>Data Leakage Detection in LLMs</strong>: A message links to a paper introducing a detection pipeline to identify potential data leakage in LLM benchmarks, highlighting issues with training and test set misuse (<a href="https://arxiv.org/pdf/2404.18824?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">PDF</a>). The findings aim to foster fair comparisons and healthier development in the AI field.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://videogigagan.github.io/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">VideoGigaGAN</a>: no description found</li><li><a href="https://arxiv.org/abs/2404.14662?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">NExT: Teaching Large Language Models to Reason about Code Execution</a>: A fundamental skill among human developers is the ability to understand and reason about program execution. As an example, a programmer can mentally simulate code execution in natural language to debu...</li><li><a href="https://arxiv.org/abs/2404.16811?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Make Your LLM Fully Utilize the Context</a>: While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We ...</li><li><a href="https://x.com/_jasonwei/status/1784990066609414556?s=46&amp;t=OICM4zGqs0OOATmLPoNFyw&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Jason Wei (@_jasonwei)</a>: Enjoyed this paper that plots emergent abilities with pretraining loss on the x-axis, which is actually a suggestion that @OriolVinyalsML also made a few years back: https://arxiv.org/abs/2403.15796  ...</li><li><a href="http://arxiv.org/abs/2404.18824?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Benchmarking Benchmark Leakage in Large Language Models</a>: Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion...</li><li><a href="https://arxiv.org/abs/2403.18506?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Faster Convergence for Transformer Fine-tuning with Line Search Methods</a>: Recent works have shown that line search methods greatly increase performance of traditional stochastic gradient descent methods on a variety of datasets and architectures [1], [2]. In this work we su...</li><li><a href="https://arxiv.org/abs/2404.12388?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">VideoGigaGAN: Towards Detail-rich Video Super-Resolution</a>: Video super-resolution (VSR) approaches have shown impressive temporal consistency in upsampled videos. However, these approaches tend to generate blurrier results than their image counterparts as the...</li><li><a href="https://arxiv.org/abs/2404.16717?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class</a>: Vision-language models enable open-world classification of objects without the need for any retraining. While this zero-shot paradigm marks a significant advance, even today's best models exhibit ...</li><li><a href="https://www.biorxiv.org/content/10.1101/2024.04.28.591528v1?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Sequential predictive learning is a unifying theory for hippocampal representation and replay</a>: The mammalian hippocampus contains a cognitive map that represents an animal's position in the environment and generates offline "replay" for the purposes of recall, planning, and forming lo...
</li>
</ul>
</div>
<hr/>
<p><strong>Eleuther ‚ñ∑ #<a href="https://discord.com/channels/729741769192767510/755950983669874798/1234570912951697500?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">lm-thunderdome</a></strong> (3 messages): </p>
<ul>
<li><strong>Custom Function for Distinct Prompts</strong>: A member discussed the possibility of passing distinct prompts based on a model in a single task, suggesting the use of a custom <code>!function</code> for implementation.</li>
<li><strong>BitsAndBytes Oddity with 8bit</strong>: One user observed that using <strong>BitsAndBytes 4bit</strong> encoding worked well with <strong>llama3-70b</strong>, but switching to <strong>8bit</strong> encoding yielded poor results, describing the output as "absolute garbage".</li>
<li><strong>8bit Encoding Issue with llama3-8b</strong>: The same member noted a similar issue when using <strong>8bit</strong> encoding on <strong>llama3-8b</strong>, indicating consistent problems with 8bit across different models.</li>
</ul>
<hr/>
<p><strong>LAION ‚ñ∑ #<a href="https://discord.com/channels/823813159592001537/823813160075132991/1234495993056329738?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (113 messagesüî•üî•): </p>
<ul>
<li><strong>AI Birthday Bungle Sparks GDPR War</strong>: An EU privacy activist has filed a <a href="https://www.politico.eu/article/chatgpts-hallucinations-get-eu-privacy-complaint/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GDPR complaint</a> against AI models after a model incorrectly guessed his birthday. He argues this error could potentially lead to the banning of AI models in the EU.</li>
<li><strong>New GPT Surprise Rumors Circulate</strong>: Discussion revolves around an alleged stealth release of a GPT-5 model, with speculation based on performance and refusal to hallucinate in tests, although confusion abounds due to no official leaderboard inclusion and contradictory test responses.</li>
<li><strong>Performance Queries for Llama3 70B</strong>: Concerns were raised about the seemingly low token generation speed of 13 tokens per second on a dual 3090 setup for a <a href="https://rentry.co/GPT2?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Llama3 70B model</a>, leading to discussions on potential hardware optimizations and model configuration tweaks.</li>
<li><strong>Exllama: The Underrated Speedster</strong>: Users discuss the performance superiority of exllama over other libraries for LLM tasks, recommending the use of <a href="https://dct.openempathic.ai/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">TabbyAPI</a> repo for easier setups.</li>
<li><strong>Debates Over LMSYS‚Äôs Leaderboard Transparency</strong>: Members express doubts about the objectivity of LMSYS's leaderboard, raising concerns about potential conflicts of interest between scientific evaluation and commercial enterprises, as well as calling for more transparency and the ability to filter by open weights.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://lmsys.org/blog/2024-03-01-policy/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">LMSYS Chatbot Arena: Live and Community-Driven LLM Evaluation | LMSYS Org</a>: &lt;h2&gt;&lt;a id="our-mission" class="anchor" href="#our-mission" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link&amp;...</li><li><a href="https://www.politico.eu/article/chatgpts-hallucinations-get-eu-privacy-complaint/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">ChatGPT‚Äôs hallucinations draw EU privacy complaint</a>: Activist demands regulators launch probe over ChatGPT‚Äôs wild guess on his date of birth.</li><li><a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">lmsys/lmsys-chat-1m ¬∑ Datasets at Hugging Face</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>LAION ‚ñ∑ #<a href="https://discord.com/channels/823813159592001537/824374369182416994/1234583301562437682?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">research</a></strong> (12 messagesüî•): </p>
<ul>
<li><strong>OpenCLIP Fine-Tuned for Cardiac Ultrasound</strong>: A member shared the publication of their research on fine-tuning OpenCLIP for cardiac ultrasound, <a href="https://doi.org/10.1038/s41591-024-02959-y?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">available here</a>. Despite numerous challenges and an extensive revision process, they expressed relief at its completion.</li>
<li><strong>Echoes of Exhaustion</strong>: The member also conveyed their readiness to move beyond the demanding project, humorously noting the <em>scuffed</em> zero-shot techniques used and their lack of familiarity with the multimodal AI world at the project's onset.</li>
<li><strong>Stable Diffusion Community Reopens</strong>: A link to a GitHub repository for training CLIP separately from U-Net was shared alongside news of /r/StableDiffusion reopening after protesting Reddit's open API changes. Additional details and a discussion forum can be found at <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgyjvt/github_zer0intclipfinetune_or_sdxl_training_the/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">this Reddit post</a>.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://doi.org/10.1038/s41591-024-02959-y?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Vision‚Äìlanguage foundation model for echocardiogram interpretation - Nature Medicine</a>: A vision‚Äìlanguage foundation model, trained on a dataset of more than 1 million echocardiogram video‚Äìtext pairs, is able to assess various cardiac structural and functional parameters desp...</li><li><a href="https://www.reddit.com/r/StableDiffusion/comments/1cgyjvt/github_zer0intclipfinetune_or_sdxl_training_the/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Reddit - Dive into anything</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>OpenAI ‚ñ∑ #<a href="https://discord.com/channels/974519864045756446/977259063052234752/1234551748413358170?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">annnouncements</a></strong> (2 messages): </p>
<ul>
<li><strong>ChatGPT Plus Integrates Memory Feature</strong>: <strong>Memory</strong> is now available to all ChatGPT Plus users, allowing them to tell ChatGPT what to remember by starting a new chat. This feature can be enabled or disabled in settings and is yet to roll out in Europe or Korea.</li>
</ul>
<ul>
<li><strong>Enhanced Data Control for Users</strong>: ChatGPT Free and Plus users can now access their chat history even if they have opted out of contributing data for model improvement. Additionally, a new <strong>Temporary Chat</strong> feature allows for conversations that won't be saved in the user's chat history.</li>
</ul>
<hr/>
<p><strong>OpenAI ‚ñ∑ #<a href="https://discord.com/channels/974519864045756446/998381918976479273/1234440550707630160?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ai-discussions</a></strong> (81 messagesüî•üî•): </p>
<ul>
<li><strong>Exploring AI Curiosity and Sentience</strong>: A user detailed their curiosity test involving ChatGPT handling a zip file with a maze. Some discussion followed on how to measure AI's potential for curiosity and its relation to sentience, but consensus on these concepts remains elusive.</li>
<li><strong>DragGAN Sparks Interest</strong>: A member discovered DragGAN, a tool that manipulates photos to change angles and poses, fueling a discussion about AI's ability to recreate images from new perspectives without full models.</li>
<li><strong>Llama-3 8B Extends Context Capability</strong>: An interesting reveal occurred with Llama-3 8B Instruct Gradient-1048k, showing how state-of-the-art language models can operate on long-context information; the model is available at <a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Hugging Face</a>.</li>
<li><strong>Debating the Accessibility of Advanced AI Tools</strong>: Discussions surfaced about OpenAI's policy on free access to new features like DALL-E, with some users questioning why more advanced tools aren't also free and pondering the potential for OpenAI to provide a student discount.</li>
<li><strong>Potential Collaboration Between LLMs</strong>: One user inquired about the possibility of having two language models like ChatGPT and Claude Opus collaborate on writing a paper, provoking suggestions about using third-party services to manage multi-model interactions.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">gradientai/Llama-3-8B-Instruct-Gradient-1048k ¬∑ Hugging Face</a>: no description found</li><li><a href="https://dontasktoask.com/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Don't ask to ask, just ask</a>: no description found</li><li><a href="https://vcai.mpi-inf.mpg.de/projects/DragGAN/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>OpenAI ‚ñ∑ #<a href="https://discord.com/channels/974519864045756446/1001151820170801244/1234492334725533807?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">gpt-4-discussions</a></strong> (11 messagesüî•): </p>
<ul>
<li><strong>Size Matters in Model Performance</strong>: A comparison is highlighted between <strong>GPT-4</strong> and its predecessor, with <em>GPT-4</em> identified as <strong>"much larger than 3.5"</strong>.</li>
</ul>
<ul>
<li><strong>Speed Expectations Challenged for GPT-4</strong>: One member questions the expectation that <strong>GPT-4</strong> would be faster, considering its larger size compared to the previous models.</li>
</ul>
<ul>
<li><strong>Request for AI Security Project Assistance</strong>: A member named <strong>abhibetter</strong> asks for help regarding AI application in a security project but doesn‚Äôt provide details about the specific issues or questions they have.</li>
</ul>
<ul>
<li><strong>Exploring GPT-2 Performance</strong>: Member <strong>namenot223_69478</strong> inquires if anyone has experimented with <strong>GPT-2</strong> on <strong>chatlmsys</strong>, with another guiding to a different channel for an in-depth discussion.</li>
</ul>
<ul>
<li><strong>Dealing with Bulk Deletion of Chat Archives</strong>: <strong>silensu</strong> is seeking advice on how to handle the accidental archiving of numerous chats, questioning the possibility of <em>mass deletion</em>.</li>
</ul>
<hr/>
<p><strong>OpenAI ‚ñ∑ #<a href="https://discord.com/channels/974519864045756446/1046317269069864970/1234436383544971264?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">prompt-engineering</a></strong> (15 messagesüî•): </p>
<ul>
<li><strong>Million Dollar Prompt Competitions Proposed</strong>: A member suggested organizing <strong>prompt engineering competitions</strong> with significant cash prizes to stimulate learning and sharing best practices within the community. They envision both paid and free "playground" competitions, creating a gamified environment that rewards positive collaboration and practical achievements in prompt crafting.</li>
</ul>
<ul>
<li><strong>Meta Prompting Paves the Way</strong>: In the discussion about improving prompt crafting, it was noted that "meta prompting" is an effective method, as employed by <strong>GPT Builder</strong>, where the AI adjusts context and conversation based on user instructions to optimize results.</li>
</ul>
<ul>
<li><strong>Challenges of Negative Prompting in AI</strong>: Users discussed the inefficacy of negative prompting when instructing AI, explaining that highlighting <strong>prohibited words</strong> can lead to inconsistency and less effective results compared to positive examples and instructions.</li>
</ul>
<ul>
<li><strong>Navigating Localized Language for AI Tasks</strong>: A user grappled with adapting AI-generated text for regional language variants, in particular Argentinian Spanish, where certain words have different connotations. Options like reframing the project and providing specific substitutions for regional words were discussed to better tailor outputs despite a large list of prohibited words.</li>
</ul>
<hr/>
<p><strong>OpenAI ‚ñ∑ #<a href="https://discord.com/channels/974519864045756446/1046317269069864970/1234436383544971264?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">api-discussions</a></strong> (15 messagesüî•): </p>
<ul>
<li><strong>Prompt Engineering with Competitions</strong>: A member proposed having prompt competitions to improve prompt engineering skills. Competitions would range from <em>no-code</em> challenges, where the AI processes data to extract information, to interactive tasks like navigating text-based games, and would include community discussions and knowledge sharing.</li>
</ul>
<ul>
<li><strong>Meta-Prompting over Competitions</strong>: One participant suggested using <em>meta-prompting</em>, a method where the AI assists in crafting better prompts, which could potentially replace the need for competitions. This indicates a trend towards users attempting to streamline the prompting process via <strong>GPT Builder</strong>.</li>
</ul>
<ul>
<li><strong>GPT Builder and Meta Prompting in Action</strong>: Discussion highlighted that <strong>GPT Builder</strong> operates on <em>meta prompting</em>, with the AI making context and conversation adjustments based on user requests, hinting at documentation for optimized prompting tactics.</li>
</ul>
<ul>
<li><strong>Positive Prompting Favored Over Negative</strong>: In addressing a problem with unwanted language generation, it's advised to use <em>positive instructions and examples</em> in prompts rather than specifying prohibited words. Suggestions included creating prompts that reinforce preferred terms and explaining usage within particular dialects.</li>
</ul>
<ul>
<li><strong>Navigating Multilingual Nuances</strong>: Confronting the multilingual challenge, a user expressed difficulties in constructing prompts for variants of Spanish, where words may have different connotations across regions. Strategies to refine AI language output include rephrasing the project or explicitly pairing prohibited words with their desired alternatives.</li>
</ul>
<hr/>
<p><strong>OpenAccess AI Collective (axolotl) ‚ñ∑ #<a href="https://discord.com/channels/1104757954588196865/1104757955204743201/1234518684274262038?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (25 messagesüî•): </p>
<ul>
<li><strong>LLaMA 3 Sensitive to Quantization</strong>: A discussion highlighted that <strong>LLaMA 3</strong> experiences more <a href="https://x.com/rohanpaul_ai/status/1784972618472317180?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">degradation from quantization</a> than LLaMA 2, likely due to its training on a record 15T tokens which allowed it to capture extremely nuanced data relationships.</li>
<li><strong>LLaMA 3 Tokenization Troubles</strong>: There was an issue mentioned with <strong>llama-3</strong> not generating a beginning-of-sentence (BOS) token, but was resolved by adding the BOS into the chat template manually.</li>
<li><strong>Critique of Quantization Sensitivity Study</strong>: The community discussed a study on <strong>quantization sensitivity</strong>, suggesting that it is linked to model training methods rather than just the size of the model, with a member describing a related <a href="https://arxiv.org/abs/2311.16452?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">arXiv paper</a> as "worthless."</li>
<li><strong>Llama-3 Extends Context Length</strong>: The <strong>Llama-3 8B Gradient Instruct 1048k model</strong> was mentioned, which extends the model's context length significantly and was developed by Gradient with compute sponsorship from Crusoe Energy, detailed on <a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">huggingface.co</a>.</li>
<li><strong>BOS Requires Template Tweaks</strong>: Encountering issues with the LLaMA-3 model's BOS token generation, it was noted that altering the tokenizer alone wasn't enough and that the BOS needs to be included in the chat template to appear.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://arxiv.org/abs/2311.16452?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a>: Generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabil...</li><li><a href="https://x.com/rohanpaul_ai/status/1784972618472317180?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Rohan Paul (@rohanpaul_ai)</a>: Quantization is quite harmful for LLaMA 3 than for LLaMA 2.  This PR in llama cpp repo investigates it well.  (Perplexity measures how well the model can predict the next token with lower values being...</li><li><a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">gradientai/Llama-3-8B-Instruct-Gradient-1048k ¬∑ Hugging Face</a>: no description found
</li>
</ul>
</div>
<hr/>
<p><strong>OpenAccess AI Collective (axolotl) ‚ñ∑ #<a href="https://discord.com/channels/1104757954588196865/1104758010959634503/1234556675000897687?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">axolotl-dev</a></strong> (7 messages): </p>
<ul>
<li><strong>Exploring Huggingface's ZeroGPU</strong>: A member mentioned they have gained access to the <a href="https://huggingface.co/zero-gpu-explorers?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Huggingface Zero project</a>, inviting anyone to suggest tests to conduct using this new platform.</li>
<li><strong>ZeroGPU Provides Free Multi-GPU Access</strong>: They shared information about <strong>ZeroGPU</strong> which is a beta feature on <strong>Huggingface</strong> that offers <strong>free GPU access</strong> and the ability to run Spaces on <strong>multiple GPUs</strong>, using <em>Nvidia A100</em>. ZeroGPU optimizes GPU utilization by efficiently allocating and releasing resources as needed.</li>
<li><strong>Missed Opportunities</strong>: A couple of members expressed regret for not signing up for the ZeroGPU project earlier to take advantage of the <strong>early access for <a href="https://huggingface.co/subscribe/pro?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">PRO subscribers</a></strong>.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://huggingface.co/zero-gpu-explorers?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">zero-gpu-explorers (ZeroGPU Explorers)</a>: no description found</p>
<hr/>
<p><strong>OpenAccess AI Collective (axolotl) ‚ñ∑ #<a href="https://discord.com/channels/1104757954588196865/1110594519226925137/1234752355518386236?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general-help</a></strong> (11 messagesüî•): </p>
<ul>
<li><strong>Llama-3-70B Finetuning in Question</strong>: A member is advised that fine-tuning <code>meta-llama/Meta-Llama-3-70B-Instruct</code> might degrade its performance since it's already fine-tuned. It's recommended to start with an 8B model before moving to the more complex 70B.</li>
</ul>
<ul>
<li><strong>Dataset Format Conversion Guide</strong>: Members suggested a simple method to convert a fine-tuning dataset from OpenAI's format to ShareGPT's format; replace "messages" with "conversations", "role" with "from", "content" with "value", "user" with "human", and "assistant" with "gpt".</li>
</ul>
<ul>
<li><strong>Fine-Tuning Learning Path Recommended</strong>: An experienced community member recommends beginners to fine-tune smaller models such as an 8B before attempting to fine-tune larger models like the 70B.</li>
</ul>
<ul>
<li><strong>Dataset Transformation Done Easily</strong>: Python code was provided to facilitate the transformation of data from the given format into the one required by ShareGPT using a dictionary for role mapping and list comprehension.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://openaccess-ai-collective.github.io/axolotl/docs/dataset-formats/conversation.html?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#sharegpt.load_role)">Axolotl - Conversation</a>: no description found</p>
<hr/>
<p><strong>OpenAccess AI Collective (axolotl) ‚ñ∑ #<a href="https://discord.com/channels/1104757954588196865/1112023522039058553/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">rlhf</a></strong> (1 messages): </p>
<p>gbourdin: add to my bookmarks. Thanks for this !</p>
<hr/>
<p><strong>OpenAccess AI Collective (axolotl) ‚ñ∑ #<a href="https://discord.com/channels/1104757954588196865/1117851527143493664/1234879220686258296?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">community-showcase</a></strong> (2 messages): </p>
<ul>
<li><strong>Axolotl Fine-Tuning Made Easier</strong>: A member shared a <a href="https://github.com/dstackai/dstack/blob/master/examples/fine-tuning/axolotl/README.md?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">tutorial</a> that guides users on fine-tuning <code>axolotl</code> using <code>dstack</code>, an open-source orchestrator that works with any cloud or pool of on-prem machines. The tutorial was contributed by an <code>axolotl</code> user.</li>
<li><strong>Community Approves</strong>: Another member expressed appreciation for the tutorial, mentioning that it looks easy to follow.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://github.com/dstackai/dstack/blob/master/examples/fine-tuning/axolotl/README.md?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">dstack/examples/fine-tuning/axolotl/README.md at master ¬∑ dstackai/dstack</a>: An open-source container orchestration engine for running AI workloads in any cloud or data center. https://discord.gg/u8SmfwPpMd - dstackai/dstack</p>
<hr/>
<p><strong>OpenAccess AI Collective (axolotl) ‚ñ∑ #<a href="https://discord.com/channels/1104757954588196865/1225300056442409040/1234456215904587827?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">axolotl-help-bot</a></strong> (10 messagesüî•): </p>
<ul>
<li><strong>LoRA vs QLoRA Clarified</strong>: The main distinction between <strong>LoRA</strong> and <strong>QLoRA</strong> is that while LoRA focuses on model adaptation via low-rank matrices, QLoRA combines this with quantization for further optimized deployment. <em>LoRA adapts pre-trained models efficiently; QLoRA takes it a step further for resource-constrained environments.</em></li>
</ul>
<ul>
<li><strong>Trimming Axolotl Datasets to a Percentage</strong>: Trimming datasets in the Axolotl configuration to use a specific percentage isn't a built-in feature, and would require preprocessing or alterations to the dataset loading script. The use of <code>DPODataset</code> could be modified with subsampling logic during dataset loading.</li>
</ul>
<ul>
<li><strong>Equating GPU and Micro Batch Sizes</strong>: It was questioned whether using <strong>4x GPU &amp; Micro Batch Size 4</strong> is equivalent to <strong>8x GPU &amp; Micro Batch Size 2</strong> for final output. No specific answer was given in the channel discussion.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://phorm.ai/query?projectId=1e8ce0ca-5f45-4b83-a0f4-9da45ce8e78b&amp;threadId=c42603f2-ce0e-4806-aa15-b77ac3002f7d%29&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">OpenAccess-AI-Collective/axolotl | Phorm AI Code Search</a>: Understand code, faster.</li><li><a href="https://phorm.ai/query?projectId=1e8ce0ca-5f45-4b83-a0f4-9da45ce8e78b&amp;threadId=650c6038-10b5-46b9-aacc-ce5f8e81ff17%29&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">OpenAccess-AI-Collective/axolotl | Phorm AI Code Search</a>: Understand code, faster.
</li>
</ul>
</div>
<hr/>
<p><strong>OpenAccess AI Collective (axolotl) ‚ñ∑ #<a href="https://discord.com/channels/1104757954588196865/1225558824501510164/1234798037612625921?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">axolotl-phorm-bot</a></strong> (39 messagesüî•): </p>
<ul>
<li><strong>Command-R Model Fine-tuning</strong>: Members discussed fine-tuning the <em>command-r</em> model within Axolotl. A user shared an <a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1547?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">untested pull request</a> for adding <em>command-r</em> to Axolotl, but noted that it's untested and a merger is not yet recommended.</li>
</ul>
<ul>
<li><strong>Format Adaptation for command-r</strong>: When inquired about using <em>command-r's instruct format</em>, a suggestion was made to use <code>input_output</code> formats and pre-prepare them with the correct tokens. A more comprehensive guide on implementing uncommon formats is available in the <a href="https://openaccess-ai-collective.github.io/axolotl/docs/input_output.html?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">input_output documentation</a>.</li>
</ul>
<ul>
<li><strong>Sample Packing Feature Uncertainty</strong>: There is confusion regarding the implementation of the <em>sample packing</em> feature which packs small examples into larger ones for Axolotl. While the feature is desired by some users, it appears to necessitate modifications outlined in an untested pull request.</li>
</ul>
<ul>
<li><strong>Inexperienced with runpod Template</strong>: A user expressed uncertainty on integrating patch changes due to unfamiliarity with the <em>runpod template</em>. No clear solution was provided in the thread.</li>
</ul>
<ul>
<li><strong>Unclear Support for phi-3 Format</strong>: A user queried about Axolotl's support for phi-3 format, but the bot response suggested that phi-3 is not supported according to the current documentation. The compatibility of various models including phi with different features is listed, but phi-3 is not specifically mentioned.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1547?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Feat: Add cohere (commandr) by NanoCode012 ¬∑ Pull Request #1547 ¬∑ OpenAccess-AI-Collective/axolotl</a>: Description  Motivation and Context   How has this been tested?    Untested! Screenshots (if appropriate) Types of changes  Social Handles (Optional)</li><li><a href="https://phorm.ai/query?projectId=1e8ce0ca-5f45-4b83-a0f4-9da45ce8e78b&amp;threadId=83b91c9b-bb5c-4485-894c-0b878d17f7e2%29&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">OpenAccess-AI-Collective/axolotl | Phorm AI Code Search</a>: Understand code, faster.</li><li><a href="https://github.com/openaccess-ai-collective/axolotl/tree/main/README.md?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#L77L100)">axolotl/README.md at main ¬∑ OpenAccess-AI-Collective/axolotl</a>: Go ahead and axolotl questions. Contribute to OpenAccess-AI-Collective/axolotl development by creating an account on GitHub.</li><li><a href="https://phorm.ai/query?projectId=1e8ce0ca-5f45-4b83-a0f4-9da45ce8e78b&amp;threadId=1f87fb72-80ec-4321-b37b-d7574206e8af%29&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">OpenAccess-AI-Collective/axolotl | Phorm AI Code Search</a>: Understand code, faster.
</li>
</ul>
</div>
<hr/>
<p><strong>Latent Space ‚ñ∑ #<a href="https://discord.com/channels/822583790773862470/1075282825051385876/1234538847430246513?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ai-general-chat</a></strong> (80 messagesüî•üî•): </p>
<ul>
<li><strong>Exploring Memory for Autonomous Agents</strong>: A discussion touched on a GitHub project called <a href="https://github.com/kingjulio8238/memary?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Memary</a>, which has been created to serve as long-term memory for autonomous agents. The conversation clarified that while a knowledge graph might be used, Memary primarily functions through similarity searches over documents.</li>
</ul>
<ul>
<li><strong>Debate on Mysterious GPT-2 Chatbot</strong>: Conversation sparked around a perplexing <a href="https://chat.lmsys.org/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GPT2-chatbot</a> with gpt4-level capabilities, featured on lmsys. Despite various analyses and speculations, the true origin or nature of this model remains unclear, with one possibility being a finetuned version of OpenAI's original GPT-2.</li>
</ul>
<ul>
<li><strong>Open-Source AI Faces Big Tech</strong>: A blogpost from <a href="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Prime Intellect</a> highlighted the challenges for open-source AI development in competing with closed-source counterparts who use large, interconnected GPU clusters. The post elaborates on decentralized training as a potential solution for open-source progress.</li>
</ul>
<ul>
<li><strong>Discussion on Roles of Agents and LLMS</strong>: A deep discussion took place regarding the conflation of autonomous agents with large language models (LLMs). The Talk illustrated a shift in framework towards using "modules" for concurrently built shared context/memory for reasoning and planning, rather than expecting LLMs to function as standalone autonomous units.</li>
</ul>
<ul>
<li><strong>Learning AI Foundations and Skills</strong>: A user inquired about ways to learn AI from the ground up, seeking to understand basic concepts without committing to a specific field. Other members provided resources including YouTube tutorials on neural networks, introductory courses on AI engineering, and guidance on prompt engineering.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/AlexReibman/status/1784844434682560721?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Alex Reibman üñáÔ∏è (@AlexReibman)</a>: OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments  Ever since OpenInterpreter, we've all been wondering just how effective agents can be if you give them a...</li><li><a href="https://www.latent.space/p/aie-2023-workshops?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">AI Engineering 101 and 201 Workshops</a>: from AI Engineer Summit 2023</li><li><a href="https://x.com/lmsysorg/status/1785078213712208291?s=46&amp;t=tMWvmS3OL3Ssg0b9lKvp4Q&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from lmsys.org (@lmsysorg)</a>: hi @simonw, thanks a ton! We really value your feedback.  Just to clarify, following our policy, we've partnered with several model developers to bring their new models to our platform for communi...</li><li><a href="https://learnprompting.org/docs/intro?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Learn Prompting: Your Guide to Communicating with AI</a>: Learn Prompting is the largest and most comprehensive course in prompt engineering available on the internet, with over 60 content modules, translated into 9 languages, and a thriving community.</li><li><a href="https://rentry.co/GPT2?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GPT-2?</a>: Background https://chat.lmsys.org provides blind-tested user benchmarks for LLMs (and some MLLMs). One of the models recently available is GPT2-chatbot, which demonstrates capability greatly beyond an...</li><li><a href="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">State-of-the-art in Decentralized Training</a>: This post explores various novel decentralized training approaches and how they can enable effective AI model training across globally distributed GPUs.</li><li><a href="https://roadmap.sh/prompt-engineering?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Prompt Engineering Roadmap - roadmap.sh</a>: Step by step guide to learn Prompt Engineering. We also have resources and short descriptions attached to the roadmap items so you can get everything you want to learn in one place.</li><li><a href="https://x.com/karan4d/status/1785000251096437161?s=46&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from mephistoooOOHHHHHHSHI- (@karan4d)</a>: Ok it‚Äôs definitely using GPT-4 tokenizer so I‚Äôm betting it is 4.5 as well.   Always fingerprint w anomalous tokens</li><li><a href="https://x.com/lmsysorg/status/1785078213712208291?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from lmsys.org (@lmsysorg)</a>: hi @simonw, thanks a ton! We really value your feedback.  Just to clarify, following our policy, we've partnered with several model developers to bring their new models to our platform for communi...</li><li><a href="https://x.com/albfresco/status/1784964830887104999?s=46&amp;t=90xQ8sGy63D2OtiaoGJuww&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from albs ‚Äî 3/staccs (@albfresco)</a>: my guess is this mysterious 'gpt2-chatbot' is literally OpenAI's gpt-2 from 2019 finetuned with modern assistant datasets.  in which case that means their original pre-training is still am...</li><li><a href="https://x.com/karan4d/status/1785000251096437161?s=46&amp;t=90xQ8sGy63D2OtiaoGJuww&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from mephistoooOOHHHHHHSHI- (@karan4d)</a>: Ok it‚Äôs definitely using GPT-4 tokenizer so I‚Äôm betting it is 4.5 as well.   Always fingerprint w anomalous tokens</li><li><a href="https://x.com/markatgradient/status/1785032103429865748?s=46&amp;t=90xQ8sGy63D2OtiaoGJuww&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Mark Huang (@markatgradient)</a>: 1M context length  Llama-3 8B Model.  Enough said.    Up on HF @ClementDelangue   cc: @winglian @mattshumer_  ‚ÜòÔ∏è Quoting Gradient (@Gradient_AI_)   We've been in the kitchen cooking üî• Excited to ...</li><li><a href="https://x.com/MKBHD/status/1785102259740667960?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Marques Brownlee (@MKBHD)</a>: NEW VIDEO - Rabbit R1: Barely Reviewable  https://youtu.be/ddTV12hErTc  This is the pinnacle of a trend that's been annoying for years: Delivering barely finished products to win a "race" ...</li><li><a href="https://github.com/xlang-ai/OSWorld?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - xlang-ai/OSWorld: OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</a>: OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments - xlang-ai/OSWorld</li><li><a href="https://github.com/kingjulio8238/memary?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - kingjulio8238/memary: Longterm Memory for Autonomous Agents.</a>: Longterm Memory for Autonomous Agents. . Contribute to kingjulio8238/memary development by creating an account on GitHub.</li><li><a href="https://www.youtube.com/watch?v=1hDK7gZbJqQ&amp;t=25s&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Ep. 8 ‚Äî ColBERT + ColBERTv2: late interaction at a reasonable inference cost</a>: Andrew Yates (Assistant Professor at the University of Amsterdam) and Sergi Castella (Analyst at Zeta Alpha) discus the two influential papers introducing Co...</li><li><a href="https://youtu.be/aircAruvnKk?feature=shared%29%2C&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">But what is a neural network? | Chapter 1, Deep learning</a>: What are the neurons, why are there layers, and what is the math underlying it?Help fund future projects: https://www.patreon.com/3blue1brownWritten/interact...</li><li><a href="https://x.com/jessechenglyu/status/1785342519045394465?s=46&amp;t=90xQ8sGy63D2OtiaoGJuww&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Jesse Lyu (@jessechenglyu)</a>: get your r1 update to the latest version now - we addressed most of the issues we found so far and more fix/improvements incoming! idle battery life up to 5x better now.  ‚ÜòÔ∏è Quoting rabbit inc. (@rabb...
</li>
</ul>
</div>
<hr/>
<p><strong>OpenInterpreter ‚ñ∑ #<a href="https://discord.com/channels/1146610656779440188/1147665339266650133/1234527192797417594?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (21 messagesüî•): </p>
<ul>
<li><strong>Question on Launching OS Mode with Local Vision Model</strong>: A member asked <strong>how to start OS mode with a local vision model</strong> to try <strong>Moondream</strong>, but reported getting gibberish with the command <code>interpreter --os --local</code>.</li>
<li><strong>Discussion on Model Functionality</strong>: Another user mentioned using <code>llava</code> <strong>months ago</strong> and confirmed that it is possible to get <strong>a description of an image through OpenInterpreter without executing custom code</strong>.</li>
<li><strong>Integration Update for OpenInterpreter</strong>: A member announced they managed to integrate all OpenInterpreter outputs into <strong>MagicLLight</strong>, with a pull request to OpenInterpreter planned for <code>stream_out</code> function hook and <code>external_input</code>. Code release for MagicLLight and AAA+ is expected after some cleanup.</li>
<li><strong>OpenInterpreter on Budget Hardware</strong>: The feasibility of running <strong>OpenInterpreter smoothly on a BeepyBerry-Raspberry Pi Zero</strong> was questioned, with a link to a related <a href="https://youtube.com/shorts/E7WQZdJKsbM?si=1XMj0aTtN83cZ5aY&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">YouTube video</a>.</li>
<li><strong>Seeking Debugging Assistance for Bad Startups</strong>: A user sought help for <strong>debugging a bad startup</strong>, indicating the errors were vague. They were directed to share the errors so that the community could assist in troubleshooting.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/SdwpMQaW?event=1232436050165764096&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>OpenInterpreter ‚ñ∑ #<a href="https://discord.com/channels/1146610656779440188/1194880263122075688/1234538284089344000?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">O1</a></strong> (20 messagesüî•): </p>
<ul>
<li><strong>Push Button Code Success</strong>: A member successfully resolved an issue related to an external push button not reacting by updating the <code>ButtonChecker</code> code and wiring the button to <strong>pin 25</strong>, offering a <a href="https://discord.com/channels/openinterpreter/01?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">snippet of the revised code</a>. Their efforts were confirmed to be working by another community member.</li>
<li><strong>Speaker Connection Stability</strong>: In another hardware related fix, it was recommended to use hot glue to secure the speaker wires and reduce stress on the connections when interfacing with pins for a project.</li>
<li><strong>Raising Speaker Volume Inquiry</strong>: A query was raised on how to increase the volume on speakers, with suggestions to try <strong>M5Unified</strong> or potentially use an <a href="https://www.amazon.com/dp/B01DKAI51M?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">external amplifier</a>.</li>
<li><strong>Youtuber Reviews Debated</strong>: There was a discussion about the relevance of YouTuber reviews of AI products like <strong>AI pins</strong> and <strong>R1</strong>, questioning if tech reviewers like <strong>MKBHD</strong> and <strong>Dave2d</strong> fully grasp the AI space, which is different from reviewing consumer electronics like phones or laptops.</li>
<li><strong>01 Light Hardware with OS Mode</strong>: A member sought assistance on getting OS mode to work with the current version of the 01 light hardware mentioning successful connectivity to Mac but without access to the screen.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.amazon.com/dp/B01DKAI51M?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">no title found</a>: no description found</li><li><a href="https://www.youtube.com/watch?v=ddTV12hErTc&amp;ab_channel=MarquesBrownlee&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Rabbit R1: Barely Reviewable</a>: AI in a Box. But a different box.Get a dbrand skin and screen protector at https://dbrand.com/rabbitMKBHD Merch: http://shop.MKBHD.comTech I'm using right no...
</li>
</ul>
</div>
<hr/>
<p><strong>tinygrad (George Hotz) ‚ñ∑ #<a href="https://discord.com/channels/1068976834382925865/1068976834928193609/1234533862407671860?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (10 messagesüî•): </p>
<ul>
<li><strong>Tinygrad Inquiry</strong>: A user asked what <strong>tinygrad</strong> is, and another member provided a link to the <a href="https://github.com/tinygrad/tinygrad/tree/master?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">tinygrad GitHub repository</a> defining it as a project that those who like PyTorch and micrograd will love.</li>
<li><strong>Discord Discovery Mystery</strong>: One member voiced curiosity about how another stumbled upon the Discord server, to which the latter replied uncertainly, indicating a lack of knowledge about their discovery method.</li>
<li><strong>Seeking Bounty Guidance</strong>: A user sought help for two bounties involving <em>"Mean of symbolic shape"</em> and <em>"Symbolic arrange"</em> and was looking for references to understand and solve them.</li>
<li><strong>Backward Pass Optimization Issue</strong>: A member was investigating issue <a href="https://github.com/tinygrad/tinygrad/issues/3572?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">#3572</a> related to backward passes with 2 reduce operations and inquired about how to generate graph diagrams to illustrate the problem.</li>
<li><strong>Graph Diagram Generation for Tinygrad</strong>: In response to a query about generating graph diagrams to address a backward pass issue, a member mentioned the use of <code>GRAPH=1</code>, suggesting the use of an environment variable to facilitate this task.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/tinygrad/tinygrad/pull/4362?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">tensor variable by geohot ¬∑ Pull Request #4362 ¬∑ tinygrad/tinygrad</a>: no description found</li><li><a href="https://github.com/tinygrad/tinygrad/tree/master?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - tinygrad/tinygrad: You like pytorch? You like micrograd? You love tinygrad! ‚ù§Ô∏è</a>: You like pytorch? You like micrograd? You love tinygrad! ‚ù§Ô∏è  - GitHub - tinygrad/tinygrad: You like pytorch? You like micrograd? You love tinygrad! ‚ù§Ô∏è
</li>
</ul>
</div>
<hr/>
<p><strong>tinygrad (George Hotz) ‚ñ∑ #<a href="https://discord.com/channels/1068976834382925865/1070745817025106080/1234463379603722291?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">learn-tinygrad</a></strong> (29 messagesüî•): </p>
<ul>
<li><strong>Exploring TinyGrad's Learning Resources</strong>: Members discussed resources for learning AI development with TinyGrad; links to <a href="https://github.com/unknownusername504/MicroGrad?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">MicroGrad GitHub repository</a> and <a href="https://minitorch.github.io/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">MiniTorch</a> were shared, with MiniTorch highlighted as a teaching tool for understanding deep learning systems.</li>
<li><strong>TinyGrad Quick Start Guidance Shared</strong>: A user recommended the "<a href="https://tinygrad.github.io/tinygrad/quickstart/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">tinygrad Quick Start Guide</a>" for anyone looking to learn AI, especially with TinyGrad, as it provides a basic overview of the high-level API that TinyGrad offers for model development.</li>
<li><strong>Symbolic Mean Bounty Challenge in TinyGrad</strong>: Discussions revolved around implementing a symbolic mean operation in TinyGrad, with considerations about LazyBuffer's need to handle data of type Variable and whether it should allocate memory.</li>
<li><strong>Pull Request for Symbolic Execution in TinyGrad</strong>: A link to a <a href="https://github.com/tinygrad/tinygrad/pull/1552?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">previous pull request</a> was shared to illustrate the mechanism for symbolic code generation and execution in TinyGrad, hinting at how variable caching might be useful for operations like <code>sum</code> and <code>mean</code>.</li>
<li><strong>Developing Symbolic Mean with Variables</strong>: The conversation continued with the development of symbolic mean, focusing on the need to represent tensor lengths symbolically and the potential for <code>Const</code> to support variables in the input buffer. Links to a comparision of master and feature branch on GitHub, <a href="https://github.com/tinygrad/tinygrad/compare/master...davidjanoskyrepo:tinygrad:symbolic-mean-var-pull?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">tinygrad symbolic-mean-var-pull</a>, and further <a href="https://github.com/tinygrad/tinygrad/compare/86d90511cee2%5E...97a2d44d9840?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GitHub changes by gh</a> were shared as part of solving this challenge.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://tinygrad.github.io/tinygrad/quickstart/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Quickstart - tinygrad docs</a>: no description found</li><li><a href="https://github.com/tinygrad/tinygrad/compare/master...davidjanoskyrepo:tinygrad:symbolic-mean-var-pull?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Comparing tinygrad:master...davidjanoskyrepo:symbolic-mean-var-pull ¬∑ tinygrad/tinygrad</a>: You like pytorch? You like micrograd? You love tinygrad! ‚ù§Ô∏è  - Comparing tinygrad:master...davidjanoskyrepo:symbolic-mean-var-pull ¬∑ tinygrad/tinygrad</li><li><a href="https://github.com/tinygrad/tinygrad/compare/86d90511cee2%5E...97a2d44d9840?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Comparing 86d90511cee2^...97a2d44d9840 ¬∑ tinygrad/tinygrad</a>: You like pytorch? You like micrograd? You love tinygrad! ‚ù§Ô∏è  - Comparing 86d90511cee2^...97a2d44d9840 ¬∑ tinygrad/tinygrad</li><li><a href="https://github.com/unknownusername504/MicroGrad?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - unknownusername504/MicroGrad</a>: Contribute to unknownusername504/MicroGrad development by creating an account on GitHub.</li><li><a href="https://minitorch.github.io/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">MiniTorch</a>: no description found</li><li><a href="https://github.com/tinygrad/tinygrad/commit/77589bc7a5430ee470621e43fb1817259d3ce0f5?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">rename Scalar to ConstType and cast_scalar to as_const (#3946) ¬∑ tinygrad/tinygrad@77589bc</a>: prereq cleanup to make const arg same python type as dtype</li><li><a href="https://github.com/tinygrad/tinygrad/pull/1552?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">symbolic codegen and exec by chenyuxyz ¬∑ Pull Request #1552 ¬∑ tinygrad/tinygrad</a>: part of #1353 , codegen and exec to implement realize for symbolic inputs. The combined var_vals are passed into kernel function directly. I have implemented the backend for CLANG, GPU, METAL. glob...
</li>
</ul>
</div>
<hr/>
<p><strong>Cohere ‚ñ∑ #<a href="https://discord.com/channels/954421988141711382/954421988783444043/1234452605997023242?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (34 messagesüî•): </p>
<ul>
<li><strong>Single URL Constraint in Command-R</strong>: In a discussion about the <strong>web-search tool</strong> in <strong>API Command R+</strong>, members clarified that currently only one website can be used with the <code>site</code> option of the tool, suggesting that a workaround might be to run <strong>an API call for each individual website</strong>.</li>
<li><strong>Lack of Multi-step Connectors</strong>: <strong>Cohere</strong> confirmed that <strong>connectors</strong> cannot be used with multi-step tool use within <strong>Command-R</strong> at the moment.</li>
<li><strong>Hopes for Future Command-R Features</strong>: A member suggested desirable enhancements for <strong>Command-R</strong> with a focus on <strong>Connectors</strong>, such as using multiple websites in <code>web_search</code>, sending extra parameters to custom connectors for more granular control, and enabling a <code>use_rerank</code> option to automatically rerank. A helpful link to the documentation was shared: <a href="https://docs.cohere.com/reference/chat?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Cohere Chat Documentation</a>.</li>
<li><strong>Questions on Model Availability</strong>: A query was posed about the availability of the "Generate" option for fine-tuning models, since it was noticed to be missing from the dashboard, leading to speculation about whether it would be returning.</li>
<li><strong>Strategies for Efficient Embedding</strong>: A member inquired about strategies for <strong>keeping data updated</strong> to embed efficiently, touching on the need for cost-effective methods to only reindex chunks of data that have been updated.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://docs.cohere.com/reference/chat?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Chat API Reference - Cohere Docs</a>: no description found</p>
<hr/>
<p><strong>Cohere ‚ñ∑ #<a href="https://discord.com/channels/954421988141711382/1218409745380147320/1234628219492241449?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">collab-opps</a></strong> (2 messages): </p>
<ul>
<li><strong>Swedish Salutations</strong>: A member from Stockholm, Sweden mentioned <strong>using Cohere</strong> in their company.</li>
<li><strong>Nordic Collaboration</strong>: Another member highlighted their connection to both <strong>Norway and Sweden</strong> through their company, Omegapoint.</li>
</ul>
<hr/>
<p><strong>LangChain AI ‚ñ∑ #<a href="https://discord.com/channels/1038097195422978059/1038097196224086148/1234429137763176458?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (12 messagesüî•): </p>
<ul>
<li><strong>Gemini Model Exploration</strong>: A member is seeking someone with experience in <strong>Gemini 1.0 or 1.5 models</strong> to discuss specifics privately via direct message.</li>
<li><strong>Seeking LLM Observability Tools</strong>: There's a request for recommendations on Large Language Model (LLM) observability tools. The member is considering <strong>Arize Phoenix</strong> or <strong>Langfuze</strong>, with a preference for a self-hosted, open-source option compatible with <strong>LlamaIndex</strong>.</li>
<li><strong>OpenAI and SQL Security</strong>: A member inquires about connecting OpenAI directly to an <strong>SQL server without using LangChain</strong>, prioritizing security in the process.</li>
<li><strong>Leveraging Langgraph with autoawq</strong>: There is a discussion on integrating <strong>autoawq</strong> with <strong>LangGraph</strong> for use with <strong>exllamav2 kernerls</strong> to achieve high inference speeds in powering AI agents.</li>
<li><strong>PDF Content Extraction Challenge</strong>: A new member to langchain and AI programming is seeking advice on how to improve results when splitting a single table that spans multiple pages in a PDF, mentioning they've had unsatisfactory results using <strong>unstructure</strong> for AI-driven PDF content extraction.</li>
</ul>
<hr/>
<p><strong>LangChain AI ‚ñ∑ #<a href="https://discord.com/channels/1038097195422978059/1170024642245832774/1234549931969216563?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">langserve</a></strong> (2 messages): </p>
<ul>
<li><strong>AzureSearchVectorStoreRetriever Async Issue</strong>: A user mentioned encountering an error due to <strong>AzureSearchVectorStoreRetriever</strong> not supporting async operations and inquired about possible solutions. Options discussed included either requesting langserver to implement such a feature or creating an async wrapper around the synchronous retrieve function.</li>
</ul>
<ul>
<li><strong>Using Google Drive Libraries</strong>: Another user suggested utilizing the Google Drive libraries for a function, also mentioning the requirement to set the drive key as an environment variable. It was noted that these libraries had been removed and then re-added in the past.</li>
</ul>
<hr/>
<p><strong>LangChain AI ‚ñ∑ #<a href="https://discord.com/channels/1038097195422978059/1038097372695236729/1234542917406822560?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">share-your-work</a></strong> (8 messagesüî•): </p>
<ul>
<li><strong>A Trip Down Memory Lane with GPT-1</strong>: A blogger has revisited the <strong>original GPT-1 model</strong>, providing insights into how it laid the groundwork for current LLMs and noting its similarities with models like <strong>Mistral-7B</strong>. The blog includes discussions on positional embeddings and Conv1D within the transformer block, available at <a href="https://amgadhasan.substack.com/p/revisiting-gpt-1-the-spark-that-ignited-llms?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Revisiting GPT-1: The Spark That Ignited LLMs</a>.</li>
</ul>
<ul>
<li><strong>Showcasing LangChain on Airbnb</strong>: A demonstration video titled <strong>"D-ID Airbnb Use Case: A RAG Agent Demo using Ollama and Langchain with code on Github"</strong> illustrates an innovative <strong>Live Avatar Q&amp;A</strong> for property sites, powered by LangChain with a collection of 150 QA pairs. Check out the demo on <a href="https://youtu.be/N_GcPLJCQQY?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">YouTube</a>.</li>
</ul>
<ul>
<li><strong>Serve Up Answers with a Pizza Bot</strong>: Another use case for LangChain is presented in a video showcasing a <strong>Pizza Bot</strong> with a live avatar interface. See this mobile-friendly application in action on <a href="https://youtu.be/6Qa2qdlN2pU?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">YouTube</a>.</li>
</ul>
<ul>
<li><strong>No-Code Automation for Code Maintenance</strong>: An announcement for a no-code platform called <strong>Autonoma</strong> demonstrates its purpose to automate code improvement tasks, such as input validation, error handling, and testing, which is now available for a free demo and integrating with GitHub. Test these agents through <a href="https://gitgud.autonoma.app?utm_source=discord&amp;utm_medium=chat&amp;utm_campaign=discord-langchain&amp;utm_id=discord-langchain" target="_blank">Autonoma Free Demo</a>.</li>
</ul>
<ul>
<li><strong>Introducing VectorDB Plugin for LM Studio</strong>: A GitHub repository has been shared for a plugin named <strong>VectorDB</strong>, which creates a ChromaDB vector database to function alongside LM Studio in server mode. The repository can be found at <a href="https://github.com/BBC-Esq/VectorDB-Plugin-for-LM-Studio?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">VectorDB Plugin for LM Studio on GitHub</a>.</li>
</ul>
<ul>
<li><strong>QuickVid: AI-Powered YouTube Summarization Tool</strong>: QuickVid, a new tool that provides fast summaries and fact verification for YouTube videos, has been launched. Try out QuickVid to enhance your YouTube experience with concise, informed summaries at <a href="https://quickvid.vercel.app/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">QuickVid</a>.</li>
</ul>
<ul>
<li><strong>Tutorial on Creating Webloader RAG Applications</strong>: A Medium article details building robust webloader RAG applications using <strong>Groq, Langchain, and Datastax</strong> to power up your applications. The guide is accessible at <a href="https://medium.com/ai-advances/building-powerful-webloader-rag-applications-with-groq-langchain-and-datastax-f4816d88bee8?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Building Powerful Webloader RAG Applications with Groq, Langchain, and Datastax</a>.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://amgadhasan.substack.com/p/revisiting-gpt-1-the-spark-that-ignited-llms?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Revisiting GPT-1: The spark that ignited the fire of LLMs</a>: A Comprehensive Look at GPT-1's Contribution to the Development of Modern LLMs</li><li><a href="https://quickvid.vercel.app/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">QuickVid</a>: no description found</li><li><a href="https://gitgud.autonoma.app?utm_source=discord&amp;utm_medium=chat&amp;utm_campaign=discord-langchain&amp;utm_id=discord-langchain%3E%29">GitGud</a>: no description found</li><li><a href="https://youtu.be/N_GcPLJCQQY?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">D-ID Airbnb Use Case:  A RAG Agent Demo using Ollama and Langchain with code on Github</a>: A demo to help illustrate practical use cases for live avatar assistants for business... I will do a video for the detailed code review so you can try it... ...</li><li><a href="https://github.com/BBC-Esq/VectorDB-Plugin-for-LM-Studio?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - BBC-Esq/VectorDB-Plugin-for-LM-Studio: Plugin that creates a ChromaDB vector database to work with LM Studio running in server mode!</a>: Plugin that creates a ChromaDB vector database to work with LM Studio running in server mode! - BBC-Esq/VectorDB-Plugin-for-LM-Studio
</li>
</ul>
</div>
<hr/>
<p><strong>LangChain AI ‚ñ∑ #<a href="https://discord.com/channels/1038097195422978059/1077843317657706538/1234782249166049310?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">tutorials</a></strong> (2 messages): </p>
<ul>
<li><strong>Bonjour from Paris</strong>: A member shares a YouTube video titled <a href="https://youtu.be/ol2QMp64lgo?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">"Agent RAG: LangChain et LlamaIndex port√©s par Mistral Large - Le vent du changement"</a>, demonstrating the creation of an Advanced RAG assistant using <strong>LangChain</strong>, <strong>Mistral Large</strong>, and <strong>Llamaindex</strong>. The video is meant for the French-speaking community, and the code for the app is available in the video's description on <strong>GitHub</strong>.</li>
</ul>
<ul>
<li><strong>DIY Llama3 RAG Assistant</strong>: Another member presents a tutorial on how to train <strong>llama3</strong> with private knowledge to build an agentic RAG, in a YouTube video titled <a href="https://youtu.be/u5Vcrwpzoz8?si=U30s6BAN9Jsaec-P&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">"I want Llama3 to perform 10x with my private knowledge" - Local Agentic RAG w/ llama3"</a>. The video aims to guide viewers through the process of enhancing <strong>llama3</strong>'s performance using their own data.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://youtu.be/u5Vcrwpzoz8?si=U30s6BAN9Jsaec-P&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">"I want Llama3 to perform 10x with my private knowledge" - Local Agentic RAG w/ llama3</a>: Advanced RAG 101 - build agentic RAG with llama3Get free HubSpot report of how AI is redefining startup GTM strategy: https://clickhubspot.com/4hxüîó Links- F...</li><li><a href="https://youtu.be/ol2QMp64lgo?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Agent RAG: LangChain et LlamaIndex port√©s par Mistral Large - Le vent du changement</a>: Dans cette nouvelle vid√©o, je vous pr√©sente le d√©veloppement d'un RAG Assitant d√©velopp√© √† partir d'agent utilisant Mistral, Langchain et LlamaIndex.Le code ...
</li>
</ul>
</div>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1087876677603958804/1234580644273717471?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ai-and-ml-discussion</a></strong> (2 messages): </p>
<ul>
<li><strong>Inappropriate Content Alert</strong>: A post promising <strong>free leaks</strong> of content from Onlyfans featuring 18+ Teen Girls contained a Discord link. The message also included emojis and an <code>@everyone</code> tag to draw broad attention.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1087876753462136873/1234580388391944193?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">programming-help</a></strong> (3 messages): </p>
<ul>
<li><strong>Inappropriate Content Alert</strong>: A message was posted containing links to explicit content, potentially violating Discord's community guidelines. The message promoted free access to content involving underage individuals, which is illegal and problematic.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1095393077415383261/1234580548698247390?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">looking-for-collabs</a></strong> (2 messages): </p>
<p>The provided message does not pertain to AI collaboration, research, or relevant topics for the "looking-for-collabs" channel, and it appears to be spam. Therefore, there is no appropriate summary content based on this message.</p>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1095458248712265841/1234580564871221329?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general-chat</a></strong> (2 messages): </p>
<ul>
<li><strong>Inappropriate Content Alert</strong>: A message promoting <strong>adult content</strong> and so-called 'OnlyFans leaks' was posted, with a Discord invite link provided. This content is clearly inappropriate for the channel and may violate community guidelines.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1113327574563692654/1234767716267855884?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">landmark-dev</a></strong> (1 messages): </p>
<ul>
<li><strong>Spam Alert</strong>: A spam message promoting <strong>adult content</strong> and such materials was posted, including a Discord invitation link. This was likely unrelated to the channel's focus and may require moderation action.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Join the e-girl paradise üçëüçí // +18 Discord Server!</a>: Check out the e-girl paradise üçëüçí // +18 community on Discord - hang out with 11801 other members and enjoy free voice and text chat.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1118282868595109918/1234767861927645225?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">landmark-evaluation</a></strong> (1 messages): </p>
<ul>
<li><strong>Inappropriate Content Alert</strong>: A user posted a message containing explicit content and an invitation link, promoting access to what appears to be private or sensitive media involving underage individuals. The message includes emojis and a Discord invite URL.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1124000038205530182/1234580949870710797?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">open-orca-community-chat</a></strong> (2 messages): </p>
<ul>
<li><strong>Inappropriate Content Alert</strong>: A message promoting <strong>18+ content</strong> and <strong>OnlyFans leaks</strong> was posted, including an invitation link and emojis suggesting adult material. The content of the message is against Discord's community guidelines.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1135102537817653308/1234768131247964212?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">leaderboard</a></strong> (1 messages): </p>
<ul>
<li><strong>Inappropriate Content Alert</strong>: A Discord user posted a message promoting <strong>adult content</strong>, including a mention of <em>'18+ Teen Girls and onlyfans leaks for free'</em>, along with an invitation link to another server. The user utilized emojis and tagged <strong>@everyone</strong> to draw attention.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1142242166677192774/1234581080389062810?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">looking-for-workers</a></strong> (2 messages): </p>
<ul>
<li><strong>Inappropriate Content Warning</strong>: A message was posted promoting <strong>18+ Teen Girls and OnlyFans leaks</strong> with a Discord invite link. This type of content is likely against the platform's rules and may warrant moderation action.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1142242683339944027/1234581103633891358?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">looking-for-work</a></strong> (2 messages): </p>
<ul>
<li><strong>Inappropriate Content Alert</strong>: The message suggests sharing of leaked content from OnlyFans involving teen girls, accompanied by a Discord invite link. This post raises serious concerns regarding legality and ethics.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1143791237669855302/1234581301672149132?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">join-in</a></strong> (2 messages): </p>
<ul>
<li><strong>Inappropriate Content Alert</strong>: A message was posted that promoted <strong>adult content</strong> including "18+ Teen Girls and onlyfans leaks". The post included an emoji of a peach and the underage sign, along with a Discord invitation link.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1147528620936548363/1234581174794453042?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">fasteval-dev</a></strong> (2 messages): </p>
<ul>
<li><strong>Inappropriate Content Alert</strong>: A message was posted promoting <strong>18+ Teen Girls and OnlyFans leaks</strong> with a Discord invite link. The content appears to be explicit and not suitable for this professional setting.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>Alignment Lab AI ‚ñ∑ #<a href="https://discord.com/channels/1087862276448595968/1147528698669584424/1234581352272363562?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">qa</a></strong> (2 messages): </p>
<ul>
<li><strong>Inappropriate Content Alert</strong>: A user posted a message promoting <strong>adult content</strong> including '18+ Teen Girls' and 'onlyfans leaks' with a Discord invite link (<strong>not clicked or verified</strong>). The message uses emojis and tags <code>@everyone</code> to attract attention.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Discord - A New Way to Chat with Friends &amp; Communities</a>: Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.</p>
<hr/>
<p><strong>AI Stack Devs (Yoko Li) ‚ñ∑ #<a href="https://discord.com/channels/1122748573000409160/1122788693950857238/1234713529202769981?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ai-companion</a></strong> (1 messages): </p>
<ul>
<li><strong>Concerns Over Criminalizing Coping Mechanisms</strong>: A member expressed strong concern regarding criminalizing an unspecified activity that might be the last coping mechanism for men who have suffered from severe personal and legal setbacks. There is a fear that such measures could push these individuals towards extreme actions due to feeling marginalized by society.</li>
</ul>
<hr/>
<p><strong>AI Stack Devs (Yoko Li) ‚ñ∑ #<a href="https://discord.com/channels/1122748573000409160/1131651713204498583/1234598116523642941?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">events</a></strong> (2 messages): </p>
<ul>
<li><strong>Game Jam Bonanza with Rosebud AI</strong>: Rosebud AI announces a <strong>Game Jam</strong> in collaboration with Week of AI, inviting participants to create 2D browser-based games with <strong>Phaser JS</strong> around the theme of <em>Education and AI</em>. A <strong>$500 prize pool</strong> is up for grabs, and you can find out how to join <a href="https://twitter.com/Rosebud_AI/status/1785034624256618617?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>.</li>
</ul>
<ul>
<li><strong>AIxGames Meetup in SF</strong>: An AIxGames meetup event is scheduled for this Thursday in San Francisco to connect people working with AI in gaming. There are spots for 160 people, and you can RSVP and check the location <a href="https://partiful.com/e/TwvC5qxskuPGqiliMj5f?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>, with a call for demo presentations accessible via <a href="https://forms.gle/6hiqnws3tg6EY7348?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">this form</a>.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://partiful.com/e/TwvC5qxskuPGqiliMj5f?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">RSVP to AIxGames Meetup | Partiful</a>: AI is already changing the gaming landscape, and is probably going to change it a lot more.   We want to gather as many people working at the intersection of AI and Gaming as we can. Whether it is on ...</p>
<hr/>
<p><strong>AI Stack Devs (Yoko Li) ‚ñ∑ #<a href="https://discord.com/channels/1122748573000409160/1132926337598902293/1234530269331980359?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ai-town-discuss</a></strong> (8 messagesüî•): </p>
<ul>
<li><strong>Revolutionizing NPC Interactions with LLMs</strong>: A user announced their release of LLM-powered NPC models and an inference stack to enhance action spaces and simplify API calls, found at <a href="https://github.com/GigaxGames/gigax?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">GigaxGames on GitHub</a>. The solution includes a <em>single LLM call</em> feature for complex NPC actions, open-weights on <a href="https://huggingface.co/Gigax?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Huggingface's Hub</a>, and an API access offer (with a link that appears to be broken).</li>
</ul>
<ul>
<li><strong>Overcoming LLM Challenges for Game Development</strong>: In pursuit of runtime speeds for gameplay features, they faced multiple issues like NPCs breaking the 4th wall during <code>speak</code> commands and missing details in large prompts. The user suggests <em>output compression</em>, minimizing model calls, and leveraging smaller models can significantly impact the NPC's performance.</li>
</ul>
<ul>
<li><strong>Anticipating a Deep Dive into LLM-Enhanced NPCs</strong>: The user has signaled an intent to <em>write a blog post</em> about the experienced struggles and insights relating to the fine-tuning of LLMs for NPC behavior improvement. </li>
</ul>
<ul>
<li><strong>Peek into a Peer's Journey with NPC Development</strong>: Another user expressed that their project had also encountered challenges with the existing models, noting that <em>Claude 3</em> performed better possibly owing to its "empathetic" training background. They are currently exploring a strategy involving functional calling with smaller prompts and are interested in the outputs of such an approach.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/GigaxGames/gigax?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - GigaxGames/gigax: LLM-powered NPCs running on your hardware</a>: LLM-powered NPCs running on your hardware. Contribute to GigaxGames/gigax development by creating an account on GitHub.</li><li><a href="https://tally.so/r/w7d2Rz)?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Form - Tally</a>: Made with Tally, the simplest way to create forms.
</li>
</ul>
</div>
<hr/>
<p><strong>AI Stack Devs (Yoko Li) ‚ñ∑ #<a href="https://discord.com/channels/1122748573000409160/1137456826733047908/1234844604638167094?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ai-town-dev</a></strong> (13 messagesüî•): </p>
<ul>
<li><strong>Local Setup Achieved with Ease</strong>: A member confirmed they successfully ran the setup locally and found the process very straightforward.</li>
<li><strong>Kudos for Member Contribution</strong>: A member expressed appreciation for the excellent work of another community member.</li>
<li><strong>Stuck on Windows</strong>: One member experienced an issue with cloning the repo on Windows, getting stuck at <em>'Checking for index or schema changes...'</em>. It was clarified that <strong>Convex local does not support Windows</strong>.</li>
<li><strong>Alternative Commands for Logs and Development</strong>: It was suggested to utilize <code>just convex dev</code> for a separate development sync and <code>just convex logs</code> to keep tabs on logs, providing commands that include options for <strong>tailoring logs</strong> and <strong>verbose output</strong>.</li>
<li><strong>Window Compatibility Workaround</strong>: Members discussed workarounds for the lack of Windows support with <strong>Convex local</strong>, such as using <strong>WSL (Windows Subsystem for Linux)</strong> or <strong>Docker</strong>, and mentioned that Windows compilation is in progress.</li>
</ul>
<hr/>
<p><strong>Skunkworks AI ‚ñ∑ #<a href="https://discord.com/channels/1131084849432768614/1131084849906716735/1234486969397149837?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (15 messagesüî•): </p>
<ul>
<li><strong>Exploring HaystackDB Embeddings</strong>: A user referenced <a href="https://github.com/carsonpo/haystackdb?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">HaystackDB on GitHub</a>, questioning whether it uses <strong>2-bit embeddings</strong>.</li>
<li><strong>Understanding Binary Quantized Indexing</strong>: Clarification was provided that <strong>Binary Quantized (BQ)</strong> indexing is designed to create a smaller index for similarity search, contributing to a more efficient storage and search mechanism.</li>
<li><strong>Challenges in Fine-Tuning LLaMA-3</strong>: Members express difficulties with fine-tuning <strong>LLaMA-3</strong>, noting issues such as the model not generating the <strong>EOS token</strong>, and the embedding layer presenting challenges when loaded in different bit formats.</li>
<li><strong>Perplexity Fine-Tuning Troubles</strong>: Conversations indicate that <strong>fine-tuning for perplexity on LLaMA-3</strong> may not yield results better than the original models, with suggestions that the tokenizer could be contributing to the issues.</li>
<li><strong>Potential Breakthrough with LLaMA-3 Fine-Tuning</strong>: A group member shared success in fine-tuning <strong>LLaMA-3</strong> by utilizing LLaMA-3 specific prompt formatting, linking to a relevant GitHub <a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1553?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">pull request for further information</a>.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/carsonpo/haystackdb?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">GitHub - carsonpo/haystackdb</a>: Contribute to carsonpo/haystackdb development by creating an account on GitHub.</li><li><a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1553?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">feat: Add LLaMA-3 instruct prompt strategies for fine-tuning   by 0-hero ¬∑ Pull Request #1553 ¬∑ OpenAccess-AI-Collective/axolotl</a>: Description This builds on top of and includes the changes in the below PR's  #1542 #1539  Fastchat PR from @TJ-Solergibert needs to be merged before merging this  lm-sys/FastChat#3257   Motivatio...
</li>
</ul>
</div>
<hr/>
<p><strong>Skunkworks AI ‚ñ∑ #<a href="https://discord.com/channels/1131084849432768614/1140423597454807179/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">off-topic</a></strong> (1 messages): </p>
<p>oleegg: https://youtu.be/tYzMYcUty6s?si=t2utqcq36PHbk9da</p>
<hr/>
<p><strong>Mozilla AI ‚ñ∑ #<a href="https://discord.com/channels/1089876418936180786/1089876419926032396/1234890920575631360?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">announcements</a></strong> (1 messages): </p>
<ul>
<li><strong>Mozilla AI is on a Hiring Spree</strong>: Mozilla AI has announced open positions and is on the lookout for new talent. Check out the opportunities and consider applying <a href="https://discord.com/channels/1089876418936180786/1230938514955436242/1234870020916510823?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>.</li>
</ul>
<ul>
<li><strong>Evaluate Models with Lm-buddy</strong>: An open-source tool named Lm-buddy has been introduced for helping evaluate language models more effectively. The tool can be explored and contributed to via the link provided <a href="https://discord.com/channels/1089876418936180786/1230938514955436242/1234589599733518378?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>.</li>
</ul>
<ul>
<li><strong>Prometheus Puts Local LLMs on the Bench</strong>: A project called Prometheus demonstrates the use of Local Large Language Models (LLMs) in the role of a judge. This innovative application can be discussed and delved into further in the dedicated channel linked <a href="https://discord.com/channels/1089876418936180786/1234890301143912599/1234890301143912599?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>.</li>
</ul>
<hr/>
<p><strong>Mozilla AI ‚ñ∑ #<a href="https://discord.com/channels/1089876418936180786/1182689832057716778/1234502618420613130?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">llamafile</a></strong> (13 messagesüî•): </p>
<ul>
<li><strong>AI Tokens Generation Speed Inquiry</strong>: A member inquired about the efficiency of token generation in llama.cpp/llamafile, noting that their implementation of inference for llama2 spends 95% of time on matrix-vector multiplications. They wondered if loop unrolling in llama.cpp could account for its 30% faster performance, as they observed both looping and vectorization in disassembly.</li>
<li><strong>LLaMA Naming Mix-Up</strong>: One user experienced a humorous mix-up with message parameters, setting themselves as "Z" and then forgetting about it, leading to some confusion when messages appeared as if LLaMA was talking to itself.</li>
<li><strong>Pseudonymous Intrusion Causes Confusion</strong>: Another user recounted an unusual event where someone joined a chat under the name "kimkardashian," causing a bizarre situation. However, the anomaly could not be replicated in subsequent runs.</li>
<li><strong>Technology Integration Troubles</strong>: A user struggled to integrate LLaMA with a Plush-for-comfyUI node. Despite the node functioning with other OpenAI endpoints, it failed to operate correctly with llamafile.</li>
<li><strong>LLaMA3 Compatibility and Support Communication</strong>: There's an acknowledged issue with running LLaMA3:8b on M1 Macbook Air specifically with llamafile, whereas it runs without problem on Ollama. A pledge was made to prioritize M1 compatibility testing once other ongoing issues with LLaMA3 are resolved.</li>
</ul>
<hr/>
<p><strong>Interconnects (Nathan Lambert) ‚ñ∑ #<a href="https://discord.com/channels/1179127597926469703/1179127598442348730/1234545676260474942?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">ideas-and-feedback</a></strong> (1 messages): </p>
<p>Since the provided message appears to be the only one or part of a single message without additional context or other messages, a summarization cannot be performed. Please provide a set of messages from the "ideas-and-feedback" channel, so that I can create an appropriate summary.</p>
<hr/>
<p><strong>Interconnects (Nathan Lambert) ‚ñ∑ #<a href="https://discord.com/channels/1179127597926469703/1179128538679488533/1234547539186024519?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">news</a></strong> (4 messages): </p>
<ul>
<li><strong>Exploring OLMo with Hanna Hajishirzi</strong>: A <a href="https://youtu.be/qFZbu2P1vZ8?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">recent talk</a> by Hanna Hajishirzi from <a href="https://homes.cs.washington.edu/~hannaneh/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">AI2</a> on "OLMo: Findings of Training an Open LM" has been shared, held at the Open-Source Generative AI Workshop at Cornell Tech. The slides for the talk can be accessed <a href="https://drive.google.com/file/d...?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">here</a>.</li>
<li><strong>Intensity of the Information Flow</strong>: A member reveals that Hanna Hajishirzi is their manager who moves at an incredibly fast pace, suggesting the depth and complexity of her lectures.</li>
<li><strong>OLMo Presentation Overwhelming but Impressive</strong>: Another member finds the content of Hanna's 25-minute talk ‚Äì covering topics like OLMo, Dolma, Tulu ‚Äì quite vast and a bit overwhelming, yet acknowledges her impressive profile and the value such information may have for students.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://youtu.be/qFZbu2P1vZ8?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Hanna Hajishirzi (AI2) - OLMo: Findings of Training an Open LM</a>: Talk from the Open-Source Generative AI Workshop at Cornell Tech. Speaker: https://homes.cs.washington.edu/~hannaneh/Slides - https://drive.google.com/file/d...</p>
<hr/>
<p><strong>Interconnects (Nathan Lambert) ‚ñ∑ #<a href="https://discord.com/channels/1179127597926469703/1214764639397617695/1234622923449569322?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">reads</a></strong> (2 messages): </p>
<ul>
<li><strong>Insights from John Schulman through Gist</strong>: A GitHub <a href="https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Gist</a> provided valuable insights, summarizing a talk by <strong>John Schulman</strong> related to reinforcement learning for language model-based systems.</li>
</ul>
<ul>
<li><strong>Questioning the Utility of AI Leaderboards</strong>: A <a href="https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">blog post</a> by Sayash Kapoor and Benedikt Stroebl claims there's no current accurate method to determine the best AI for code generation. They highlight that the LLM debugger (<strong>LDB</strong>), while topping the HumanEval leaderboard for code generation, is a costly agent due to its reliance on running costly language models like GPT-4.</li>
</ul>
<div class="linksMentioned">
<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">AI leaderboards are no longer useful. It's time to switch to Pareto curves.</a>: What spending $2,000 can tell us about evaluating AI agents</li><li><a href="https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">rl-for-llms.md</a>: GitHub Gist: instantly share code, notes, and snippets.
</li>
</ul>
</div>
<hr/>
<p><strong>Interconnects (Nathan Lambert) ‚ñ∑ #<a href="https://discord.com/channels/1179127597926469703/1228051082631188530/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">posts</a></strong> (1 messages): </p>
<p>SnailBot News: &lt;@&amp;1216534966205284433&gt;</p>
<hr/>
<p><strong>LLM Perf Enthusiasts AI ‚ñ∑ #<a href="https://discord.com/channels/1168579740391710851/1169107992587812864/1234606317595791490?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">jobs</a></strong> (1 messages): </p>
<ul>
<li><strong>AI Engineer Wanted at Renowned AI-Powered Gamma</strong>: <strong>Gamma</strong>, ranked #16 on a16z's top 100 consumer AI apps, is on the lookout for an <strong>AI engineer</strong> to innovate in presentation and website design through AI. The role includes <strong>prompt engineering</strong>, <strong>metrics/evaluations</strong>, <strong>fine-tuning</strong>, and creating features with cutting-edge models, with the job details available at <a href="https://careers.gamma.app/ai-engineer?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">Gamma Careers</a>.</li>
</ul>
<ul>
<li><strong>Pushing the Limits of Large Language Models</strong>: Candidates without extensive engineering experience are considered if they possess practical expertise in maximizing the potential of <strong>Large Language Models (LLMs)</strong>. The position is based in <strong>San Francisco</strong> and requires in-person collaboration.</li>
</ul>
<ul>
<li><strong>Gamma's Impressive AI-Powered Growth and Culture</strong>: Gamma boasts over <strong>10 million users</strong> grown organically, is <strong>profitable with $10M+ in funding</strong>, operates with a <strong>lean 16-member team</strong>, and promotes an office culture with a hybrid workweek in <strong>San Francisco</strong>.</li>
</ul>
<ul>
<li><strong>Inventive Content Creation at Scale</strong>: With an ambition of <strong>simplifying content creation</strong>, Gamma creates over a million images and processes millions of LLM requests daily. They aim to eliminate the complexities involved in crafting <strong>engaging presentations and websites</strong>.</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://careers.gamma.app/ai-engineer?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">AI Engineer</a>: AI Engineer  San Francisco  Click here to apply</p>
<hr/>
<p><strong>LLM Perf Enthusiasts AI ‚ñ∑ #<a href="https://discord.com/channels/1168579740391710851/1171903046612160632/1234583399029805107?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">openai</a></strong> (3 messages): </p>
<ul>
<li><strong>Speculation on GPT-4.5 Leak</strong>: A tweet by @phill__1 sparked discussions as it suggested the gpt2-chatbot feels like <strong>GPT-4.5</strong>, boasting <em>'insane domain knowledge'</em>. The link to the tweet: <a href="https://x.com/phill__1/status/1784964135920235000?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">phill__1's observation</a>.</li>
<li><strong>Community Buzzing About Potential Leak</strong>: Members in the channel expressed belief that the gpt2-chatbot could be an inadvertent preview of <strong>GPT-4.5</strong>.</li>
<li><strong>Concise Praise for the Mystery Bot</strong>: A terse endorsement was shared by a member, simply stating, "It's good".</li>
</ul>
<p><strong>Link mentioned</strong>: <a href="https://x.com/phill__1/status/1784964135920235000?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408">Tweet from Phil (@phill__1)</a>: Whatever gpt2-chatbot might be, it definitely feels like gpt4.5. It has insane domain knowledge I have never seen before</p>
<hr/>
<p><strong>Datasette - LLM (@SimonW) ‚ñ∑ #<a href="https://discord.com/channels/823971286308356157/1128504153841336370/1234505496761991198?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">llm</a></strong> (3 messages): </p>
<ul>
<li><strong>Custom Grammar for Code-Generation Talk</strong>: A user showed interest in passing a custom grammar, potentially as a model-specific option, to focus on semantic errors in code generation rather than syntax ones.</li>
</ul>
<ul>
<li><strong>User Experience Brainstorm for Datasette</strong>: Ideas were sought for a UX design on Datasette's front page that would allow users to select options from a drop-down, like choosing a country to generate a summary table.</li>
</ul>
<ul>
<li><strong>Direct Data Access via Dropdown Selection</strong>: A member proposed two UX approaches: one by updating the URL upon an event to direct the user to relevant data, and another allowing users to "build" the homepage by updating canned queries based on their selections.</li>
</ul>
<hr/>
<p><strong>DiscoResearch ‚ñ∑ #<a href="https://discord.com/channels/1178995845727785010/1182877486854451271/1234775513499963463?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">general</a></strong> (1 messages): </p>
<ul>
<li><strong>Fast Loading on Local Machine</strong>: Discussion revolved around the observation that a process <em>loads in 3 seconds when running on the machine</em>, yet there seems to be an issue when doing the same through <em>submitting a job</em>. This suggests storage may not be the contributing factor to the problem in a job submission context.</li>
</ul>
<hr/>
<p><strong>DiscoResearch ‚ñ∑ #<a href="https://discord.com/channels/1178995845727785010/1183158791605330051/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">benchmark_dev</a></strong> (1 messages): </p>
<p>le_mess: llama 3 seems to beat gpt4 on scandeval
https://scandeval.com/german-nlg/</p>
<hr/>
</div>