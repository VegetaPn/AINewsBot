<html>
 <body>
  <div class="email-body-content">
   <date>
    May 1, 2024
   </date>
   <h1 class="subject">
    [AINews] LLMs-as-Juries
   </h1>
   <blockquote>
    <p>
     This is AI News! an MVP of a service that goes thru all AI discords/Twitters/reddits and summarizes what people are talking about, so that you can keep up without the fatigue. Signing up
     <a href="https://buttondown.email/ainews/" target="_blank">
      here
     </a>
     opts you in to the real thing when we launch it üîú
    </p>
   </blockquote>
   <hr/>
   <blockquote>
    <p>
     AI News for 4/29/2024-4/30/2024. We checked 7 subreddits and
     <a href="https://twitter.com/i/lists/1585430245762441216?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      <strong>
       373
      </strong>
      Twitters
     </a>
     and
     <strong>
      28
     </strong>
     Discords (
     <strong>
      417
     </strong>
     channels, and
     <strong>
      4855
     </strong>
     messages) for you. Estimated reading time saved (at 200wpm):
     <strong>
      579 minutes
     </strong>
     .
    </p>
   </blockquote>
   <p>
    In the agent literature it is common to find that multiple agents  outperform single agents (if you conveniently ignore inference cost).
    <a href="https://twitter.com/cohere/status/1785284142789242932?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Cohere has now found the same for LLMs-as-Judges
    </a>
    :
   </p>
   <p>
    <img alt="image.png" class="newsletter-image" src="https://assets.buttondown.email/images/ecea573b-f0e8-4e44-968d-82e8f2f4540e.png?w=960&amp;fit=max"/>
   </p>
   <hr/>
   <p>
    <strong>
     Table of Contents
    </strong>
   </p>
   <div class="toc">
    <ul>
     <li>
      <a href="#ai-reddit-recap">
       AI Reddit Recap
      </a>
     </li>
     <li>
      <a href="#ai-twitter-recap">
       AI Twitter Recap
      </a>
     </li>
     <li>
      <a href="#ai-discord-recap">
       AI Discord Recap
      </a>
     </li>
     <li>
      <a href="#part-1-high-level-discord-summaries">
       PART 1: High level Discord summaries
      </a>
      <ul>
       <li>
        <a href="#cuda-mode-discord">
         CUDA MODE Discord
        </a>
       </li>
       <li>
        <a href="#unsloth-ai-daniel-han-discord">
         Unsloth AI (Daniel Han) Discord
        </a>
       </li>
       <li>
        <a href="#lm-studio-discord">
         LM Studio Discord
        </a>
       </li>
       <li>
        <a href="#stabilityai-stable-diffusion-discord">
         Stability.ai (Stable Diffusion) Discord
        </a>
       </li>
       <li>
        <a href="#perplexity-ai-discord">
         Perplexity AI Discord
        </a>
       </li>
       <li>
        <a href="#nous-research-ai-discord">
         Nous Research AI Discord
        </a>
       </li>
       <li>
        <a href="#modular-mojo-discord">
         Modular (Mojo üî•) Discord
        </a>
       </li>
       <li>
        <a href="#huggingface-discord">
         HuggingFace Discord
        </a>
       </li>
       <li>
        <a href="#openrouter-alex-atallah-discord">
         OpenRouter (Alex Atallah) Discord
        </a>
       </li>
       <li>
        <a href="#llamaindex-discord">
         LlamaIndex Discord
        </a>
       </li>
       <li>
        <a href="#eleuther-discord">
         Eleuther Discord
        </a>
       </li>
       <li>
        <a href="#laion-discord">
         LAION Discord
        </a>
       </li>
       <li>
        <a href="#openai-discord">
         OpenAI Discord
        </a>
       </li>
       <li>
        <a href="#openaccess-ai-collective-axolotl-discord">
         OpenAccess AI Collective (axolotl) Discord
        </a>
       </li>
       <li>
        <a href="#latent-space-discord">
         Latent Space Discord
        </a>
       </li>
       <li>
        <a href="#openinterpreter-discord">
         OpenInterpreter Discord
        </a>
       </li>
       <li>
        <a href="#tinygrad-george-hotz-discord">
         tinygrad (George Hotz) Discord
        </a>
       </li>
       <li>
        <a href="#cohere-discord">
         Cohere Discord
        </a>
       </li>
       <li>
        <a href="#langchain-ai-discord">
         LangChain AI Discord
        </a>
       </li>
       <li>
        <a href="#alignment-lab-ai-discord">
         Alignment Lab AI Discord
        </a>
       </li>
       <li>
        <a href="#ai-stack-devs-yoko-li-discord">
         AI Stack Devs (Yoko Li) Discord
        </a>
       </li>
       <li>
        <a href="#skunkworks-ai-discord">
         Skunkworks AI Discord
        </a>
       </li>
       <li>
        <a href="#mozilla-ai-discord">
         Mozilla AI Discord
        </a>
       </li>
       <li>
        <a href="#interconnects-nathan-lambert-discord">
         Interconnects (Nathan Lambert) Discord
        </a>
       </li>
       <li>
        <a href="#llm-perf-enthusiasts-ai-discord">
         LLM Perf Enthusiasts AI Discord
        </a>
       </li>
       <li>
        <a href="#datasette-llm-simonw-discord">
         Datasette - LLM (@SimonW) Discord
        </a>
       </li>
       <li>
        <a href="#discoresearch-discord">
         DiscoResearch Discord
        </a>
       </li>
      </ul>
     </li>
     <li>
      <a href="#part-2-detailed-by-channel-summaries-and-links">
       PART 2: Detailed by-Channel summaries and links
      </a>
     </li>
    </ul>
   </div>
   <hr/>
   <h1 id="ai-reddit-recap">
    AI Reddit Recap
   </h1>
   <blockquote>
    <p>
     Across r/LocalLlama, r/machinelearning, r/openai, r/stablediffusion, r/ArtificialInteligence, /r/LLMDevs, /r/Singularity. Comment crawling works now but has lots to improve!
    </p>
   </blockquote>
   <p>
    Here is the updated summary with the requested formatting and de-ranking of AGI posts:
   </p>
   <p>
    <strong>
     OpenAI News
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      Memory feature now available to all ChatGPT Plus users
     </strong>
     : OpenAI
     <a href="https://twitter.com/OpenAI/status/1784992796669096181?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      announced on Twitter
     </a>
     that the memory feature is now rolled out to all ChatGPT Plus subscribers.
    </li>
    <li>
     <strong>
      OpenAI partners with Financial Times for AI in news
     </strong>
     : OpenAI has
     <a href="https://www.reuters.com/technology/financial-times-openai-sign-content-licensing-partnership-2024-04-29/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      signed a deal to license content
     </a>
     from the Financial Times to train its AI models. An
     <a href="https://i.redd.it/s09mjga1jgxc1.jpeg?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      image was shared
     </a>
     announcing the partnership to develop AI experiences for news.
    </li>
    <li>
     <strong>
      Concerns over OpenAI's profitability with paid training data
     </strong>
     : In /r/OpenAI, a
     <a href="https://www.reddit.com/r/OpenAI/comments/1cfxd42/how_is_openai_going_to_be_profitable_if_they_have/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      post questioned
     </a>
     OpenAI's profitability as they start paying to license training data, speculating local open source models may undercut their business.
    </li>
    <li>
     <strong>
      Possible reduction in GPT-4 usage limits
     </strong>
     : A
     <a href="https://www.reddit.com/r/OpenAI/comments/1cfxzvl/has_openai_reduced_the_number_of_questions/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      user in /r/OpenAI noticed
     </a>
     GPT-4's usage has been reduced from 40 messages per 3 hours to around 20 questions per hour.
    </li>
    <li>
     <strong>
      Issues with ChatGPT after memory update
     </strong>
     : In /r/OpenAI, a user
     <a href="https://www.reddit.com/r/OpenAI/comments/1cg8zsd/chatgpt_laziness_data_cleansing_and_analysis_is/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      found ChatGPT struggled
     </a>
     with data cleansing and analysis tasks after the memory update, producing errors and incomplete outputs.
    </li>
   </ul>
   <p>
    <strong>
     OpenAI API Projects and Discussions
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      Tutorial on building an AI voice assistant with OpenAI
     </strong>
     : A
     <a href="https://www.reddit.com/r/OpenAI/comments/1cgh184/how_i_build_an_ai_voice_assistant_with_openai/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      blog post was shared
     </a>
     in /r/OpenAI on building an AI voice assistant using OpenAI's API along with web speech APIs.
    </li>
    <li>
     <strong>
      AI-powered side projects discussion
     </strong>
     : In /r/OpenAI, a
     <a href="https://www.reddit.com/r/OpenAI/comments/1cg5mm7/whats_your_ai_backed_side_project/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      post asked others to share
     </a>
     their AI-powered side projects. The poster made a requirements analysis tool with GPT-4 and an interactive German tutor with GPT-3.5.
    </li>
    <li>
     <strong>
      Interface agents powered by LLMs
     </strong>
     : A /r/OpenAI
     <a href="https://www.reddit.com/r/OpenAI/comments/1cg3f2z/p_interface_agents_building_llmenabled_agents/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      post discussed "interface agents"
     </a>
     - AI that can interact with and control user interfaces like browsers and apps. It covered key components, tools, challenges and use cases.
    </li>
    <li>
     <strong>
      Difficulty resizing elements in GPT-4 generated images
     </strong>
     : In /r/OpenAI, a
     <a href="https://www.reddit.com/r/OpenAI/comments/1cga0zy/best_way_to_tell_gpt4_to_shrink_something_in_a/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      user asked for advice
     </a>
     on instructing GPT-4 to shrink an element in a generated image, as the model struggles to consistently resize things.
    </li>
   </ul>
   <p>
    <strong>
     Stable Diffusion Models and Extensions
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      Seeking realistic SDXL models comparable to PonyXL
     </strong>
     : In /r/StableDiffusion, a
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cfv7ga/any_realistic_sdxl_model_as_good_as_ponyxl/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      user asked about
     </a>
     realistic SDXL models on par with PonyXL's quality and prompt alignment for photographic styles.
    </li>
    <li>
     <strong>
      Hi-diffusion extension for ComfyUI
     </strong>
     : A /r/StableDiffusion user
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cg2394/hidiffusion_is_very_impressive_now_the_comfyui/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      found Hi-diffusion works well
     </a>
     for generating detailed 2K images in ComfyUI with SD1.5 models, outperforming Khoya deep shrink. An extension is available but needs improvements.
    </li>
    <li>
     <strong>
      Virtuoso Nodes v1.1 adds Photoshop features to ComfyUI
     </strong>
     :
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgexi9/virtuoso_nodes_release_v11_with_new_photoshop/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Version 1.1 of Virtuoso Nodes
     </a>
     for ComfyUI was released, adding 8 new nodes that replicate key Photoshop functions like blend modes, selective color, color balance, etc.
    </li>
    <li>
     <strong>
      Styles to simplify Pony XL prompts in Fooocus
     </strong>
     : A /r/StableDiffusion user
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cglyq4/styles_for_fooocus_to_shorten_your_pony_xl/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      created styles for Fooocus
     </a>
     to handle the quality tags in Pony XL prompts, allowing cleaner and shorter prompts focused on content.
    </li>
    <li>
     <strong>
      Anime-style shading LoRA released
     </strong>
     : An
     <a href="https://huggingface.co/2vXpSwA7/iroiro-lora/blob/main/test3/sdxl-shadow_01.safetensors?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      anime-style shading LoRA
     </a>
     was announced, recommended for use with Anystyle and other ControlNets. A Hugging Face link to the LoRA file was provided.
    </li>
   </ul>
   <p>
    <strong>
     Stable Diffusion Help and Discussion
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      Avoiding explicit content in generated images
     </strong>
     : In /r/StableDiffusion, a user getting
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgjrds/80_of_my_generated_pics_have_dicks_coming_out_of/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      phallic elements in 80% of their generated images
     </a>
     asked for negative prompt advice to generate "regular porn" instead.
    </li>
    <li>
     <strong>
      Creating short video clips with AI images and animated text
     </strong>
     : A /r/StableDiffusion
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cfwxct/how_to_create_short_videos_by_using_ai_images_and/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      post asked about APIs
     </a>
     to generate AI images with animated text overlays to create short video clips.
    </li>
    <li>
     <strong>
      Newer Nvidia GPUs may be slower for AI despite gaming gains
     </strong>
     : A
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cg0gz6/be_careful_when_buying_new_nvidia_card_or_laptop/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      warning was posted
     </a>
     that newer Nvidia GPUs like the 4070 laptop version use narrower memory buses than older models, making them slower for AI workloads.
    </li>
    <li>
     <strong>
      Proposal for community image tagging project
     </strong>
     : A /r/StableDiffusion
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgbivm/community_effort_for_best_image_tagging/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      post suggested a community effort
     </a>
     to comprehensively tag images to create a dataset of consistently captioned images for training better models.
    </li>
    <li>
     <strong>
      Using VAEs for image compression
     </strong>
     : Experiments
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgdyjc/vae_as_image_compression/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      shared in /r/StableDiffusion
     </a>
     show using VAE latents for image compression is competitive with JPEG in some cases. Saving generated images as latents is lossless and much smaller than PNGs.
    </li>
    <li>
     <strong>
      Generating a full body from a headshot
     </strong>
     : In /r/StableDiffusion, a
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cg3a4z/help_me_with_this_will_pay/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      user asked if it's possible
     </a>
     to generate a full body from a headshot image without altering the face much using SD Forge.
    </li>
    <li>
     <strong>
      Textual inversion of Audrey Hepburn
     </strong>
     : A /r/StableDiffusion user
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cft1gp/give_you_a_slightly_different_audrey_hepburn/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      made a textual inversion
     </a>
     of Audrey Hepburn that produces similar but varied faces, sharing example images and a Civitai link.
    </li>
   </ul>
   <hr/>
   <h1 id="ai-twitter-recap">
    AI Twitter Recap
   </h1>
   <blockquote>
    <p>
     all recaps done by Claude 3 Opus, best of 4 runs. We are working on clustering and flow engineering with Haiku.
    </p>
   </blockquote>
   <p>
    <strong>
     LLMs and AI Models
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      Llama 3 Performance
     </strong>
     :
     <a href="https://twitter.com/abacaj/status/1785147493728039111?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @abacaj
     </a>
     noted that llama-3 models with zero-training can get
     <strong>
      32k context with exceptional quality
     </strong>
     , surpassing significantly larger models.
     <a href="https://twitter.com/rohanpaul_ai/status/1784889182558539917?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @rohanpaul_ai
     </a>
     mentioned Llama 3 captures extremely nuanced data relationships, utilizing even the minutest decimals in BF16 precision, making it
     <strong>
      more sensitive to quantization degradation
     </strong>
     compared to Llama 2.
    </li>
    <li>
     <strong>
      Llama 3 Benchmarks
     </strong>
     :
     <a href="https://twitter.com/abacaj/status/1785295341736043007?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @abacaj
     </a>
     reported llama-3 70B takes
     <strong>
      3rd place on a benchmark, replacing Haiku
     </strong>
     .
     <a href="https://twitter.com/abacaj/status/1785153286976237765?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @abacaj
     </a>
     shared a completion from the model on a
     <strong>
      code snippet benchmark
     </strong>
     that requires the model to find a function based on a description.
    </li>
    <li>
     <strong>
      Llama 3 Variants
     </strong>
     :
     <a href="https://twitter.com/mervenoyann/status/1785320444918211022?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @mervenoyann
     </a>
     noted
     <strong>
      new LLaVA-like models based on LLaMA 3 &amp; Phi-3
     </strong>
     that pass the baklava benchmark.
     <a href="https://twitter.com/AIatMeta/status/1785042326416658580?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @AIatMeta
     </a>
     mentioned Meditron, an LLM suite for low-resource medical settings built by @ICepfl &amp; @YaleMed researchers, which
     <strong>
      outperforms most open models in its parameter class
     </strong>
     on benchmarks like MedQA &amp; MedMCQA using Llama 3.
    </li>
    <li>
     <strong>
      GPT-2 Chatbot
     </strong>
     : There was speculation about the identity of the gpt2-chatbot model, with
     <a href="https://twitter.com/sama/status/1785107943664566556?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @sama
     </a>
     noting he has a soft spot for gpt2. Some theories suggested it could be a preview of GPT-4.5/5 or a derivative model, but most agreed it was
     <strong>
      unlikely to be the latest OAI model
     </strong>
     .
    </li>
    <li>
     <strong>
      Phi-3 and Other Models
     </strong>
     :
     <a href="https://twitter.com/danielhanchen/status/1785040680106234225?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @danielhanchen
     </a>
     released a
     <strong>
      Phi-3 notebook that finetunes 2x faster and uses 50% less VRAM
     </strong>
     than HF+FA2.
     <a href="https://twitter.com/rohanpaul_ai/status/1785220060803453160?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @rohanpaul_ai
     </a>
     shared a paper suggesting transformers learn in-context by
     <strong>
      performing gradient descent on a loss function constructed from the in-context data
     </strong>
     within their forward pass.
    </li>
   </ul>
   <p>
    <strong>
     Prompt Engineering and Evaluation
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      Prompt Engineering Techniques
     </strong>
     :
     <a href="https://twitter.com/cwolferesearch/status/1784992130777137362?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @cwolferesearch
     </a>
     categorized recent prompt engineering research into
     <strong>
      reasoning, tool usage, context window, and better writing
     </strong>
     . Techniques include zero-shot CoT prompting, selecting exemplars based on complexity, refining rationales, decomposing tasks, using APIs, optimizing context windows, and iterative prompting.
    </li>
    <li>
     <strong>
      LLMs as Juries
     </strong>
     :
     <a href="https://twitter.com/cohere/status/1785284142789242932?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @cohere
     </a>
     released a paper exploring
     <strong>
      replacing a single LLM judge with multiple LLM juries
     </strong>
     for evaluation. The "PoLL" method with a diverse set of LLMs
     <strong>
      outperformed single judges across datasets while being 7-8x cheaper
     </strong>
     than GPT-4.
    </li>
    <li>
     <strong>
      Evaluating LLMs
     </strong>
     :
     <a href="https://twitter.com/_lewtun/status/1785246966626029596?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @_lewtun
     </a>
     asked about research on which prompts produce an LLM-judge most correlated with human preferences for pairwise rankings, beyond the work by @lmsysorg.
     <a href="https://twitter.com/_philschmid/status/1785273493375922221?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @_philschmid
     </a>
     summarized the
     <strong>
      PoLL (Panel of LLM) method
     </strong>
     proposed by @cohere for LLM evaluation as an alternative to a single large model judge.
    </li>
   </ul>
   <p>
    <strong>
     Applications and Use Cases
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      Financial Calculations
     </strong>
     :
     <a href="https://twitter.com/llama_index/status/1785325832317415641?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @llama_index
     </a>
     shared a full-stack tutorial for building a financial assistant that can
     <strong>
      calculate percentage evolution, CAGR, and P/E ratios over unstructured financial reports
     </strong>
     using LlamaParse, RAG, Opus, and math formulas in @llama_index.
    </li>
    <li>
     <strong>
      SQL Query Generation
     </strong>
     :
     <a href="https://twitter.com/virattt/status/1785059112478257413?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @virattt
     </a>
     used @cohere cmd r+ to
     <strong>
      extract ticker and year metadata from financial queries
     </strong>
     in ~1s, then used the metadata to filter a vector db, fed results to GPT-4, and answered user query with ~3s total latency.
    </li>
    <li>
     <strong>
      Multi-Agent RAG
     </strong>
     :
     <a href="https://twitter.com/LangChainAI/status/1785066609847291986?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @LangChainAI
     </a>
     announced a YouTube workshop on exploring "multi-agent" applications that
     <strong>
      combine independent agents to solve complex problems
     </strong>
     using planning, reflection, tool use, and their LangGraph library.
    </li>
    <li>
     <strong>
      Robotics and Embodied AI
     </strong>
     :
     <a href="https://twitter.com/DrJimFan/status/1785292766387302897?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @DrJimFan
     </a>
     advocated for
     <strong>
      robotics as the next frontier after LLMs
     </strong>
     , sharing MIT AI Lab's 1971 proposal emphasizing robotics and reflecting on the current state.
     <a href="https://twitter.com/_akhaliq/status/1785139220534730771?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @_akhaliq
     </a>
     shared a paper on Ag2Manip, which
     <strong>
      improves imitation learning for manipulation tasks
     </strong>
     using agent-agnostic visual and action representations.
    </li>
   </ul>
   <p>
    <strong>
     Frameworks, Tools and Platforms
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      LangChain Tutorials
     </strong>
     :
     <a href="https://twitter.com/LangChainAI/status/1784970647875330251?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @LangChainAI
     </a>
     shared a
     <strong>
      4-hour course on understanding how LangChain works
     </strong>
     with various technologies to build 6 projects.
     <a href="https://twitter.com/llama_index/status/1784962053641478454?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @llama_index
     </a>
     provided a
     <strong>
      reference architecture for advanced RAG
     </strong>
     using LlamaParse, AWS Bedrock, and @llama_index.
    </li>
    <li>
     <strong>
      Diffusers Library
     </strong>
     :
     <a href="https://twitter.com/RisingSayak/status/1785162074844197174?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @RisingSayak
     </a>
     explained how the Diffusers library
     <strong>
      supports custom pipelines and components
     </strong>
     , allowing flexibility in building diffusion models while maintaining the benefits of the
     <code>
      DiffusionPipeline
     </code>
     class.
    </li>
    <li>
     <strong>
      Amazon Bedrock
     </strong>
     :
     <a href="https://twitter.com/cohere/status/1785015769971220720?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @cohere
     </a>
     announced their
     <strong>
      Command R model series is now available on Amazon Bedrock
     </strong>
     for enterprise workloads.
     <a href="https://twitter.com/llama_index/status/1785105949818237227?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @llama_index
     </a>
     showed how to use LlamaParse for advanced parsing in the AWS/Bedrock ecosystem and
     <strong>
      build RAG with the Bedrock Knowledge Base
     </strong>
     .
    </li>
    <li>
     <strong>
      DeepSpeed Support
     </strong>
     :
     <a href="https://twitter.com/StasBekman/status/1785091895733154116?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @StasBekman
     </a>
     noted a PR merged into
     <code>
      main@accelerate
     </code>
     that makes FSDP
     <strong>
      converge at the same speed as DeepSpeed when loading fp16 models
     </strong>
     , by automatically upcasting trainable params to fp32.
    </li>
   </ul>
   <p>
    <strong>
     Memes, Humor and Other
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      ASCII Art
     </strong>
     : Several tweets poked fun at the ASCII art capabilities of LLMs, with
     <a href="https://twitter.com/ylecun/status/1785109502565531699?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @ylecun
     </a>
     noting how
     <strong>
      AI hype has become indistinguishable from satire
     </strong>
     .
     <a href="https://twitter.com/teortaxesTex/status/1785325820166185399?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @teortaxesTex
     </a>
     shared a prompt to draw a Katamari Damacy level map using emojis that strains "GPT2"'s instruction following.
    </li>
    <li>
     <strong>
      Anthropic Slack
     </strong>
     :
     <a href="https://twitter.com/alexalbert__/status/1785369914204938326?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @alexalbert__
     </a>
     shared his
     <strong>
      10 favorite things from Anthropic's internal Slack channel
     </strong>
     where employees post cool Claude interactions and memes since its launch.
    </li>
    <li>
     <strong>
      Rabbit Disappointment
     </strong>
     : Several users expressed disappointment with the Rabbit AI device, noting its
     <strong>
      limited functionality compared to expectations
     </strong>
     .
     <a href="https://twitter.com/agihippo/status/1785359480294936882?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      @agihippo
     </a>
     questioned what the Rabbit r1 can do that a phone can't.
    </li>
   </ul>
   <hr/>
   <h1 id="ai-discord-recap">
    AI Discord Recap
   </h1>
   <blockquote>
    <p>
     A summary of Summaries of Summaries
    </p>
   </blockquote>
   <p>
    <strong>
     1) Fine-Tuning and Optimizing Large Language Models
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      Challenges in Fine-Tuning LLaMA-3
     </strong>
     : Engineers faced issues like the model
     <strong>
      not generating EOS tokens
     </strong>
     , and
     <strong>
      embedding layer compatibility across bit formats
     </strong>
     . However, one member achieved success by utilizing
     <strong>
      <a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1553?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       LLaMA-3 specific prompt strategies
      </a>
     </strong>
     for fine-tuning.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      LLaMA-3 Sensitive to Quantization
     </strong>
     : Discussions highlighted that
     <strong>
      <a href="https://x.com/rohanpaul_ai/status/1784972618472317180?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       LLaMA-3 experiences more degradation from quantization
      </a>
     </strong>
     compared to LLaMA-2, likely due to capturing nuanced relationships from training on 15T tokens.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Perplexity Fine-Tuning Challenges
     </strong>
     : Fine-tuning
     <strong>
      LLaMA-3 for perplexity
     </strong>
     may not surpass the base model's performance, with the tokenizer suspected as a potential cause.
    </li>
   </ul>
   <p>
    <strong>
     2) Extending Context Lengths and Capabilities
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      Llama-3 Hits New Context Length Highs
     </strong>
     : The release of
     <strong>
      <a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       Llama-3 8B Gradient Instruct 1048k
      </a>
     </strong>
     extends the context length from 8k to over 1048k tokens, showcasing state-of-the-art long context handling.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Llama 3 Gains Vision with SigLIP
     </strong>
     : A breakthrough integrates
     <strong>
      <a href="https://huggingface.co/qresearch/llama-3-vision-alpha-hf?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       vision capabilities for Llama 3
      </a>
     </strong>
     using SigLIP, enabling direct use within Transformers despite quantization limitations.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Extending Context to 256k with PoSE
     </strong>
     : The context length of
     <strong>
      Llama 3 8B
     </strong>
     has been expanded from 8k to
     <strong>
      <a href="https://huggingface.co/winglian/llama-3-8b-256k-PoSE?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       256k tokens using PoSE
      </a>
     </strong>
     , though inferencing challenges remain for 'needle in haystack' scenarios.
    </li>
   </ul>
   <p>
    <strong>
     3) Benchmarking and Evaluating LLMs
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      Llama 3 Outperforms GPT-4 in German NLG
     </strong>
     : On the
     <strong>
      <a href="https://scandeval.com/german-nlg/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       ScanEval German NLG benchmark
      </a>
     </strong>
     ,
     <strong>
      Llama 3
     </strong>
     surpassed the performance of
     <strong>
      GPT-4
     </strong>
     , indicating its strong language generation capabilities.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Mysterious GPT2-Chatbot Sparks Speculation
     </strong>
     : A
     <strong>
      <a href="https://chat.lmsys.org/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       GPT2-chatbot
      </a>
     </strong>
     with gpt4-level capabilities surfaced, leading to debates on whether it could be an early glimpse of
     <strong>
      GPT-4.5
     </strong>
     or a finetuned version of the original GPT-2.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Questioning Leaderboard Utility for Code Generation
     </strong>
     : A
     <strong>
      <a href="https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       blog post
      </a>
     </strong>
     challenges the effectiveness of AI leaderboards for code generation, citing the high operational cost of top performers like LLM debugger despite ranking highly.
    </li>
   </ul>
   <p>
    <strong>
     4) Revolutionizing Gaming with LLM-Powered NPCs
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      LLM-Powered NPCs and Inference Stack
     </strong>
     : The release of
     <strong>
      <a href="https://github.com/GigaxGames/gigax?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       LLM-powered NPC models
      </a>
     </strong>
     aims to enhance action spaces and simplify API calls, including a single LLM call feature and open-weights on Hugging Face.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Overcoming LLM Challenges for Gameplay
     </strong>
     : Developers faced issues like
     <strong>
      NPCs breaking the fourth wall
     </strong>
     , missing details in large prompts, and optimizing for runtime speeds, suggesting solutions like
     <strong>
      output compression
     </strong>
     ,
     <strong>
      minimizing model calls
     </strong>
     , and leveraging
     <strong>
      smaller models
     </strong>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Insights into Fine-Tuning LLMs for NPCs
     </strong>
     : Developers plan to share their
     <strong>
      struggles and triumphs in fine-tuning LLMs for dynamic NPC behavior
     </strong>
     through an upcoming blog post, pointing towards new strategies for gaming applications.
    </li>
   </ul>
   <p>
    <strong>
     5) Misc
    </strong>
   </p>
   <ul>
    <li>
     <strong>
      CUDA Optimization Techniques
     </strong>
     : CUDA developers discussed various optimization strategies, including using
     <code>
      Packed128
     </code>
     custom structs for memory access patterns, replacing integer division with bit shifts (
     <a href="https://godbolt.org/z/9K9Gf1v6P?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Compiler Explorer link
     </a>
     ), and comparing performance of
     <strong>
      CUTLASS vs CuBLAS
     </strong>
     for matrix multiplications. The
     <strong>
      Effort Engine
     </strong>
     algorithm was introduced, enabling adjustable computational effort during LLM inference to achieve speeds comparable to standard matrix multiplications on Apple Silicon (
     <a href="https://kolinko.github.io/effort?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      kolinko.github.io/effort
     </a>
     ,
     <a href="https://github.com/kolinko/effort?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      GitHub
     </a>
     ).
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      LLaMA-3 Context Length Extension and Fine-Tuning
     </strong>
     : The
     <strong>
      LLaMA-3 8B
     </strong>
     model's context length was extended to over 1M tokens using
     <strong>
      PoSE
     </strong>
     (
     <a href="https://huggingface.co/winglian/llama-3-8b-256k-PoSE?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      huggingface.co/winglian/llama-3-8b-256k-PoSE
     </a>
     ), sparking discussions on its retrieval performance and compute requirements. Fine-tuning LLaMA-3 presented challenges like
     <strong>
      quantization degradation
     </strong>
     ,
     <strong>
      EOS token generation
     </strong>
     , and
     <strong>
      embedding layer compatibility
     </strong>
     across bit formats. A potential breakthrough was shared in a
     <a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1553?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      GitHub pull request
     </a>
     demonstrating successful fine-tuning with model-specific prompt strategies.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Civitai Monetization Backlash
     </strong>
     : Stable Diffusion community members expressed discontent with
     <strong>
      Civitai's monetization strategies
     </strong>
     , particularly the
     <strong>
      Buzz donation system
     </strong>
     , which was labeled a "rip-off" by some like Tower13Studios (
     <a href="https://youtu.be/nLT32AR5c68?si=bV9wXlRzb_oLutW9&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      The Angola Effect
     </a>
     ). Discussions also highlighted the potential profitability of
     <strong>
      NSFW AI-generated art commissions
     </strong>
     compared to the saturated SFW market.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Perplexity AI Performance Issues
     </strong>
     : Users reported significant slowdowns and poor performance across various Perplexity AI models during Japan's Golden Week, with specific issues in
     <strong>
      Japanese searches
     </strong>
     resulting in meaningless outputs. Frustrations arose over
     <strong>
      expired Pro subscription coupons
     </strong>
     and the removal of the
     <strong>
      7-day free trial
     </strong>
     . Technical troubles included
     <strong>
      email link delays
     </strong>
     affecting login and inconsistencies in the
     <strong>
      iOS voice feature
     </strong>
     depending on app versions.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Decentralized AI Training Initiatives
     </strong>
     : Prime Intellect proposed a decentralized training approach using
     <strong>
      H100 GPU clusters
     </strong>
     to enable open-source AI to compete with proprietary models (
     <a href="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      blog post
     </a>
     ). The initiative aims to address computing infrastructure limitations by leveraging globally distributed GPU resources.
    </li>
   </ul>
   <hr/>
   <h1 id="part-1-high-level-discord-summaries">
    PART 1: High level Discord summaries
   </h1>
   <h2 id="cuda-mode-discord">
    <a href="https://discord.com/channels/1189498204333543425?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     CUDA MODE
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Triton Troubles
     </strong>
     : Engineers discussed limitations with
     <strong>
      Triton blocks
     </strong>
     , identifying an issue where blocks of 4096 elements are feasible, yet blocks of 8192 are not, hinting at discrepancies with expected
     <strong>
      CUDA
     </strong>
     limits.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      CUDA Cognitions and Collaborations
     </strong>
     : Various
     <strong>
      CUDA
     </strong>
     topics were mulled over, including
     <strong>
      CUTLASS vs. CuBLAS performance
     </strong>
     ,
     <strong>
      CUDA checkpointing
     </strong>
     , and the replacement of integer division with bit shifts. A link to the
     <a href="https://godbolt.org/z/9K9Gf1v6P?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Compiler Explorer
     </a>
     was shared to help with experiments.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      PyTorch Peculiarities Pursued
     </strong>
     : Members examined the behavior of PyTorch's
     <code>
      linear
     </code>
     function and matrix multiplication kernel launches, with observations about double kernel launches and the false expectation of performance differences due to transposition.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      LLM Inference Optimization with Effort Engine
     </strong>
     : Discussion revolved around the
     <strong>
      Effort Engine
     </strong>
     algorithm, which enables adjustable computational effort during LLM inference, purportedly yielding speeds comparable to standard matrix multiplications on Apple Silicon at lower efforts. The implementation and details are provided on
     <a href="https://kolinko.github.io/effort?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      kolinko.github.io/effort
     </a>
     and
     <a href="https://github.com/kolinko/effort?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      GitHub
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      InstaDeep's Machine Learning Manhunt
     </strong>
     :
     <strong>
      InstaDeep
     </strong>
     is on the hunt for
     <strong>
      Machine Learning Engineers
     </strong>
     with expertise in
     <strong>
      high-performance ML engineering, custom CUDA kernels, and distributed training
     </strong>
     . Candidates can scout for opportunities at
     <a href="https://www.instadeep.com/job-offer/92900fa3-5501-4506-a63f-cebee958fc6f/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      InstaDeep Careers
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Llama-3 Levitates to Longer Contexts
     </strong>
     : The release of
     <a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Llama-3 8B Gradient Instruct 1048k
     </a>
     set a new benchmark for context length capabilities in LLMs.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      ROCm Rallies for Flash Attention 2
     </strong>
     : Conversations in the
     <strong>
      ROCM
     </strong>
     channel centered on adapting NVIDIA's Flash Attention 2 for ROCm, with a focus on compatibility with
     <strong>
      ROCM 6.x
     </strong>
     versions and a link to the relevant repository
     <a href="https://github.com/ROCm/flash-attention?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      ROCm/flash-attention on GitHub
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      CUDA Conclave Converges on ‚ÄúPacked128‚Äù Innovations
     </strong>
     : The
     <strong>
      llmdotc
     </strong>
     channel was a hotspot with discussions focused on optimizing
     <code>
      Packed128
     </code>
     data structures and
     <strong>
      BF16 mixed-precision strategies
     </strong>
     , while also touching on the nuanced use of
     <strong>
      NVTX
     </strong>
     contexts and the utility of different benchmarking toolsets like
     <strong>
      Modal
     </strong>
     .
    </li>
   </ul>
   <hr/>
   <h2 id="unsloth-ai-daniel-han-discord">
    <a href="https://discord.com/channels/1179035537009545276?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Unsloth AI (Daniel Han)
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Fusing Checkpoints to Avoid Overfitting
     </strong>
     : A member sought guidance on checkpoint merging to avoid overfitting and was directed to the Unsloth
     <a href="https://github.com/unslothai/unsloth/wiki?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408#finetuning-from-your-last-checkpoint" target="_blank">
      finetuning checkpoint wiki
     </a>
     . Techniques such as
     <em>
      warmup steps
     </em>
     and
     <em>
      resuming from checkpoints
     </em>
     were recommended for nuanced training regimens.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Quantization Quandary in WSL2
     </strong>
     : Users reported
     <strong>
      RuntimeError: Unsloth: Quantization failed
     </strong>
     when converting models to F16 within WSL2. Despite attempts at rebuilding the
     <code>
      llama.cpp
     </code>
     and re-quantization, the error persisted.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Phi-3: A Model of Interest
     </strong>
     : The upcoming release of
     <strong>
      Phi-3
     </strong>
     stirred interest, with engineers debating whether to adopt the 3.8b version or wait for the heftier 7b or 14b variants.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      OOM Countermeasures and Performance Data Confusion
     </strong>
     : Tips for handling Out of Memory errors on Google Colab by cache clearing were exchanged. Meanwhile, confusion surfaced over reported performance measures for quantized
     <strong>
      LLama 2
     </strong>
     and
     <strong>
      LLama 3
     </strong>
     , hinting at possible data misplacement between Bits Per Word (BPW) and Perplexity (PPL).
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Extended Possibilities
     </strong>
     :
     <strong>
      Llama 3 8B
     </strong>
     reached new potential with a Context length increase to 256k tokens, achieved with
     <strong>
      <a href="https://huggingface.co/papers/2309.10400?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       PoSE
      </a>
     </strong>
     , showcased at
     <a href="https://huggingface.co/winglian/llama-3-8b-256k-PoSE?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      winglian/llama-3-8b-256k-PoSE
     </a>
     . Community applause went to Winglian, though some voiced skepticism about non-official context-extended model behavior.
    </li>
   </ul>
   <hr/>
   <h2 id="lm-studio-discord">
    <a href="https://discord.com/channels/1110598183144399058?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     LM Studio
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Groq's Gift to Discord Bots
     </strong>
     : A user shared a
     <a href="https://youtu.be/ySwJT3Z1MFI?si=qFfek8gTGXVJWoxB&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      YouTube video
     </a>
     highlighting the
     <em>
      free
     </em>
     Groq API enabling access to the LLAMA-3 model's impressive 300 tokens per second speed, optimally suited for small server Discord bots due to its no-cost setup.
    </li>
    <li>
     <strong>
      Spec Smackdown
     </strong>
     : Users recommended posting system specs in specific channels when troubleshooting
     <strong>
      LM Studio on Ubuntu GPUs
     </strong>
     , debated the compatibility of GPUs with
     <strong>
      inference tasks
     </strong>
     , and discussed the potentially incorrect VRAM capacity display in LM Studio causing concerns with
     <strong>
      GPU offloading efficiency
     </strong>
     .
    </li>
    <li>
     <strong>
      Model Mania
     </strong>
     : The community buzzed about alternative methods for downloading the GGUF model from sources other than Huggingface, the time and resource demands of creating
     <em>
      iQuants
     </em>
     and
     <em>
      imatrices
     </em>
     , and shared reward offers for optimizing the
     <strong>
      Goliath 120B Longlora
     </strong>
     model to create its
     <em>
      iQuant
     </em>
     version.
    </li>
    <li>
     <strong>
      Model Mayhem on Modest Machines
     </strong>
     : Users grappled with issues like the Phi-3 model's
     <strong>
      leaking prompts
     </strong>
     ,
     <em>
      local training
     </em>
     queries for Hugging Face-based models, and the unexpected noises from hard drives during token generation by the Llama3m. Some determined that more dated hardware could just about manage a
     <strong>
      7b Q4 model
     </strong>
     but nothing heftier.
    </li>
    <li>
     <strong>
      ROCm Ruminations
     </strong>
     : Enthusiasts dissected ROCm versions, mulling over the benefits of
     <strong>
      beta 0.2.20
     </strong>
     for AMD functionality, addressed confusion about compatibility‚Äîespecially the RX 6600's support with the current
     <strong>
      HIP SDK
     </strong>
     ‚Äîand discussed discrepancies in ROCm's functionality on different operating systems like
     <strong>
      Ubuntu versus Windows
     </strong>
     .
    </li>
   </ul>
   <hr/>
   <h2 id="stabilityai-stable-diffusion-discord">
    <a href="https://discord.com/channels/1002292111942635562?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Stability.ai (Stable Diffusion)
    </a>
    Discord
   </h2>
   <p>
    <strong>
     Buzz Off, Civitai
    </strong>
    : AI creators in the guild are upset with Civitai's monetization strategies, particularly the Buzz donation system, which was labeled a
    <strong>
     "rip-off"
    </strong>
    by some members, such as Tower13Studios. The discontent revolves around value not being fairly returned to creators (
    <a href="https://youtu.be/nLT32AR5c68?si=bV9wXlRzb_oLutW9&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     The Angola Effect
    </a>
    ).
   </p>
   <p>
    <strong>
     Finding The AI Art Goldmine
    </strong>
    : A vibrant discussion unfolded on the economics of AI-generated art, with consensus pointing towards NSFW commissions, including furry and vtuber content, as a more profitable avenue compared to the more crowded SFW market.
   </p>
   <p>
    <strong>
     Race for Real-Time Rendering
    </strong>
    : Members actively shared Python scripting techniques for accelerating Stable Diffusion (SDXL) models, eyeing uses in dynamic realms like Discord bots, aiming to enhance image generation speed for real-time applications.
   </p>
   <p>
    <strong>
     Anticipation Builds for Collider
    </strong>
    : The community is keenly awaiting Stable Diffusion's next iteration, dubbed "Collider," with speculation about release dates and potential advancements fueling eager anticipation among users.
   </p>
   <p>
    <strong>
     Tech Troubleshooting Talk
    </strong>
    : Guild members exchanged insights and solutions on a spectrum of technical challenges, from creating LoRAs and IPAdapters to running AI models on low-spec hardware, demonstrating a collective effort to push the boundaries of model implementation and optimization.
   </p>
   <hr/>
   <h2 id="perplexity-ai-discord">
    <a href="https://discord.com/channels/1047197230748151888?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Perplexity AI
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Japanese Golden Week Glitches
     </strong>
     : During Japan's Golden Week, users observed a noticeable performance drop in tools like
     <strong>
      Opus, Sonar Large 32K,
     </strong>
     and
     <strong>
      GPT-4 Turbo
     </strong>
     , with specific issues in Japanese searches, resulting in outputs that users deemed
     <em>
      meaningless garbage
     </em>
     . To address the problem, vigilant monitoring and optimization of these models was suggested.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Frustration over Pro Subscription and Trial Perils
     </strong>
     :
     <strong>
      Pro subscription
     </strong>
     users reported expired coupons on the due date, with offers linked to the
     <strong>
      Nothing Phone 2(a)
     </strong>
     aborted prematurely due to fraudulent activities. Moreover, the 7-day free trial's removal from the site prompted disappointment, emphasizing its value as a user conversion tool.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Tech Turbulence with Perplexity AI
     </strong>
     : The community grappled with
     <strong>
      email link delays
     </strong>
     , causing login difficulties, particularly for non-Gmail services. Additionally, variations in the
     <strong>
      iOS voice feature
     </strong>
     were found to be dependent on the
     <strong>
      app version
     </strong>
     being used, reflecting inconsistencies in user experience.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      API Avenues Explored
     </strong>
     : Engineers queried the
     <strong>
      pplx-api
     </strong>
     channel regarding
     <strong>
      source URL
     </strong>
     access through the API, following its mention in roadmap documentation, and debated whether using
     <strong>
      Claude 3
     </strong>
     would entail adherence to
     <strong>
      Anthropic's political usage
     </strong>
     restrictions under Perplexity's terms.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Miscellaneous Inquiries and Insights Surface
     </strong>
     : A post in the
     <strong>
      #
      <a href="https://discord.com/channels/1047197230748151888/1054944216876331118/1234586871569449121?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
       sharing
      </a>
     </strong>
     channel spotlighted Lenny's Newsletter on product growth and building concepts, while queries about WhatsApp's autoreply feature and Vimeo's API were thrown in. These discussions, particularly on the API, highlight engineers' focus on integrating and utilizing various functionalities in their systems/processes.
    </li>
   </ul>
   <hr/>
   <h2 id="nous-research-ai-discord">
    <a href="https://discord.com/channels/1053877538025386074?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Nous Research AI
    </a>
    Discord
   </h2>
   <p>
    <strong>
     Bold Decentralization Move
    </strong>
    : Prime Intellect's initiative for decentralized AI training, leveraging
    <em>
     H100 GPU clusters
    </em>
    , promises to push the boundaries by globalizing distributed training. The open-source approach may address current computing infrastructure bottlenecks as discussed in their
    <a href="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     decentralized training blog
    </a>
    .
   </p>
   <p>
    <strong>
     Retrieval Revolution with LLama-3
    </strong>
    : The extension of
    <strong>
     LLama-3 8B's
    </strong>
    context length to over 1040K tokens sparks discussions on whether its retrieval performance lives up to the hype. Skeptics remain, emphasizing the ongoing necessity of improvements and training, supported by an
    <a href="https://arxiv.org/abs/2404.16811?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     ArXiv paper on IN2 training
    </a>
    .
   </p>
   <p>
    <strong>
     PDF Challenges Tackled
    </strong>
    : To address PDF parsing challenges within AI models, particularly for tables, the community discussed workarounds and tools like
    <a href="https://platform.openai.com/docs/assistants/tools/file-search?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     OpenAI's file search
    </a>
    for better multimodal functionality handling roughly 10k files.
   </p>
   <p>
    <strong>
     World Sims Showcase AI's Role-Playing Prowess
    </strong>
    : Engagements with AI-driven world simulations highlight the capacities of
    <strong>
     llama 3 70b
    </strong>
    and
    <strong>
     Claude 3
    </strong>
    , from historical figures to business and singing career simulators. OpenAI's chat on
    <a href="https://hf.co/chat/assistant/65ffac7250c6fddecfd20bc8?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     HuggingChat
    </a>
    and links to niche simulations like
    <a href="https://hf.co/chat/assistant/6626e4869232378718adc5f2?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Snow Singer Simulator
    </a>
    reflect the diversity and depth achievable.
   </p>
   <p>
    <strong>
     Leveraging Datasets for Multilingual Dense Retrieval
    </strong>
    : A noted
    <a href="https://huggingface.co/collections/nthakur/swim-ir-dataset-662ddaecfc20896bf14dd9b7?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Wikipedia RAG dataset
    </a>
    on HuggingFace earmarks the rise of fostering AI's language retrieval capabilities. The included Halal and Kosher data points toward a trend of creating diverse and inclusive AI resources.
   </p>
   <hr/>
   <h2 id="modular-mojo-discord">
    <a href="https://discord.com/channels/1087530497313357884?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Modular (Mojo üî•)
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Mojo's Memory Safety and Concurrency Debated
     </strong>
     : Despite buzz around
     <strong>
      Mojo's
     </strong>
     potential, it was clarified that features like
     <strong>
      Golang-like concurrency
     </strong>
     and
     <strong>
      Rust-like memory safety
     </strong>
     are not currently implemented due to
     <strong>
      borrow checking being disabled
     </strong>
     . However, possibilities regarding the use of actor model concurrency are being explored which may enhance Mojo‚Äôs runtime efficiency.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Installation Tactics for Mojo on Varied Systems
     </strong>
     : Users face challenges installing
     <strong>
      Mojo
     </strong>
     with
     <strong>
      Python 3.12.3
     </strong>
     particularly on
     <strong>
      Mac M1
     </strong>
     , for which using a
     <strong>
      Conda environment
     </strong>
     is recommended. Also, while native
     <strong>
      Windows support
     </strong>
     is pending,
     <strong>
      WSL on Windows
     </strong>
     is a current workaround, with cross-compilation capabilities hinted through
     <strong>
      LLVM involvement
     </strong>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Community Contributions to Mojo Ecosystem
     </strong>
     : Several community-driven projects are enhancing the Mojo ecosystem, from a Mojo-based forum on GitHub to a
     <strong>
      20% performance optimized
     </strong>
     atof-simd project for long strings. Enthusiasm for collaboration and knowledge-sharing is evident as members share projects and call for joint efforts to tackle challenges such as the 1brc.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Nightly Compilations Trigger Discussions on SIMD and Source Location
     </strong>
     : A new
     <strong>
      nightly
     </strong>
     release of the
     <strong>
      Mojo compiler
     </strong>
     spurred conversation about the conversion of
     <strong>
      SIMD to EqualityComparable
     </strong>
     and the need for explicit
     <code>
      reduce_and
     </code>
     or
     <code>
      reduce_or
     </code>
     in place of implicit conversion to
     <code>
      Bool
     </code>
     . The move of
     <code>
      __source_location()
     </code>
     to
     <code>
      __call_location()
     </code>
     incited exchanges on proper usage within the language.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Performance and Benchmarking Take the Spotlight
     </strong>
     : From optimizing SIMD-based error correction code to sharing substantial speed gains in the 1brc project, performance topics spurred discussions on
     <strong>
      LLVM/MLIR optimizations
     </strong>
     . There were calls to form a "team-mojo" for communal challenge tackling, underscoring a shared interest in progressing Mojo‚Äôs benchmarking endeavors against other languages.
    </li>
   </ul>
   <hr/>
   <h2 id="huggingface-discord">
    <a href="https://discord.com/channels/879548962464493619?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     HuggingFace
    </a>
    Discord
   </h2>
   <p>
    <strong>
     Snowflake's MoE Model Breaks Through
    </strong>
    : Snowflake introduces a
    <a href="https://x.com/reach_vb/status/1783129119435210836?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     monumental 408B parameter Dense + Hybrid MoE model
    </a>
    with a 4K context window, entirely under Apache 2.0 license, sparking excitement for its performance on sophisticated tasks.
   </p>
   <p>
    <strong>
     Gradio Share Server on the Fritz
    </strong>
    : Gradio acknowledges
    <a href="https://status.gradio.app/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     issues with their Share Server
    </a>
    , impacting Colab integrations, which is under active resolution with updates available on their status page.
   </p>
   <p>
    <strong>
     CVPR 2023 Sparks Competitive Spirit
    </strong>
    : CVPR 2023
    <a href="https://huggingface.co/spaces/BVRA/SnakeCLEF2024?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     announced competetive events
    </a>
    like SnakeCLEF, FungiCLEF, and PlantCLEF, boasting over $120k in rewards and happening June 17-21, 2024.
   </p>
   <p>
    <strong>
     MIT Deep Learning Course Goes Live
    </strong>
    : MIT updates its Introduction to Deep Learning course for 2024, with comprehensive
    <a href="https://www.youtube.com/watch?v=ErnWZxJovaM&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=2&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     lecture videos on YouTube
    </a>
    .
   </p>
   <p>
    <strong>
     NLP Woes in Chatbot Land
    </strong>
    : Within the NLP community, effort mounts to finetune a chatbot using the Rasa framework, despite struggles with intent recognition and categorization, and plans to augment performance with a custom NER model and company-specific intents.
   </p>
   <hr/>
   <h2 id="openrouter-alex-atallah-discord">
    <a href="https://discord.com/channels/1091220969173028894?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     OpenRouter (Alex Atallah)
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Alex Atallah Signposts Collaboration with Syrax
     </strong>
     : Alex Atallah has initiated experiments with
     <strong>
      Syrax
     </strong>
     and extended support by proposing a group chat for collaborative efforts, marking the start of a partnership acknowledged with enthusiasm by Mart02.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Frontend for the Rest of Us
     </strong>
     : The community explored solutions for deploying multi-user frontends on shared hosting without advanced technical requirements.
     <strong>
      LibreChat
     </strong>
     was suggested as a viable platform, with Vercel's free tier hosting mentioned as a means to address hosting and cost obstacles.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      LLMs Throwdown
     </strong>
     : A robust debate unfolded over several large language models including
     <em>
      Llama-3 8B
     </em>
     ,
     <em>
      Dolphin 2.9
     </em>
     , and
     <em>
      Mixtral-8x22B
     </em>
     , touching on aspects like context window size and censorship concerns related to conversational styles and datasets.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Training Unhinged AIs
     </strong>
     : An intriguing experiment involved training a model with a toxic dataset to foster a more "unhinged" persona. Discussions dug into model limitations with long contexts, with an agreement that although models like
     <em>
      Llama 3 8B
     </em>
     handle extensive contexts, performance dips were likely past a threshold.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Cost-Effective Experimentation on OpenRouter
     </strong>
     : Conversations centered on finding efficient yet affordable models on
     <strong>
      OpenRouter
     </strong>
     . Noteworthy was the mix of surprise and approval for the human-like output of models like
     <em>
      GPT-3.5
     </em>
     that deliver a solid blend of affordability and performance.
    </li>
   </ul>
   <hr/>
   <h2 id="llamaindex-discord">
    <a href="https://discord.com/channels/1059199217496772688?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     LlamaIndex
    </a>
    Discord
   </h2>
   <p>
    <strong>
     AWS Architecture Goes Academic
    </strong>
    :
    <strong>
     LlamaIndex
    </strong>
    revealed an advanced AWS-based architecture for building sophisticated RAG systems, aimed at parsing and reasoning. Details are accessible in their
    <a href="https://t.co/sfQOvhHHg5?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     code repository
    </a>
    .
   </p>
   <p>
    <strong>
     Documentation Bot Triumphs in Hackathon
    </strong>
    : Hackathon victors,
    <strong>
     Team CLAB
    </strong>
    , developed an impressive documentation bot leveraging
    <strong>
     LlamaIndex
    </strong>
    and
    <strong>
     Nomic embeddings
    </strong>
    ; check out the hackathon wrap-up in this
    <a href="https://t.co/2UMqrHwO56?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     blog post
    </a>
    .
   </p>
   <p>
    <strong>
     Financial Assistants Get a Boost
    </strong>
    : Constructing financial assistants that interpret unstructured data and perform complex computations has been greatly improved. The methodology is thoroughly explored in a
    <a href="https://t.co/6cTNxUBJcr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     recent post
    </a>
    .
   </p>
   <p>
    <strong>
     Turbocharging RAG with Semantic Caching
    </strong>
    : Collaboration with @Redisinc demonstrated significant performance gains for RAG applications using
    <strong>
     semantic caching
    </strong>
    to speed up queries. The collaboration details can be found
    <a href="https://t.co/oGxFrZLMRn?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     here
    </a>
    .
   </p>
   <p>
    <strong>
     GPT-1: The Trailblazer Remembered
    </strong>
    : A reflective glance at GPT-1 and its contributions to LLM development was shared, discussing features like positional embeddings which paved the way for modern models like Mistral-7B. The nostalgia-laden
    <a href="https://amgadhasan.substack.com/p/revisiting-gpt-1-the-spark-that-ignited-llms?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     blog post
    </a>
    revisits GPT-1's architecture and impact.
   </p>
   <hr/>
   <h2 id="eleuther-discord">
    <a href="https://discord.com/channels/729741769192767510?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Eleuther
    </a>
    Discord
   </h2>
   <p>
    <strong>
     Plug Into New Community Projects
    </strong>
    : Members are seeking opportunities to contribute to community AI projects that provide computational resources, addressing the issue for those lacking personal GPU infrastructure.
   </p>
   <p>
    <strong>
     Unlock the Mysteries of AI Memory
    </strong>
    : Intricacies of memory processes in AI were covered with a particular focus on "clear-ing", orthogonal keys, and the delta rule in compressive memory. There‚Äôs an interest in discussing whether infini-attention has been overhyped, despite its theoretical promise.
   </p>
   <p>
    <strong>
     Comparing Apples to Supercomputers
    </strong>
    : There's an active debate regarding performance discrepancies between models like
    <em>
     mixtral 8x22B
    </em>
    and
    <em>
     llama 3 70B
    </em>
    , where
    <em>
     llama's
    </em>
    reduced number of layers, despite having more parameters, may be impacting its speed and batching efficiency.
   </p>
   <p>
    <strong>
     LLMs: Peering Inside the Black Box
    </strong>
    : The community is contemplating the ‚Äúblack box‚Äù nature of Large Language Models, discussing emergent abilities and data leakage. A connection was made between emergent abilities and pretraining loss, challenging the focus on compute as a performance indicator.
   </p>
   <p>
    <strong>
     Bit Depth Bewilderment
    </strong>
    : A user reported issues when encoding with
    <strong>
     8bit
    </strong>
    on models like
    <strong>
     llama3-70b
    </strong>
    and
    <strong>
     llamma3-8b
    </strong>
    , experiencing significant degradation in output quality, suggesting a cross-model encoding challenge that needs addressing.
   </p>
   <hr/>
   <h2 id="laion-discord">
    <a href="https://discord.com/channels/823813159592001537?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     LAION
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      GDPR Complaint Challenges AI Birthdays
     </strong>
     : An EU privacy advocate has filed a
     <a href="https://www.politico.eu/article/chatgpts-hallucinations-get-eu-privacy-complaint/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      GDPR complaint
     </a>
     after an AI model incorrectly estimated his birthday, triggering discussions on the potential implications for AI operations in Europe.
    </li>
    <li>
     <strong>
      Mysterious GPT-5 Speculations
     </strong>
     : Amidst rumors of a new GPT-5 model release, the community debates inconsistent test outcomes and the absence of official communication or leaderboard recognitions, questioning the framework's evasiveness in generating hallucinations.
    </li>
    <li>
     <strong>
      Llama3 70B's Slow Performance Spotlight
     </strong>
     : AI engineers are troubleshooting the
     <a href="https://rentry.co/GPT2?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Llama3 70B
     </a>
     model's unexpectedly sluggish token generation rate of 13 tokens per second on a dual 3090 rig, delving into possible hardware and configuration enhancements.
    </li>
    <li>
     <strong>
      Exllama Library Outraces Rivals
     </strong>
     : Users endorse
     <strong>
      Exllama
     </strong>
     for its fast performance on language model tasks and suggest utilizing the
     <a href="https://dct.openempathic.ai/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      TabbyAPI
     </a>
     repository for simpler integrations, naming it a superior choice compared to other libraries.
    </li>
    <li>
     <strong>
      Research Breakthrough with OpenCLIP
     </strong>
     : The successful application of
     <strong>
      OpenCLIP
     </strong>
     to cardiac ultrasound analysis has been published, highlighting the rigorous revision process and a move towards novel, non-zero-shot techniques, with the study available
     <a href="https://doi.org/10.1038/s41591-024-02959-y?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      here
     </a>
     ; meanwhile
     <em>
      r/StableDiffusion
     </em>
     is back online and a relevant CLIP training repository is discussed in the context of Reddit's recent API changes, found at
     <a href="https://www.reddit.com/r/StableDiffusion/comments/1cgyjvt/github_zer0intclipfinetune_or_sdxl_training_the/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      this Reddit discussion
     </a>
     .
    </li>
   </ul>
   <hr/>
   <h2 id="openai-discord">
    <a href="https://discord.com/channels/974519864045756446?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     OpenAI
    </a>
    Discord
   </h2>
   <p>
    <strong>
     Memory Lane with Upscaled ChatGPT Plus
    </strong>
    : ChatGPT Plus now allows users to command the AI to remember specific contexts, which can be toggled on and off in settings; the rollout has not reached Europe or Korea yet. Plus, both Free and Plus users gain enhanced data control, including a 'Temporary Chat' option that discards conversations immediately after they end.
   </p>
   <p>
    <strong>
     AI Ghosh-darn Curiosity and Camera Tricks
    </strong>
    : Discussions swung from defining AI curiosity and sentience with maze challenges to the merits of DragGAN altering photos with new angles. Meanwhile, the Llama-3 8B model emerged, flaunting its long-context skills and is accessible at
    <a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Hugging Face
    </a>
    , but the community still wrestled with the accessibility of advanced AI technologies and the dream of inter-model collaboration.
   </p>
   <p>
    <strong>
     GPT-4: Bigger and Maybe Slower?
    </strong>
    : The community dove into the attributes of GPT-4, noting its significantly larger size than the 3.5 version and raising concerns about whether its scale may affect processing speed. Meanwhile, the possibility of mass-deleting archived chats was also a topic of concern.
   </p>
   <p>
    <strong>
     Prompt Engineering's Competitive Edge
    </strong>
    : Prompt engineering drew attention, with suggestions for competitions to hone skills, and 'meta prompting' via GPT Builder to refine AI output. The group agreed that positive prompting trumps listing prohibites, and wrestled with optimizing regional Spanish nuances in AI text generation.
   </p>
   <p>
    <strong>
     Cross-Channel Theme of Prompting Excellence
    </strong>
    : Both AI discussions and API channels tackled prompt engineering, with meta-prompting techniques at the spotlight, indicating a shift toward more efficient prompting strategies that might decrease the need for competitions. Navigating the complexities of multilingual outputs also emerged as a shared challenge, emphasizing adaptation rather than prohibition.
   </p>
   <hr/>
   <h2 id="openaccess-ai-collective-axolotl-discord">
    <a href="https://discord.com/channels/1104757954588196865?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     OpenAccess AI Collective (axolotl)
    </a>
    Discord
   </h2>
   <p>
    <strong>
     LLaMA 3 Struggles with Quantization
    </strong>
    :
    <strong>
     LLaMA 3
    </strong>
    is observed to have significant performance degradation from quantization processes, more so than its predecessor, which might be due to its expansive training on 15T tokens capturing very nuanced data relations. A critique within the community called a study on quantization sensitivity "worthless," suggesting that the issue may be more related to model training approaches rather than size; the critique referenced a
    <a href="https://arxiv.org/abs/2311.16452?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     study on arXiv
    </a>
    .
   </p>
   <p>
    <strong>
     Riding the Zero Train
    </strong>
    : The Guild discussed
    <strong>
     Huggingface's ZeroGPU
    </strong>
    , a beta feature offering free access to multi-GPU resources like Nvidia A100, with some members expressing regret at missing early access. A member has
    <a href="https://huggingface.co/zero-gpu-explorers?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     shared access
    </a>
    and is open to suggestions for testing on the platform.
   </p>
   <p>
    <strong>
     Finetuning Finesse
    </strong>
    : Advised against fine-tuning
    <code>
     meta-llama/Meta-Llama-3-70B-Instruct
    </code>
    , it was suggested that members start with smaller models like 8B to sharpen their fine-tuning skills. The Guild clarified how to convert a fine-tuning dataset from OpenAI to ShareGPT format, and provided guidance with Python code for dataset transformation.
   </p>
   <p>
    <strong>
     Tutorial Spreads Its Wings
    </strong>
    : A helpful
    <a href="https://github.com/dstackai/dstack/blob/master/examples/fine-tuning/axolotl/README.md?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     tutorial was shared
    </a>
    on fine-tuning Axolotl using dstack, showing the community's knack for collaboratively improving practices. Appreciation was conveyed by members, noting the tutorial's ease of use.
   </p>
   <p>
    <strong>
     Axolotl Adaptations
    </strong>
    : Discussing the fine-tuning of
    <em>
     command-r
    </em>
    within Axolotl and related format adaptations, a member shared an
    <a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1547?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     untested pull request
    </a>
    relating to this topic, while also noting its prematurity for merging. In addition, there's uncertainty about the support for phi-3 format and the implementation standing of
    <em>
     sample packing
    </em>
    feature, indicating a need for further clarification or development.
   </p>
   <hr/>
   <h2 id="latent-space-discord">
    <a href="https://discord.com/channels/822583790773862470?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Latent Space
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Memary: An Autonomous Agent's Long-term Memory
     </strong>
     : The
     <a href="https://github.com/kingjulio8238/memary?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Memary
     </a>
     project on GitHub has introduced a new approach for long-term memory in autonomous agents, using document similarity searches over traditional knowledge graphs.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      The GPT-2 Chatbot Enigma
     </strong>
     : Intense debates have emerged on a
     <a href="https://chat.lmsys.org/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      GPT2-chatbot
     </a>
     that showcases surprisingly advanced capabilities, leading to speculation that it might be a finetuned version of OpenAI's GPT-2.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Can Decentralized Training Compete with Big Tech?
     </strong>
     :
     <a href="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Prime Intellect's blog post
     </a>
     discusses decentralized training as a plausible avenue for open-source artificial intelligence to compete with the proprietary models developed by large corporations with extensive GPU resources.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Redefining LLMs with Modular Context and Memory
     </strong>
     : Discussions are emerging that suggest a paradigm shift towards designing autonomous agents with modularized shared context and memory capabilities for reasoning and planning, stepping away from the reliance on standalone large language models (LLMs).
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Educational Resources for Aspiring AI Enthusiasts
     </strong>
     : For those seeking to learn AI fundamentals, community members recommended resources including neural network tutorials such as the one on
     <a href="https://youtu.be/aircAruvnKk?feature=shared&amp;utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      YouTube
     </a>
     and courses like
     <em>
      Learn Prompting
     </em>
     , providing a glimpse into AI engineering and prompt engineering basics.
    </li>
   </ul>
   <hr/>
   <h2 id="openinterpreter-discord">
    <a href="https://discord.com/channels/1146610656779440188?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     OpenInterpreter
    </a>
    Discord
   </h2>
   <p>
    <strong>
     OS Start-up with a Vision
    </strong>
    : A user faced challenges attempting to
    <strong>
     launch OS mode with a local vision model for Moondream
    </strong>
    and received gibberish output, but the discussion did not yield a solution or direct advice.
   </p>
   <p>
    <strong>
     Integration Achievements
    </strong>
    : An exciting integration of
    <strong>
     OpenInterpreter
    </strong>
    outputs into
    <strong>
     MagicLLight
    </strong>
    was mentioned, with anticipation for a future code release and pull request including a
    <code>
     stream_out
    </code>
    function hook and
    <code>
     external_input
    </code>
    .
   </p>
   <p>
    <strong>
     Hardware Hiccup Help
    </strong>
    : Queries about running
    <strong>
     OpenInterpreter on budget hardware
    </strong>
    like a Raspberry Pi Zero were brought up alongside requests for assistance with
    <strong>
     debugging startup issues
    </strong>
    . Community members offered to help with troubleshooting once more details were provided.
   </p>
   <p>
    <strong>
     Push Button Programming
    </strong>
    : An individual fixed an external push button issue on
    <strong>
     pin 25
    </strong>
    and shared a
    <a href="https://discord.com/channels/openinterpreter/01?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     code snippet
    </a>
    , also getting community confirmation that the fix was effective.
   </p>
   <p>
    <strong>
     Volume Up on Tech Talk
    </strong>
    : There were mixed opinions on whether tech YouTubers have a grasp on AI technologies while advising on options for increasing speaker volume, including using
    <strong>
     M5Unified
    </strong>
    or an
    <a href="https://www.amazon.com/dp/B01DKAI51M?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     external amplifier
    </a>
    .
   </p>
   <hr/>
   <h2 id="tinygrad-george-hotz-discord">
    <a href="https://discord.com/channels/1068976834382925865?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     tinygrad (George Hotz)
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Peek into Tinygrad's Inner Workings
     </strong>
     : The
     <a href="https://github.com/tinygrad/tinygrad/tree/master?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      tinygrad GitHub repository
     </a>
     was recommended to someone curious about
     <strong>
      tinygrad
     </strong>
     , an educational project for enthusiasts of PyTorch and micrograd. Another community member inquired about graph visualization, leading to the suggestion to use the
     <code>
      GRAPH=1
     </code>
     environment variable to generate diagrams for addressing backward pass issues
     <a href="https://github.com/tinygrad/tinygrad/issues/3572?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      #3572
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      The Discovery of Learning Resources
     </strong>
     : The community explored learning AI with TinyGrad through resources like
     <a href="https://github.com/unknownusername504/MicroGrad?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      MicroGrad
     </a>
     and
     <a href="https://minitorch.github.io/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      MiniTorch
     </a>
     , with MiniTorch being singled out as particularly useful for understanding deep learning systems. The "
     <a href="https://tinygrad.github.io/tinygrad/quickstart/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      tinygrad Quick Start Guide
     </a>
     " was highlighted as a starting point for beginners.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Taking the Symbolic Route
     </strong>
     : Implementing a symbolic mean operation in TinyGrad brought up discussions about LazyBuffer's interaction with data types and the practicality of variable caching for operations like
     <code>
      sum
     </code>
     and
     <code>
      mean
     </code>
     . A
     <a href="https://github.com/tinygrad/tinygrad/pull/1552?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      pull request
     </a>
     demonstrated symbolic code execution while further GitHub compare views tackled the development of symbolic mean with variables at
     <a href="https://github.com/tinygrad/tinygrad/compare/master...davidjanoskyrepo:tinygrad:symbolic-mean-var-pull?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      tinygrad symbolic-mean-var-pull
     </a>
     and
     <a href="https://github.com/tinygrad/tinygrad/compare/86d90511cee2%5E...97a2d44d9840?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      GitHub changes by gh
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Bounty Hunting for Mean Solutions
     </strong>
     : The community sought guidance for bounty challenges related to
     <em>
      "Mean of symbolic shape"
     </em>
     and
     <em>
      "Symbolic arrange"
     </em>
     . Discussion centered around the implementation nuances and practical approaches to these problems in the TinyGrad environment.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Cluster of Curiosities
     </strong>
     : A spontaneous question about how a member discovered the Discord server triggered a chain of speculations, with the respondent admitting they did not recall the method of encounter, adding a touch of mystery to the channel discourse.
    </li>
   </ul>
   <hr/>
   <h2 id="cohere-discord">
    <a href="https://discord.com/channels/954421988141711382?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Cohere
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Single-Site Restrictions in Command-R
     </strong>
     :
     <strong>
      API Command R+
     </strong>
     's
     <code>
      web_search
     </code>
     tool only allows for one website at a time, and the workaround discussed involves separate
     <strong>
      API calls for each site
     </strong>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Feature Request Frenzy
     </strong>
     : Engineers are eager for
     <strong>
      Command-R
     </strong>
     improvements with an emphasis on
     <strong>
      Connectors
     </strong>
     , including multi-website searches and extra parameter control; to get familiar with current capabilities, refer to the
     <a href="https://docs.cohere.com/reference/chat?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Cohere Chat Documentation
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Multi-Step Connector Capabilities Currently Limited
     </strong>
     : It was confirmed that
     <strong>
      multi-step tool use
     </strong>
     with
     <strong>
      connectors
     </strong>
     isn't yet possible within
     <strong>
      Command-R
     </strong>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Generate Option Gone Missing
     </strong>
     : Queries rose regarding the disappearance of 'Generate' for fine-tuning models from the dashboard, leaving its future presence in question.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Strategic Embedding Sought
     </strong>
     : Discussion revolved around cost-effective strategies for keeping data fresh for embeddings, with a focus on reindexing only modified segments.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Nordic Networking Noted
     </strong>
     : Members highlighted operations within
     <strong>
      Sweden
     </strong>
     using
     <strong>
      Cohere
     </strong>
     and existing connections through the company
     <strong>
      Omegapoint
     </strong>
     , spanning both Sweden and Norway.
    </li>
   </ul>
   <hr/>
   <h2 id="langchain-ai-discord">
    <a href="https://discord.com/channels/1038097195422978059?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     LangChain AI
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Gemini Experience Wanted &amp; Observability Tools Sought
     </strong>
     : Users in the
     <strong>
      general
     </strong>
     channel are seeking expertise in
     <strong>
      Gemini 1.0 or 1.5 models
     </strong>
     and discussing available tools for Large Language Model (LLM) observability, with interest in self-hosted, open-source options compatible with
     <strong>
      LlamaIndex
     </strong>
     . Meanwhile, there's a push for enhanced SQL security when connecting to OpenAI models and a technical discussion on integrating
     <strong>
      autoawq
     </strong>
     with
     <strong>
      LangGraph
     </strong>
     for high-speed AI agent inference using
     <strong>
      exllamav2 kernels
     </strong>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Asynchronous Adventures and Google Drive Gyrations
     </strong>
     : Within the
     <strong>
      langserve
     </strong>
     channel, a user is challenged by the lack of async support in
     <strong>
      AzureSearchVectorStoreRetriever
     </strong>
     and is considering whether to push for an async feature or to craft an async wrapper themselves. Separately, the discussion turned to the nuances of using Google Drive libraries and the importance of setting the drive key as an environment variable.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Showcase Extravaganza &amp; Plugin Revelation
     </strong>
     : In
     <strong>
      share-your-work
     </strong>
     , there's an insights-filled trip back to
     <strong>
      GPT-1
     </strong>
     's role in initiating current LLM advancements and several LangChain use cases, including a "D-ID Airbnb Use Case" and a "Pizza Bot", both featured on
     <strong>
      YouTube
     </strong>
     . The
     <strong>
      VectorDB plugin for LM Studio
     </strong>
     also made an appearance, aiming to bolster ChromaDB vector databases in server mode, while
     <strong>
      QuickVid
     </strong>
     was launched to deliver YouTube video summaries and fact checks.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      RAG Agents Go Multilingual &amp; Private
     </strong>
     : Tutorials channel is sharing resources for interested French speakers in building RAG assistants with
     <strong>
      LangChain, Mistral Large
     </strong>
     , and
     <strong>
      Llamaindex
     </strong>
     . Another guide demonstrates enhancing
     <strong>
      llama3
     </strong>
     's performance by incorporating personal knowledge bases to create agentic RAGs, revealing potential for more localized and data-rich AI capabilities.
    </li>
   </ul>
   <hr/>
   <h2 id="alignment-lab-ai-discord">
    <a href="https://discord.com/channels/1087862276448595968?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Alignment Lab AI
    </a>
    Discord
   </h2>
   <p>
    <strong>
     Alert: Illicit Spam Floods Channels
    </strong>
    : Numerous messages across different channels promoted explicit material involving "18+ Teen Girls and OnlyFans leaks," accompanied by a
    <a href="https://discord.gg/CYNumE8ABr?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Discord invite link
    </a>
    . All messages were similar in nature, using emojis and
    <code>
     @everyone
    </code>
    to garner attention, and are flagrant violations of Discord's community guidelines.
   </p>
   <p>
    <strong>
     Prompt Moderation Action Required
    </strong>
    : The repeated posts are indicative of a coordinated spam attack necessitating immediate moderation intervention. Each message invariably linked to an external Discord server, potentially baiting users into exploitative environments.
   </p>
   <p>
    <strong>
     Engineer Vigilance Advocacy
    </strong>
    : Members are encouraged to report such posts to maintain professional decorum. The content breaches both legal and ethical boundaries and does not align with the guild's purpose or standards.
   </p>
   <p>
    <strong>
     Discord Server Safety at Risk
    </strong>
    : The proliferation of these messages highlights a concern for server security and member safety. The spam suggests a compromise of server integrity, underscoring the need for robust anti-spam measures.
   </p>
   <p>
    <strong>
     Community Urged to Disregard Suspicious Links
    </strong>
    : Engineers and members are urged to avoid engaging with or clicking on unsolicited links. Such practices help safeguard personal information and the community's credibility while adhering to legal and ethical codes.
   </p>
   <hr/>
   <h2 id="ai-stack-devs-yoko-li-discord">
    <a href="https://discord.com/channels/1122748573000409160?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     AI Stack Devs (Yoko Li)
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Game Devs Gear Up for Gamification
     </strong>
     : Rosebud AI's
     <strong>
      Game Jam
     </strong>
     invites creators to fashion 2D browser-based games using
     <strong>
      Phaser JS
     </strong>
     with a $500 prize pool, and an
     <strong>
      AIxGames Meetup
     </strong>
     is slated for Thursday in SF to bring together AI and gaming professionals
     <a href="https://partiful.com/e/TwvC5qxskuPGqiliMj5f?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      RSVP here
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      NPC Revolution with LLMs
     </strong>
     : A developer has introduced LLM-powered NPC models and an inference stack, available on
     <a href="https://github.com/GigaxGames/gigax?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      GigaxGames at GitHub
     </a>
     , promising an LLM single call feature and open-weights on
     <a href="https://huggingface.co/Gigax?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Huggingface's Hub
     </a>
     , despite a hiccup with a broken API access link.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Grappling with Gaming NPC Realities
     </strong>
     : Developers are experimenting with
     <em>
      output compression
     </em>
     , minimized model calls, and smaller models to improve NPC runtime performance and grappling with NPCs that break the fourth wall, with the
     <em>
      Claude 3
     </em>
     model showing promise in empathetic interactions for better gaming experiences.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Blog Teased on LLMs for NPCs
     </strong>
     : There's an upcoming blog post chronicling the struggles and triumphs in finetuning LLMs for dynamic NPC behavior, pointing towards new strategies that could be shared within the community.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Navigating Windows Woes with Convex
     </strong>
     : The
     <strong>
      Convex local
     </strong>
     setup does not play nice with Windows, causing users to encounter sticking points, though potential solutions like
     <strong>
      WSL
     </strong>
     or
     <strong>
      Docker
     </strong>
     have been floated, and a Windows-compatible Convex is reportedly on the horizon.
    </li>
   </ul>
   <hr/>
   <h2 id="skunkworks-ai-discord">
    <a href="https://discord.com/channels/1131084849432768614?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Skunkworks AI
    </a>
    Discord
   </h2>
   <p>
    <strong>
     Binary Quest in HaystackDB
    </strong>
    : Curiosity piqued about the potential use of
    <strong>
     2-bit embeddings
    </strong>
    in
    <a href="https://github.com/carsonpo/haystackdb?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     HaystackDB
    </a>
    , while
    <strong>
     Binary Quantized (BQ)
    </strong>
    indexing becomes a spotlight topic due to its promise of leaner and faster similarity searches.
   </p>
   <p>
    <strong>
     The Rough Lane of Fine-Tuning LLaMA-3
    </strong>
    : Engineers face a bumpy road with
    <strong>
     LLaMA-3 fine-tuning
    </strong>
    , battling issues from the model neglecting
    <strong>
     EOS token generation
    </strong>
    to embedding layer compatibility across bit formats.
   </p>
   <p>
    <strong>
     Perplexed by Perplexity
    </strong>
    : The community debates fine-tuning
    <strong>
     LLaMA-3 for perplexity
    </strong>
    , suggesting that performance may not surpass the base model, possibly due to tokenizer-related complications.
   </p>
   <p>
    <strong>
     Shining a Light on LLaMA-3 Improvement
    </strong>
    : A beacon of hope shines as one user successfully fine-tunes
    <strong>
     LLaMA-3
    </strong>
    with model-specific prompt strategies, sparking interest with a GitHub
    <a href="https://github.com/OpenAccess-AI-Collective/axolotl/pull/1553?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     pull request
    </a>
    for the collective's scrutiny.
   </p>
   <p>
    <strong>
     Off-Topic Oddities Go Unsummarized
    </strong>
    : A solitary link in
    <strong>
     #off-topic
    </strong>
    stands alone, contributing no technical discussion to the collective knowledge pool.
   </p>
   <hr/>
   <h2 id="mozilla-ai-discord">
    <a href="https://discord.com/channels/1089876418936180786?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Mozilla AI
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Mozilla's AI Talent Search
     </strong>
     : Mozilla AI is actively recruiting for various roles, with job opportunities available for those interested in contributing to their initiatives. For those looking to join the team, they can find more information and apply using the provided
     <a href="https://discord.com/channels/1089876418936180786/1230938514955436242/1234870020916510823?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      link
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      LM-buddy: Eval Tool for Language Models
     </strong>
     : The release of Lm-buddy, an open-source evaluation tool for language models, stands to improve the assessment of LLMs. Contributors and users are encouraged to engage with the project through the given
     <a href="https://discord.com/channels/1089876418936180786/1230938514955436242/1234589599733518378?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      link
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Prometheus Benchmarks LLMs in Judicial Roles
     </strong>
     : The Prometheus project has demonstrated the potential for Local Large Language Models (LLMs) to act as arbiters, a novel concept sparking discussion. Interested parties can join the conversation about this application by following the
     <a href="https://discord.com/channels/1089876418936180786/1234890301143912599/1234890301143912599?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      link
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      In-Depth Code Analysis Request for LLaMA
     </strong>
     : An engineer has noted that token generation in llama.cpp/llamafile is a bottleneck, with matrix-vector multiplications consuming 95% of the inference time for LLaMA2. This has led to speculation on whether loop unrolling contributes to the 30% better performance of llama.cpp over alternative implementations.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      LLaMA Tales of Confusion and Compatibility
     </strong>
     : The Discord discussed amusing mix-ups and pseudonymous confusion with LLaMA parameters. Additionally, challenges were shared regarding the integration with Plush-for-comfyUI and LLaMA3's compatibility issues on M1 Macbook Air, promising priority testing for the M1 once current LLaMA3 issues are addressed.
    </li>
   </ul>
   <hr/>
   <h2 id="interconnects-nathan-lambert-discord">
    <a href="https://discord.com/channels/1179127597926469703?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Interconnects (Nathan Lambert)
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      OLMo Deep Dive Shared by AI Maverick
     </strong>
     : A detailed talk on "OLMo: Findings of Training an Open LM" by Hanna Hajishirzi was posted, featuring her work at the
     <a href="https://youtu.be/qFZbu2P1vZ8?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Open-Source Generative AI Workshop
     </a>
     . Her pace of presenting substantive content on OLMo, Dolma, Tulu, etc., was noted to be rapid, possibly challenging for students to digest, thus reflecting her expertise and the extensive research involved in these projects.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      RL in LM-Based Systems Exposed
     </strong>
     : Key takeaways from John Schulman's discussion on reinforcement learning for language model-based systems were encapsulated in a GitHub
     <a href="https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Gist
     </a>
     , providing engineers with a compressed synthesis of his approach and findings.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      AI Leaderboard Limitations Pointed Out
     </strong>
     : A
     <a href="https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      blog post
     </a>
     by Sayash Kapoor and Benedikt Stroebl challenges the effectiveness of AI leaderboards for code generation, highlighting LLM debugger's (LDB) high operational cost despite its top rankings, calling into question the utility of such benchmarks in the face of significant expenses.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      SnailBot
     </strong>
     : A mention for an update or news related to SnailBot was made but lacked further information or context for a substantive summary.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Notice
     </strong>
     : Based on the provided snippets from the Discord guild there is no additional content that warrants a summary, indicating that these messages may have been part of a larger context or subsequent discussions that were not included.
    </li>
   </ul>
   <hr/>
   <h2 id="llm-perf-enthusiasts-ai-discord">
    <a href="https://discord.com/channels/1168579740391710851?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     LLM Perf Enthusiasts AI
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Gamma Seeking AI Wizard
     </strong>
     :
     <strong>
      Gamma
     </strong>
     is hiring an
     <strong>
      AI engineer
     </strong>
     to drive innovation in AI-driven presentation and website design, with a focus on prompt engineering, metrics, and model fine-tuning; details are at
     <a href="https://careers.gamma.app/ai-engineer?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      Gamma Careers
     </a>
     . Despite the need for an in-person presence in
     <strong>
      San Francisco
     </strong>
     , the role is open to those with strong Large Language Model (LLM) skills even if they lack extensive engineering experience.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      AI-Powered Enterprise on Growth Fast-track
     </strong>
     : Flaunting over
     <strong>
      10 million users
     </strong>
     and
     <strong>
      $10M+ in funding
     </strong>
     ,
     <strong>
      Gamma
     </strong>
     is looking for an AI engineer to help sustain its growth while enjoying a hybrid work culture within its
     <strong>
      profitable
     </strong>
     and compact 16-member team.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      The Case of GPT-4.5 Speculations
     </strong>
     : A tweet by
     <strong>
      @phill__1
     </strong>
     hinted at gpt2-chatbot possessing 'insane domain knowledge,' leading to speculation that it might represent the capabilities of a
     <strong>
      GPT-4.5
     </strong>
     version
     <a href="https://x.com/phill__1/status/1784964135920235000?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      phill__1's observation
     </a>
     .
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Chatbot Causes Community Commotion
     </strong>
     : The engineer community is abuzz with the idea that the gpt2-chatbot could be an unintentional glimpse at the prowess of
     <strong>
      GPT-4.5
     </strong>
     , with a member succinctly endorsing it as "good".
    </li>
   </ul>
   <hr/>
   <h2 id="datasette-llm-simonw-discord">
    <a href="https://discord.com/channels/823971286308356157?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     Datasette - LLM (@SimonW)
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Snazzy Syntax-Nixing for Code-Gen
     </strong>
     : A user discussed the concept of incorporating a
     <strong>
      custom grammar
     </strong>
     within a language model to prioritize identifying semantic rather than syntactic errors during code generation.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      Data-fied Dropdowns for Datasette
     </strong>
     : Suggestions were exchanged on improving
     <strong>
      Datasette's UX
     </strong>
     , including a front-page design that features dropdown menus to enable users to generate summary tables based on selected parameters, such as country choice.
    </li>
   </ul>
   <ul>
    <li>
     <strong>
      UX Magic with Direct Data Delivers
     </strong>
     : Members proposed enhanced UX solutions for
     <strong>
      Datasette
     </strong>
     , including dynamically updating URLs or building homepage queries adjusted by user selection to streamline access to relevant data.
    </li>
   </ul>
   <hr/>
   <h2 id="discoresearch-discord">
    <a href="https://discord.com/channels/1178995845727785010?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
     DiscoResearch
    </a>
    Discord
   </h2>
   <ul>
    <li>
     <strong>
      Loading Anomalies Enigma
     </strong>
     : A conversation highlighted that a process
     <strong>
      loads in 3 seconds on a local machine
     </strong>
     but faces delays when run through job submission, implying that the issue may not be related to storage but perhaps environment-specific overheads.
    </li>
    <li>
     <strong>
      Llama Trumps GPT-4 in Language Benchmark
     </strong>
     :
     <strong>
      Llama 3
     </strong>
     outperformed
     <strong>
      GPT-4
     </strong>
     in the
     <strong>
      ScanEval benchmark for German NLG
     </strong>
     , as shown on
     <a href="https://scandeval.com/german-nlg/?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-to-be-named-4408" target="_blank">
      ScandEval's leaderboard
     </a>
     .
    </li>
   </ul>
   <hr/>
   <p>
    The
    <strong>
     AI21 Labs (Jamba) Discord
    </strong>
    has no new messages. If this guild has been quiet for too long, let us know and we will remove it.
   </p>
   <hr/>
  </div>
 </body>
</html>
